<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>臭咸鱼的缺氧瓶</title>
  
  <subtitle>快给我氧气！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://chouxianyu.github.io/"/>
  <updated>2021-04-27T03:13:03.268Z</updated>
  <id>https://chouxianyu.github.io/</id>
  
  <author>
    <name>臭咸鱼</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>李宏毅机器学习课程笔记-12.1对抗攻击入门</title>
    <link href="https://chouxianyu.github.io/2021/04/27/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-12-1%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E5%85%A5%E9%97%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/27/李宏毅机器学习课程笔记-12-1对抗攻击入门/</id>
    <published>2021-04-27T02:23:23.000Z</published>
    <updated>2021-04-27T03:13:03.268Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>我们希望我们的模型不仅仅是在大多数情况下可用的，还希望它能够应对来自外界的“攻击”，特别是在垃圾邮件分类、恶意软件检测、网络入侵检测等任务中。</p><p>这个领域为对抗攻击与防御（Adversarial Attack and Defense），目前攻击是比较容易的而防御比较困难。</p><h2 id="What-is-Attack"><a href="#What-is-Attack" class="headerlink" title="What is Attack"></a>What is Attack</h2><p>attack就是往原输入$x^0$中添加一些特别的噪声$\Delta x$(并不是随机生成的)得到一个稍微有些不同的输入$x’=x^0+\Delta x$，而模型却得到一个与原输出截然不同的输出。如下图所示，在图像分类任务中，对一张“Tiger Cat”图片添加一些特别的噪声$\Delta x$后，人类还能看出它是“Tiger Cat”，但模型却认为它是其它的类别。</p><p><img src="https://pic1.zhimg.com/80/v2-5610b9b55bf8893362a8526352810834_720w.png" alt="img"></p><h2 id="Loss-Function-For-Attack"><a href="#Loss-Function-For-Attack" class="headerlink" title="Loss Function For Attack"></a>Loss Function For Attack</h2><p>攻击可以分为两种：Non-targeted Attack和Targeted Attack。</p><h3 id="How-To-Train"><a href="#How-To-Train" class="headerlink" title="How To Train"></a>How To Train</h3><p>我们是这样训练一个普通的神经网络的：将输入$x^0$输入到模型后，我们希望<strong>模型的输出$y^0$和标签$y^{true}$越接近越好</strong>，则损失函数为$L_{train}(\theta)=C(y^0,y^{true})$。<strong>此时输入$x^0$是固定的，我们需要不断调整模型参数$\theta$</strong>，使得$L_{train}(\theta)$最小。</p><h3 id="Non-targeted-Attack"><a href="#Non-targeted-Attack" class="headerlink" title="Non-targeted Attack"></a>Non-targeted Attack</h3><p>如果是Non-targeted Attack，将输入$x’=x+\Delta x$输入到模型后，我们希望<strong>模型的输出$y’$和标签$y^{true}$的差异越大越好</strong>，则损失函数为$L_{Non-targeted\ Attack}(x’)=-C(y’,y^{true})$，比$L_{train}(\theta)$多了一个负号。<strong>此时模型参数$\theta$是固定的，我们需要不断调整输入$x’$</strong>，使$L_{Non-targeted\ Attack}(x’)$最小。</p><h3 id="Targeted-Attack"><a href="#Targeted-Attack" class="headerlink" title="Targeted Attack"></a>Targeted Attack</h3><p>如果是Targeted Attack，将输入$x’=x+\Delta x$输入到模型后，我们希望<strong>模型的输出$y’$和标签$y^{true}$的差异越大越好并且模型的输出$y’$与某个$y^{false}$越接近越好</strong>，其中$y^{false}$需要人为选择，则损失函数为$L_{Targeted\ Attack}(x’)=-C(y’,y^{true})+C(y’,y^{false})$。<strong>此时模型参数$\theta$是固定的，我们需要不断调整输入$x’$</strong>，使$L_{Targeted\ Attack}(x’)$最小。</p><h2 id="Constraint-For-Attack"><a href="#Constraint-For-Attack" class="headerlink" title="Constraint For Attack"></a>Constraint For Attack</h2><p>在Attack中，除了要使$L_{Non-targeted\ Attack}(x’)$和$L_{Targeted\ Attack}(x’)$最小之外，我们还希望<strong>$x^0$和$x’$之间的差异较小</strong>，即$d(x^0,x’)\leq\epsilon$，其中$\epsilon$需要人为选择，这样才能实现真正的Attack。</p><p>主要有两种计算$d(x^0,x’)$的方法，但在不同的任务中应该有不同的计算方法，因为其代表着人类视角下$x^0$和$x’$之间的差异。</p><h3 id="L2-norm"><a href="#L2-norm" class="headerlink" title="L2-norm"></a>L2-norm</h3><p>L2-norm为$x^0$和$x’$中每个像素之差的平方和，即$d(x^0,x’)=||x^0-x’||_2=||\Delta x||_2=(\Delta x_1)^2+(\Delta x_2)^2+(\Delta x_3)^2+\dots$</p><h3 id="L-infinity"><a href="#L-infinity" class="headerlink" title="L-infinity"></a>L-infinity</h3><p>L-infinity为$x^0$和$x’$中每个像素之差的最大值，即$d(x^0,x’)=||x^0-x’||_{\infin}=||\Delta x||_{\infty}=max\{\Delta x_1,\Delta x_2,\Delta_3,\dots\}$</p><p>对于图像中的pixel来讲，也许L-infinity是更有效的计算方法。</p><h2 id="How-to-Attack"><a href="#How-to-Attack" class="headerlink" title="How to Attack"></a>How to Attack</h2><p>我们在Attack时要训练的参数是输入$x’$而非模型参数$\theta$，在$d(x^0,x’)\leq\epsilon$的情况下使得$L(x’)$最小，即$x^*=arg\mathop{min}_\limits {d(x^0,x’)\leq\epsilon}L(x’)$。</p><p>关于如何训练输入$x’$而非模型参数$\theta$，可以参考下Explainable AI的代码部分，其实就是设置输入$x’$的梯度是可追踪的并在定义优化器时传入输入$x’$而非模型参数$\theta$。</p><p><img src="https://pic1.zhimg.com/80/v2-cabb0848c34eca5fe5f4009181b49e7b_720w.png" alt="img"></p><p>如上图所示，在训练时有两个关键点。第一点是输入$x’$应该用$x^0$初始化；第二点是在每个iteration中需要判断保证$d(x^0,x’)\leq\epsilon$，具体来讲就是当发现$d(x^0,x’)&gt;\epsilon$时就需要将$x’$修正为所有满足$d(x^0,x)&gt;\epsilon$的$x$中与$x’$最接近的那个。那要怎么找到所有满足$d(x^0,x)&gt;\epsilon$的$x$中与$x’$最接近的那个呢？下图中圆形和正方形中的$x$均满足$d(x^0,x)&gt;\epsilon$，所以中心$x^0$与$x’$连线与圆形或正方形的交点就是要修正的结果。</p><p><img src="https://pic2.zhimg.com/80/v2-163a5c710e5ed504d06debfb7c509493_720w.png" alt="img"></p><h2 id="Attack-Approaches"><a href="#Attack-Approaches" class="headerlink" title="Attack Approaches"></a>Attack Approaches</h2><ul><li>FGSM (<a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">https://arxiv.org/abs/1412.6572</a>)</li><li>Basic iterative method (<a href="https://arxiv.org/abs/1607.02533" target="_blank" rel="noopener">https://arxiv.org/abs/1607.02533</a>) </li><li>L-BFGS (<a href="https://arxiv.org/abs/1312.6199" target="_blank" rel="noopener">https://arxiv.org/abs/1312.6199</a>)</li><li>Deepfool (<a href="https://arxiv.org/abs/1511.04599" target="_blank" rel="noopener">https://arxiv.org/abs/1511.04599</a>)</li><li>JSMA (<a href="https://arxiv.org/abs/1511.07528" target="_blank" rel="noopener">https://arxiv.org/abs/1511.07528</a>)</li><li>C&amp;W (<a href="https://arxiv.org/abs/1608.04644" target="_blank" rel="noopener">https://arxiv.org/abs/1608.04644</a>)</li><li>Elastic net attack (<a href="https://arxiv.org/abs/1709.04114" target="_blank" rel="noopener">https://arxiv.org/abs/1709.04114</a>)</li><li>Spatially Transformed (<a href="https://arxiv.org/abs/1801.02612" target="_blank" rel="noopener">https://arxiv.org/abs/1801.02612</a>) </li><li>One Pixel Attack (<a href="https://arxiv.org/abs/1710.08864" target="_blank" rel="noopener">https://arxiv.org/abs/1710.08864</a>)</li><li>……</li></ul><p>虽然有很多方法都可以进行attack，但它们的主要区别在于使用了不同的constraint或者使用了不同的optimization method。</p><h2 id="FGSM"><a href="#FGSM" class="headerlink" title="FGSM"></a>FGSM</h2><p>FGSM即Fast Gradient Sign Method，本次homework(hw6_AdversarialAttack)就使用了FGSM。</p><p>FGSM中输入的更新规则为$x^*=x^0-\epsilon\Delta x$。它首先计算出损失$L$关于$x$的每个维度的梯度，如果梯度大于0则修改为+1、小于0则修改为-1，也就是说$x^0$的所有维要么$+\epsilon$是要么是$-\epsilon$。</p><p>假设FGSM使用L-infinity计算$d(x^0,x^<em>)$，如果梯度指向左下角那么$x^</em>$就在方框的右上角；如果gradient指向左上角那么$x^<em>$就在方框的右下角；因此在FGSM中我们只在意梯度方向而不在意其大小。我们可以认为FGSM使用一个非常大的学习率使$x$飞出正方形，但因为要保证$d(x^0,x’)\leq\epsilon$所以$x^</em>$就会被限制到方形区域内部。所以就像是“一拳超人”，只攻击一次就达到好的效果。</p><p><img src="https://pic1.zhimg.com/80/v2-8c2bf52f38a95de4070288719e1eed4c_720w.png" alt="img"></p><h2 id="Black-Box-Attack"><a href="#Black-Box-Attack" class="headerlink" title="Black Box Attack"></a>Black Box Attack</h2><p>Attack可以分为White Box和Black Box。White Box Attack指<strong>模型参数$\theta$是已知的</strong>，Black Box Attack指<strong>模型参数$\theta$是未知的</strong>。</p><p>在Black Box Attack中，我们不知道Black Network的参数$\theta$。现假设我们知道Black Network的训练集，那我们就可以使用这份训练集自行训练出一个Proxy Network，然后基于Proxy Network和训练集就可以得到$x’$，这个$x’$一般也可以成功攻击Black Network。如果Black Network是一个在线API，我们既不知道Black Network的参数也不知道它的训练集，那上传大量输入后就可以得到大量对应的输出，并以这些输入输出对为训练集得到Proxy Network和$x’$。</p><p>有相关实验证明，Black Box Attack是非常有可能攻击成功的，详见：Delving into Transferable Adversarial Examples and Black-box Attacks(<a href="https://arxiv.org/abs/1611.02770" target="_blank" rel="noopener">https://arxiv.org/abs/1611.02770</a>)</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;我们希望我们的模型不仅仅是在大多数情况下可用的，还希望它能够应对来自外界的“
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="对抗攻击与防御" scheme="https://chouxianyu.github.io/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B8%8E%E9%98%B2%E5%BE%A1/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-11.5基于PyTorch的Explainabe AI实战</title>
    <link href="https://chouxianyu.github.io/2021/04/25/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-11-5%E5%9F%BA%E4%BA%8EPyTorch%E7%9A%84Explainabe-AI%E5%AE%9E%E6%88%98/"/>
    <id>https://chouxianyu.github.io/2021/04/25/李宏毅机器学习课程笔记-11-5基于PyTorch的Explainabe-AI实战/</id>
    <published>2021-04-25T01:48:58.000Z</published>
    <updated>2021-04-25T01:56:27.641Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework5的记录。</p><p>全部课程PPT、数据和代码下载链接：</p><p>链接：<a href="https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg" target="_blank" rel="noopener">https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg</a> 提取码：tpmc</p><p>代码仓库：<a href="https://github.com/chouxianyu/LHY_ML2020_Codes" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes</a></p><h1 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h1><p>在homework3中我们通过CNN实现了食物图片分类，详见我之前的文章《李宏毅机器学习课程笔记-7.4基于CNN和PyTorch的食物图片分类》，这次作业的任务就是探究这个CNN的可解释性，具体如下</p><ol><li><p><strong>Saliency Map</strong></p><p> 按照《Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps》，计算每个像素对最终分类结果的重要性。</p><p> 我們把一张图片输入到model，将model的输出与label进行对比计算得到loss，因此与loss相关的变量有image、model parameter和label这3者。</p><p> 通常情況下，我們训练模型时希望找到一组好的model parameter来拟合image和label，因此loss在backward时我们只在乎loss关于model parameter的梯度。但在数学上image本身也是continuous tensor，我们可以<strong>在model parameter和label都固定的情况下计算loss关于image的梯度，这个梯度代表稍微改变image的某个pixel value会对loss产生什么影响，我们习惯把这个影响的程度解读为该pixel对于结果的重要性（每个pixel都有自己的梯度）</strong>。</p><p> 因此将loss关于一张图片中每个pixel的梯度计算并画出来，就可以看出该图中哪些像素是model在计算结果时的重要依据。那如何用代码实现我们的这个想法呢？非常简单，在一般训练中我们都是在forward后计算模型输出与标签之间的loss，然后进行loss的backward，其实在PyTorch中这个backword计算的是loss对<strong>model parameter</strong>的梯度，因此我們只需要用一行代码<code>images.requires_grad_()</code>使得<strong>image</strong>也要被计算梯度。</p></li><li><p><strong>Filter Visualization</strong></p><p> 基于Gradient Ascent，实现Activation maximization，找到最能够激活某个filter的图片，以观察模型学到了什么。</p><p> 这里我们想要知道某一个filter到底学习到了什么，我们需要做两件事情：<strong>①Filter Visualization：挑几张图片看看某个filter的输出；②Filter Activation：看看什么图片可以最大程度地activate该filter</strong>。</p><p> 在代码实现方面，我们一般是直接把图片输入到model，然后直接forward，那要如何取出model中某层的输出呢？虽然我们可以直接修改model的forward函数使其返回某层的输出，但这样比较麻烦，还可能会因此改动其它部分的代码。因此PyTorch提供了方便的解决方法：<strong>hook</strong>。</p></li><li><p><strong>LIME</strong></p><p> 绿色代表一个component和结果正相关，红色则代表该component和结果负相关。</p><p> 《”Why Should I Trust You?”: Explaining the predictions of Any Classifier》</p><p> 注：根据助教的示例，我遇到了一个BUG<code>KeyError: &#39;Label not in explanation&#39;</code>，暂未解决……</p></li></ol><h1 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h1><p>使用homework3使用的数据集以及训练出的CNN模型。</p><h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><p><a href="https://github.com/utkuozbulak/pytorch-cnn-visualizations" target="_blank" rel="noopener">https://github.com/utkuozbulak/pytorch-cnn-visualizations</a></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework5的记录。&lt;/p&gt;
&lt;p&gt;全部课程PPT、数据和代码下载链接：&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg&quot; target
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
      <category term="ExplainableAI" scheme="https://chouxianyu.github.io/tags/ExplainableAI/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-11.4模型无关的局部解释(LIME)</title>
    <link href="https://chouxianyu.github.io/2021/04/24/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-11-4%E6%A8%A1%E5%9E%8B%E6%97%A0%E5%85%B3%E7%9A%84%E5%B1%80%E9%83%A8%E8%A7%A3%E9%87%8A-LIME/"/>
    <id>https://chouxianyu.github.io/2021/04/24/李宏毅机器学习课程笔记-11-4模型无关的局部解释-LIME/</id>
    <published>2021-04-24T02:36:52.000Z</published>
    <updated>2021-04-24T02:37:44.740Z</updated>
    
    <content type="html"><![CDATA[<p>模型无关的局部解释（Local Interpretable Model-Agnostic Explanations，LIME）指<strong>使用一个Interpretable Model拟合一个Uninterpretable Model的局部</strong>，比如使用Linear Model或者Decision Tree模拟Neural Network。</p><p>具体来讲，对于局部范围内的相同输入，我们希望两个模型的输出尽可能接近。这里要重点强调局部范围的概念，因为实际上Linear Model并不能模拟整个Neural Network但却可以模拟其中的一个Local Region，这也是LIME可行的原因。</p><h2 id="案例：使用Linear-Model拟合Neural-Network"><a href="#案例：使用Linear-Model拟合Neural-Network" class="headerlink" title="案例：使用Linear Model拟合Neural Network"></a>案例：使用Linear Model拟合Neural Network</h2><ol><li><p>定位：确定需要解释的data point(下图5个蓝点中最中间的蓝点)</p></li><li><p>取样：在上一步确定的data point周围sample获得更多的data point(下图5个蓝点)</p><p> sample的<strong>范围</strong>需要根据情况调整，一般范围小则拟合得更准确</p><p> sample的<strong>方法</strong>不同，结果也会不同</p></li><li><p>拟合：使用Linear Model模拟上一步确定的data point及其对应的Neural Network输出</p></li><li><p>解释：对Linear Model进行解释，进而解释Neural Network的局部</p></li></ol><p><img src="https://pic1.zhimg.com/80/v2-a29bafdc2312575cab096090938493b2_720w.png" alt="img"></p><h2 id="案例：LIME-For-Image-Classification"><a href="#案例：LIME-For-Image-Classification" class="headerlink" title="案例：LIME For Image Classification"></a>案例：LIME For Image Classification</h2><p>如何将LIME应用到Image Classification呢？假设有一张图片被分类为frog，下面只讲一些关键点。</p><ul><li><p>如何进行sample？</p><p>  首先可以将图片分成多个segment：$\{s_1,s_2,\dots,s_n\}$，随机去除其中的一些segment就可以得到该图片“周围”的一些图片</p></li><li><p>将LIME应用于图片时，一般要进行Feature Extraction，那如何做呢？</p><p>  使用$x_i$表示图片中的每个segment是否被删除，其中$i=1,…,n$，若$x_i$为1则表示该segment被删除，否则表示该segment未被删除</p></li><li><p>如何解释Linear Model？</p><p>  设Linear Model为$y=w_1x_1+..+w_nx_n$，$w_i$的值有以下3种情况</p><ul><li>$w_i\approx 0$表示$s_i$对分类为frog没有影响；</li><li>$w_i&gt; 0$表示$s_i$对分类为frog具有正面影响，即这个segment使得模型倾向于将图片分类为frog</li><li>$w_i&lt;0$表示$s_i$对分类为frog具有负面影响，即这个segment使得模型倾向于认为该图片不是frog类别</li></ul></li></ul><h2 id="Tree-Regularization"><a href="#Tree-Regularization" class="headerlink" title="Tree Regularization"></a>Tree Regularization</h2><p><strong>理论上可以用无深度限制的Decision Tree拟合完整的Neural Network，但Decision Tree的深度不可能没有限制</strong>，因此在使用Decision Tree拟合Neural Network时需要对Decision Tree的复杂度进行约束。</p><p>设Neural Network的参数为$\theta$、Decision Tree的参数为$T_\theta$，使用Decision Tree的平均深度表示其参数$T_\theta$的复杂度$O(T_\theta)$。在使用Decision Tree拟合Neural Network时，不仅要使两者输出相近还要使$O(T_\theta)$最小化，因此优化目标为$\theta^*=arg\ {\rm min}\ L(\theta) + \lambda O(T_\theta)$。</p><p>因此我们<strong>在训练神经网络时就可以使用Tree Regularization使得训练出的神经网络更具有可解释性</strong>，$O(T_\theta)$不能微分，解决方法详见《Beyond Sparsity: Tree Regularization of Deep Models for Interpretability》。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;模型无关的局部解释（Local Interpretable Model-Agnostic Explanations，LIME）指&lt;strong&gt;使用一个Interpretable Model拟合一个Uninterpretable Model的局部&lt;/strong&gt;，比如使用
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="ExplainableAI" scheme="https://chouxianyu.github.io/tags/ExplainableAI/"/>
    
      <category term="LIME" scheme="https://chouxianyu.github.io/tags/LIME/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-11.3Explainable AI(Global Explanation)</title>
    <link href="https://chouxianyu.github.io/2021/04/23/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-11-3Explainable-AI-Global-Explanation/"/>
    <id>https://chouxianyu.github.io/2021/04/23/李宏毅机器学习课程笔记-11-3Explainable-AI-Global-Explanation/</id>
    <published>2021-04-23T03:33:37.000Z</published>
    <updated>2021-04-23T03:34:13.996Z</updated>
    
    <content type="html"><![CDATA[<p>假定在图片分类任务中，<strong>Global Explanation要求机器说明它认为一个类别(比如“cat”)是什么样子，而非针对一张图片进行解释。</strong></p><h2 id="Activation-maximization"><a href="#Activation-maximization" class="headerlink" title="Activation maximization"></a>Activation maximization</h2><p>在<a href="https://zhuanlan.zhihu.com/p/361283328" target="_blank" rel="noopener">李宏毅机器学习课程笔记-7.2CNN学到了什么</a>一文中，我们已讲过Activation maximization，不再复述，这里讲一些相关的新知识。</p><ol><li>在Activation maximization的MNIST手写数字识别案例中，我们观察到机器学习到的数字在人类看来完全就是噪音，由此可以想到：将机器认为是数字(或其它事物)的内容作为噪声添加到其它数据中，也许这样就可以实现Attack。</li><li>在使用Activation maximization观察模型学习到的内容时，我们可能需要使用大量<strong>Regularization(保证“可解释性”)</strong>以及暴调超参数，详见《Understanding Neural Networks Through Deep Visualization》。</li></ol><h2 id="“Regularization”-From-Generator"><a href="#“Regularization”-From-Generator" class="headerlink" title="“Regularization” From Generator"></a>“Regularization” From Generator</h2><p><strong>除了使用人工设置的Regularization来告诉机器什么是一张正常的输出(比如image)，还可以使用Generator进行Regularization</strong>。</p><p>Image Generator的输入是一个低维向量$z$，其输出为一张图片$x$，即$x=G(z)$。通常这个低维向量$z$是从某个已知的distribution(比如高斯分布、正态分布)中sample出来的，我们可以收集很多图片并使用GAN或者VAE训练这个Generator。</p><p>那如何使用Image Generator生成对图片的限制呢？以图片分类为例，将Generator输出的图片$x$输入到Image Classifier中得到输出分类结果$y_i$，目标是找到一个$z^<em>$使得图片$x$属于对应类别$i$的可能性$y_i$最大，即$z^</em>=arg \ max y_i$。得到$z^<em>$之后将其输入至Image Generator就可以得到一个图片$x$。如果通过$x^</em>=arg \ max y_i$直接得到图片呢，则不能保证结果的“可解释性”，因而需要对结果进行“可解释性”的约束，上述过程中Generator的作用就是对图片进行约束以确保生成的图片$x$是“可解释”的。</p><p>注：在进行$z^*=arg \ max y_i$时，Image Generator和Classifier的参数是不参与本次训练的。</p><p>那使用Generator能得到什么样的结果呢？结果挺好的，详见《Plug &amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space》。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;假定在图片分类任务中，&lt;strong&gt;Global Explanation要求机器说明它认为一个类别(比如“cat”)是什么样子，而非针对一张图片进行解释。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;Activation-maximization&quot;&gt;&lt;a href=&quot;#Ac
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="ExplainableAI" scheme="https://chouxianyu.github.io/tags/ExplainableAI/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-11.2Explainable AI(Local Explanation)</title>
    <link href="https://chouxianyu.github.io/2021/04/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-11-2Explainable-AI-Local-Explanation/"/>
    <id>https://chouxianyu.github.io/2021/04/22/李宏毅机器学习课程笔记-11-2Explainable-AI-Local-Explanation/</id>
    <published>2021-04-22T00:40:25.000Z</published>
    <updated>2021-04-22T00:41:07.591Z</updated>
    
    <content type="html"><![CDATA[<p>假定在图片分类任务中有一张图片，<strong>Local Explanation</strong>则要求机器说明为什么它认为这张图片是某个类别(比如“cat”)。</p><p>Explainable AI(<strong>Local Explanation</strong>)的目标是，知道每个component对于最终结果的重要性。我们可以通过remove或者modify其中一个component，看decision会有什么变化。</p><h2 id="基于梯度判断Component重要性"><a href="#基于梯度判断Component重要性" class="headerlink" title="基于梯度判断Component重要性"></a>基于梯度判断Component重要性</h2><p>假设输入是$x$，它有很多component $\{x_1,x_2,\dots,x_N\}$组成。如果输入是image，则component一般是pixel、segment或patch等；如果输入是text，则component一般是word。对于图片，我们可以在图片上“放置”一个灰块以覆盖图像的一小部分，观察其对结果的影响，见《<strong>Visualizing <em>and</em> Understanding Convolutional Networks</strong>》。注：component的选取、remove或者modify也是需要研究的。</p><p>还有另一种方法是，输入为$\{x_1,…,x_n\}$，对某个pixel $x_n$加上$\Delta x$，用$\frac{\Delta y}{\Delta x}$来表示扰动$\Delta x$对结果$y$的影响，即通过$\frac{\partial y_k}{\partial x_n}$的绝对值表示某个pixel对$y_k$的影响，见《<strong>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</strong>》。</p><p>还有很多其它<strong>基于梯度来判断Component重要性</strong>的方法：</p><ul><li><p>Grad-CAM</p></li><li><p>SmoothGrad</p></li><li><p>Layer-wise Relevance Propagation(LRP)</p><p>  Redistribute the output, Backward propagation until reaching input</p></li><li><p>Guided Backpropagation</p></li></ul><h2 id="梯度饱和"><a href="#梯度饱和" class="headerlink" title="梯度饱和"></a>梯度饱和</h2><p>基于梯度来判断component重要性的方法也存在着局限性：<strong>梯度饱和(Gradient Saturation)和Noisy Gradient</strong>。</p><p>考虑$\frac{\partial大象}{\partial鼻子长度}$，可知在一定范围内，鼻子越长，则判定为大象的概率就越大，但随着鼻子长度增加到一定数值后，鼻子长度对于判定大象的影响几乎为0，这时就出现了梯度饱和，如下图所示。</p><p><img src="https://pic4.zhimg.com/80/v2-ccef0f3a481afd976ed03ce3e6a12484_720w.png" alt="img"></p><p>那如何解决梯度饱和的问题呢？解决方法就是<strong>Global Explanation</strong>，可以参考Integrated gradient和DeepLIFT。</p><p>相对于梯度饱和，另外一个问题就是<strong>Noisy Gradient</strong>，即Gradient变化非常大，解决方法是<strong>SmoothGrad</strong>（在计算梯度时添加噪声以扰动生成多个样本，并计算平均梯度）</p><h2 id="Attack-Interpretation"><a href="#Attack-Interpretation" class="headerlink" title="Attack Interpretation"></a>Attack Interpretation</h2><p>向输入中加入一些细微的噪声，这样并不影响视觉效果和模型的输出，但这样可以<strong>攻击explanation</strong>，如下图所示，详见《Interpretation of Neural Networks is Fragile》。</p><p><img src="https://pic2.zhimg.com/80/v2-704a838681754edb670b33ef0c228987_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;假定在图片分类任务中有一张图片，&lt;strong&gt;Local Explanation&lt;/strong&gt;则要求机器说明为什么它认为这张图片是某个类别(比如“cat”)。&lt;/p&gt;
&lt;p&gt;Explainable AI(&lt;strong&gt;Local Explanation&lt;/stron
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="ExplainableAI" scheme="https://chouxianyu.github.io/tags/ExplainableAI/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-11.1Explainable AI引言</title>
    <link href="https://chouxianyu.github.io/2021/04/21/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-11-1Explainable-AI%E5%BC%95%E8%A8%80/"/>
    <id>https://chouxianyu.github.io/2021/04/21/李宏毅机器学习课程笔记-11-1Explainable-AI引言/</id>
    <published>2021-04-21T01:48:40.000Z</published>
    <updated>2021-04-21T01:58:47.017Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Explainable-AI是什么"><a href="#Explainable-AI是什么" class="headerlink" title="Explainable AI是什么"></a>Explainable AI是什么</h2><p>我们希望，机器不仅要知道“是什么”还要知道“为什么”，或者说<strong>机器不仅要给出答案还要给出explanation</strong>。</p><p>Explanation可以分为两类：</p><ol><li><p><strong>Local Explanation</strong></p><p> 假定在图片分类任务中有一张图片，要求机器说明<strong>为什么它认为这张图片是某个类别(比如“cat”)</strong>。</p></li><li><p><strong>Global Explanation</strong></p><p> 假定在图片分类任务中，要求机器说明<strong>它认为一个类别(比如“cat”)是什么样子</strong>，而非针对一张图片进行解释。</p></li></ol><h2 id="Explainable-AI有什么用"><a href="#Explainable-AI有什么用" class="headerlink" title="Explainable AI有什么用"></a>Explainable AI有什么用</h2><p>在使用机器挑选简历时，我们需要知道机器为什么选择某份简历(性别?还是实力)。</p><p>在使用机器判定罪犯是否可以假释时，我们需要知道机器为什么判定是或否(实证?还是肤色)。</p><p>在使用机器判定是否给某人贷款时，我们需要知道机器为什么判定是或否。</p><p>通过Explainable AI，我们可以知道模型学到了什么从而进行模型诊断，对模型进行改进和调整。我们不仅只关注模型在数据集上的精确度，还需要进行模型诊断，因为有可能精确度很高但实际上机器什么都没学到。</p><h2 id="Explainable-AI是否有必要"><a href="#Explainable-AI是否有必要" class="headerlink" title="Explainable AI是否有必要"></a>Explainable AI是否有必要</h2><p>李宏毅老师认为Explainable AI的目标并非完全理解模型是如何work的，而是为了让人感到comfortable。</p><p>因为深度学习是一个黑盒所以有些人认为深度学习不可信，这有些因噎废食。人脑等很多事物对现在的人类来讲都也还是黑盒，完全理解模型的work机理不是必要的，因为某些东西是黑盒就不使用它也不行。</p><p>Explainable AI其实就是为了使老板、客户、自己等感到comfortable，甚至对不同人也应该有不同的解释。</p><h2 id="Interpretable-VS-Powerful"><a href="#Interpretable-VS-Powerful" class="headerlink" title="Interpretable VS Powerful"></a>Interpretable VS Powerful</h2><p>决策树既是interpretable又是powerful的，但当分支特别多的时候决策树的表现也会很差，这时可以使用Random Forest或者XGBoost，但它们虽然powerful但不interpretable。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Explainable-AI是什么&quot;&gt;&lt;a href=&quot;#Explainable-AI是什么&quot; class=&quot;headerlink&quot; title=&quot;Explainable AI是什么&quot;&gt;&lt;/a&gt;Explainable AI是什么&lt;/h2&gt;&lt;p&gt;我们希望，机器不仅要
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="ExplainableAI" scheme="https://chouxianyu.github.io/tags/ExplainableAI/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-10.4基于Smoothness假设的半监督学习</title>
    <link href="https://chouxianyu.github.io/2021/04/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-10-4%E5%9F%BA%E4%BA%8ESmoothness%E5%81%87%E8%AE%BE%E7%9A%84%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <id>https://chouxianyu.github.io/2021/04/19/李宏毅机器学习课程笔记-10-4基于Smoothness假设的半监督学习/</id>
    <published>2021-04-19T00:27:39.000Z</published>
    <updated>2021-04-19T00:29:53.557Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Smoothness假设"><a href="#Smoothness假设" class="headerlink" title="Smoothness假设"></a>Smoothness假设</h2><h3 id="Smoothness假设的定义"><a href="#Smoothness假设的定义" class="headerlink" title="Smoothness假设的定义"></a>Smoothness假设的定义</h3><p>基于Smoothness假设的半监督学习的基本思路是“近朱者赤近墨者黑”，即<strong>相似的$x$具有相同的$\hat y$</strong>，其具体<strong>定义</strong>为：</p><ol><li>$x$的<strong>分布不平均</strong>，在某些地方(high density region)很集中，在某些地方很分散</li><li>如果$x^1$和$x^2$在一个<strong>high density region</strong>中距离非常近，则$x^1$和$x^2$通过1个<strong>high density path</strong>相连、$\hat y^1=\hat y^2$。</li></ol><p>举一个例子，如下图所示，$x^1,x^2,x^3$是3个样本，如果单纯地看它们之间的相似度，显然$x^2$和$x^3$更接近一些。但对于smoothness assumption来说，$x^1$和$x^2$是位于同一个high density region中，它们之间有high density path；而$x^2$与$x^3$之间则是“断开”的，没有high density path，因此$x^1$与$x^2$更“像”。</p><p><img src="https://pic1.zhimg.com/80/v2-a45cf1b179557c638d294edf952053a2_720w.png" alt="img"></p><h3 id="手写数字识别举例"><a href="#手写数字识别举例" class="headerlink" title="手写数字识别举例"></a>手写数字识别举例</h3><p>再举一个手写数字识别和人脸识别的例子，如下图所示，最左侧的2、最右侧的2和最右侧的3，从<strong>pixel角度</strong>来看明显是最右侧的2和3更加相似(尽管两者并非是同一个数字)，但如果考虑最左侧的2朝着最右侧的2的演化过程，可以发现产生了一种“<strong>间接相似性</strong>”(high density path)。根据Smoothness假设，由于这6个2之间存在间接的相似而这6个2和最右侧的3之间不存在high density path，因此这6个2是彼此相似的；而最右侧的3和这6个2是不相似的。</p><p><img src="https://pic4.zhimg.com/80/v2-c8fd736cc00ef0209e973b93b62b2541_720w.png" alt="img"></p><h3 id="文章分类举例"><a href="#文章分类举例" class="headerlink" title="文章分类举例"></a>文章分类举例</h3><p>假设对天文学(astronomy)和旅行(travel)的文章进行分类，它们有各自的专属词汇，此时如果unlabeled data与label data的词汇是相同或重合(overlap)的，那么就很容易分类；但真实情况中unlabeled data和labeled data之间可能没有任何重复的word，因为世界上的词汇太多了，sparse的分布中overlap难以发生。</p><p>但如果unlabeled data足够多，就会以一种<strong>相似传递</strong>的形式，建立起文档之间相似的桥梁。</p><h2 id="cluster-and-then-label"><a href="#cluster-and-then-label" class="headerlink" title="cluster and then label"></a>cluster and then label</h2><p>如何实现基于Smoothness假设的半监督学习呢？在具体实现上，最简单的方式是cluster and then label。</p><p>cluster and then label就是先<strong>把所有样本(包括有标签样本和无标签样本)分成几个cluster，然后根据每个cluster中各类别有标签样本的数量确定该cluster中所有样本的label，然后进一步用这些cluster学习得到分类器</strong>。</p><p>这种方法不一定会得到好的结果，因为该方法有个<strong>前提是我们能够把同类别的样本分到同一个cluster</strong>，而这并不容易。对图像分类来说，如果仅仅依据pixel-wise相似度来划分cluster，得到的结果一般都会很差。所以为了满足这个前提，我们需要设计较好的方法来描述一张图片(比如使用Deep Autoencoder提取图片特征feature)，以保证cluster时能够将同类别的样本分到同一个cluster。</p><h2 id="Graph-based-Approach"><a href="#Graph-based-Approach" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h2><h3 id="high-density-path"><a href="#high-density-path" class="headerlink" title="high density path"></a>high density path</h3><p>如何实现基于Smoothness假设的半监督学习呢？我们可以将每个样本视为图中的1个点，<strong>通过图来表示connected by a high density path</strong>。Graph-based方法的基本思路是图中的有标注样本会影响与它们邻近的无标注样本并在图中产生“<strong>间接相似性</strong>”，即使某些无标注样本没有直接与有标注样本相连也仍然可以被判定为相似。如果想要让这种方法生效，<strong>收集到的数据一定要足够多</strong>，否则可能导致无法形成path、失去了information的传递效果。</p><h3 id="如何建立一张图"><a href="#如何建立一张图" class="headerlink" title="如何建立一张图"></a>如何建立一张图</h3><p>有时候点之间的边是比较好建立的(比如网页超链接、论文引用)，有时候需要我们自行建立点之间的边。图的好坏对最终结果的影响是非常关键的，但如何建图是一件heuristic的事情，需要我们凭经验和直觉来做，<strong>建图步骤</strong>如下：</p><ol><li><p>定义两个样本$x^i,x^j$之间的相似度计算方法$s(x^i,x^j)$</p><p> 如果是基于pixel-wise的相似度，那结果可能比较差，建议使用更好的方法(比如使用Autoencoder提取图片特征，并基于提取到的特征计算相似度)。</p><p> 推荐使用的相似度计算方法为高斯径向基函数(Gaussian Radial Basis Function)：$s(x^i,x^j)=exp(-\gamma||x^i-x^j||^2)$，其中$x^i,x^j$均为vector。经验上来说exp(exponential)通常是可以帮助提升性能的，因为它使得仅当$x^i,x^j$非常接近时similarity才会大、只要距离稍微远一点similarity就会迅速变小，也就是使用exponential可以做到<strong>只有非常近的两个点才能相连、稍微远一点就无法相连</strong>的效果。</p></li><li><p>建立点和点之间的边</p><ul><li><p>k-Nearest Neighbor(K近邻算法)</p><p>  在特征空间中，如果一个样本附近的k个最近样本的大多数属于某一类别，则该样本也属于这个类别</p></li><li><p>e-Neighborhood</p></li></ul></li><li><p>设置点和点之间边的权重</p><p> 一条边的权重应该和该边两个顶点的相似度成比例</p></li></ol><h3 id="如何定量地评估一个图符合Smoothness假设的程度"><a href="#如何定量地评估一个图符合Smoothness假设的程度" class="headerlink" title="如何定量地评估一个图符合Smoothness假设的程度"></a>如何定量地评估一个图符合Smoothness假设的程度</h3><p>那如何定量地评估一个图符合Smoothness假设的程度呢？</p><p><img src="https://pic1.zhimg.com/80/v2-db490c6a659a6fb1765c956d9f584cac_720w.png" alt="img"></p><p>如上图所示，我们定义1个图的<strong>Smoothness</strong>为$S=\frac{1}{2}\sum_{i,j}w_{i,j}(y^i-y^j)^2$并希望它<strong>越小越好</strong>，其中$i,j$为所有样本点的索引、$x^i$表示样本的特征、$y^j$表示样本的标注(有可能是伪标签)、$w_{i,j}$是2个样本之间边的权重、$\frac{1}{2}$只是为了方便计算。</p><p>上式Smoothness定义还可以表示为$S=y^TLy$，其中$y$是1个(R+U)-dim的vector：$y=[\dots,y^i,\dots,y^j,\dots]^T$、$L$是1个(R+U)×(R+U)的矩阵(名字叫做Graph Laplacian)。</p><p>Graph Laplacian的定义为$L=D-W$，其中$W_{i,j}$为2个样本点之间边的权重，把$W$每行元素之和放在该行对应的对角线上其它元素值为0即可得到$D$，如上图所示。在图神经网络(Spectral-based Convolution)中，有关于Graph Laplacian的介绍。</p><h3 id="如何训练"><a href="#如何训练" class="headerlink" title="如何训练"></a>如何训练</h3><p>Smoothness定义$S=y^TLy$中$y$是模型的输出(预测得到的类别)，它是取决于模型参数的，因此训练时仅需在有标注数据的损失函数上加上Smoothness即可：$L=\sum_{x^r}C(y^r,\hat y^r)+\lambda S$，其中$\lambda S$可以被视为正则项。</p><p>由上可知，<strong>训练目标</strong>为：</p><ol><li>有标注数据的交叉熵越小越好，即模型的输出与标注越接近越好</li><li>所有数据的Smoothness越小越好，即不管是有标注数据还是无标注数据，模型输出都要符合Smoothness Assumption的假设</li></ol><p>注：训练时可以不仅仅要求整个模型的输出层要smooth，还可以对模型中任意一个隐藏层加上smooth的限制。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Smoothness假设&quot;&gt;&lt;a href=&quot;#Smoothness假设&quot; class=&quot;headerlink&quot; title=&quot;Smoothness假设&quot;&gt;&lt;/a&gt;Smoothness假设&lt;/h2&gt;&lt;h3 id=&quot;Smoothness假设的定义&quot;&gt;&lt;a href=
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="半监督学习" scheme="https://chouxianyu.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-10.3基于Low-density Separation假设的半监督学习</title>
    <link href="https://chouxianyu.github.io/2021/04/18/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-10-3%E5%9F%BA%E4%BA%8ELow-density-Separation%E5%81%87%E8%AE%BE%E7%9A%84%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <id>https://chouxianyu.github.io/2021/04/18/李宏毅机器学习课程笔记-10-3基于Low-density-Separation假设的半监督学习/</id>
    <published>2021-04-18T07:17:34.000Z</published>
    <updated>2021-04-18T07:18:14.773Z</updated>
    
    <content type="html"><![CDATA[<p>按照“非黑即白”的思路，假设类别之间的boundary周围的data是很少的，即<strong>假设不同类别数据之间应该有1个很明显的boundary</strong>。</p><h3 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h3><p>最简单的基于Low-density Separation假设的半监督学习是Self-training。</p><ol><li><p>使用有标签的数据训练1个模型$f^*$，模型类型和训练方式没有限制，神经网络、深或浅、其它机器学习方法等等都可以</p></li><li><p>使用模型$f^<em>$生成未标注数据的伪标签(Pseudo-label)，即$y^u=f^</em>(x^u)$</p></li><li><p>取出一部分未标注数据将它们添加到有标签数据中，然后回到步骤1</p><p> 如何选择未标注数据仍然是一个open question，可以自行设计策略，比如给每个样本一个置信度。</p></li></ol><p>Self-training和<strong>生成模型中的半监督学习</strong>(见上1篇文章)还挺像的，它们的区别在于：</p><ol><li>Self-training使用<strong>hard label</strong>，即假定某个无标签样本一定属于某个类别(“非黑即白”)</li><li>生成模型使用<strong>soft label</strong>，即假定某个无标签样本有一定概率属于某类别(也可以理解为一个样本可以按照后验概率划分成多个部分，不同部分属于不同类别)</li></ol><p>Self-training使用了hard label，它并不适用于regression。</p><p>生成模型使用了soft label，它生成的伪标签在分类任务中是没有用的。因为把某个无标签样本(通过soft label生成伪标签)丢进模型重新训练模型，模型参数根本不会发生变化。</p><p>实际上，low-density separation就是<strong>通过hard label来提升分类效果</strong>的方法。</p><h3 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h3><p>该方法是Self-training的进阶版。</p><p>Self-training中使用的hard label还是有些武断和激进，Entropy-based Regularization对此进行了改进。</p><p><strong>在使用神经网络进行分类时，$y^u=f^<em>_{\theta^</em>}(x^u)$，其中$y_u$是1个one-hot编码。现在我们并不限制其必须是某个类别，而是将其看做1个分布，我们希望这个分布越集中越好(“非黑即白”)，因为分布越集中时它的含义就是样本$x^u$属于某类别的概率很大属于其它类别的概率很小</strong>。</p><p>我们可以使用Entropy评估分布$y^u$的集中程度$E(y^u)=-\sum_{m=1}^5y_m^uln(y_m^u)$，假设是5分类，其值越小则表示分布$y^u$越集中。</p><p>无监督分类的目标为有标签数据分类正确、无标签数据分类结果集中，所以损失函数则为$L=\sum_{x^r}C(y^r,\hat y^r)+\lambda\sum_{x^u}E(y^u)$，其中第1项为有标签数据的交叉熵损失、第2项为无标签数据的entropy、$\lambda$表示无标签数据的损失权重，因为式中第2项的作用类似于regularization，所以该方法被称为Entropy-based Regularization。</p><h3 id="Semi-supervised-SVM"><a href="#Semi-supervised-SVM" class="headerlink" title="Semi-supervised SVM"></a>Semi-supervised SVM</h3><p>SVM为两个类别的数据找到一个boundary，该boundary与两个类别的margin最大、分类错误最小。</p><p>Semi-supervised SVM穷举所有无标签数据的类别并进行计算，最终选择与两个类别的margin最大、分类错误最小的boundary。</p><p>在数据量大的时候，Semi-supervised SVM难以穷举出所有情况，但还有一种求近似解的方法，其大致思路是初始化一些label，然后每次尝试改动1个样本的label并判断是否更优，如果更优则改变该样本的label，具体见Transductive Inference for Text Classification using Support Vector Machines。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;按照“非黑即白”的思路，假设类别之间的boundary周围的data是很少的，即&lt;strong&gt;假设不同类别数据之间应该有1个很明显的boundary&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&quot;Self-training&quot;&gt;&lt;a href=&quot;#Self-training&quot;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="半监督学习" scheme="https://chouxianyu.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-10.2生成模型中的半监督学习</title>
    <link href="https://chouxianyu.github.io/2021/04/17/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-10-2%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <id>https://chouxianyu.github.io/2021/04/17/李宏毅机器学习课程笔记-10-2生成模型中的半监督学习/</id>
    <published>2021-04-17T02:46:22.000Z</published>
    <updated>2021-04-17T02:49:17.526Z</updated>
    
    <content type="html"><![CDATA[<p>生成模型中的半监督学习：Semi-supervised Learning for Generative Model</p><h2 id="有监督生成模型"><a href="#有监督生成模型" class="headerlink" title="有监督生成模型"></a>有监督生成模型</h2><p>有监督生成模型：Supervised Generative Model</p><p>如下图所示，在有监督生成模型中，得到$P(C_1),P(C_2),\mu^1,\mu^2,\Sigma$后，就可以计算出$x$属于类别$C_i$的概率$P(C_i|x)$。</p><p><img src="https://pic4.zhimg.com/80/v2-ee9b2801a6e660c4526a62103c10d2e0_720w.png" alt="img"></p><h2 id="半监督生成模型"><a href="#半监督生成模型" class="headerlink" title="半监督生成模型"></a>半监督生成模型</h2><p>半监督生成模型：Semi-supervised Generative Model</p><p>基于有监督生成模型，当有了无标签数据之后(下图中绿色圆点)，我们会明显发现有监督生成模型中的$P(C_1),P(C_2),\mu^1,\mu^2,\Sigma$并不够正确，比如2个类别的分布应该接近于下图中虚线圆圈、先验概率$P(C_1)$应该小于$P(C_2)$，所以应该使用无标签数据重新估计$P(C_1),P(C_2),\mu^1,\mu^2,\Sigma$。</p><p><img src="https://pic4.zhimg.com/80/v2-a334756241f389f2ca620c5f35ab5269_720w.png" alt="img"></p><h3 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h3><p>具体来讲，按照以下步骤进行计算：</p><ol><li><p>初始化参数：$\theta=\{P(C_1),P(C_2),\mu^1,\mu^2,\Sigma\}$</p><p> 可以随机初始化，也可以用有标签数据估算</p></li><li><p>通过$\theta$计算每个样本$x^u$属于类别$C_i$的概率$P_\theta(C_i|x^u)$</p></li><li><p><strong>更新参数$\theta$</strong>（其实重点就是如何同时利用有标签数据和无标签数据实现半监督）</p><ul><li>$P(C_1)=\frac{N_1+\sum_{x^u}P(C_1|x^u)}{N}$，其中$N$是所有样本的数量、$N_1$是属于类别$C_1$的样本的数量。</li><li>$\mu^1=\frac{1}{N_1}\sum_{x^r\in C_1}x^r+\frac{1}{\sum_{x^u}P(C_1|x^u)}\sum_{x^u}P(C_1|x^u)x^u$，其中$x^r,x^u$分别指有标签的样本和无标签的样本</li></ul><p>同理可知其它参数的计算和更新方法</p></li><li><p>返回第2步</p></li></ol><p>理论上，上述步骤是可以收敛的，但参数$\theta$的初始化值会影响结果。其实上面的第2步是EM算法中的E，第3步是EM算法中的M。</p><h3 id="理论推导"><a href="#理论推导" class="headerlink" title="理论推导"></a>理论推导</h3><p>$\theta=\{P(C_1),P(C_2),\mu^1,\mu^2,\Sigma\}$</p><ul><li><p>Maximum likelihood with labelled data</p><p>  使得$logL(\theta)=\sum_{x^r}logP_\theta(x^r, \hat y^r)$最大(有一个Closed-form solution)，其中每个有标注样本$x^r$的$P_\theta(x^r,\hat y^r)=P_\theta(x^r|\hat y^r)P(\hat y^r)$。</p></li><li><p>Maximum likelihood with labelled &amp; unlabeled data</p><p>  使得$logL(\theta)=\sum_{x^r}logP_\theta(x^r, \hat y^r)+\sum_{x^u}logP_\theta(x^u)$最大(该式并不是凹函数，所以需要迭代求解)，其中每个无标注样本$x^u$的$P_\theta(x^u)=P_\theta(x^u|C_1)P(C_1)+P_\theta(x^u|C_2)P(C_2)$</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;生成模型中的半监督学习：Semi-supervised Learning for Generative Model&lt;/p&gt;
&lt;h2 id=&quot;有监督生成模型&quot;&gt;&lt;a href=&quot;#有监督生成模型&quot; class=&quot;headerlink&quot; title=&quot;有监督生成模型&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="半监督学习" scheme="https://chouxianyu.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概率生成模型" scheme="https://chouxianyu.github.io/tags/%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-10.1半监督学习简介</title>
    <link href="https://chouxianyu.github.io/2021/04/16/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-10-1%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/"/>
    <id>https://chouxianyu.github.io/2021/04/16/李宏毅机器学习课程笔记-10-1半监督学习简介/</id>
    <published>2021-04-16T00:22:16.000Z</published>
    <updated>2021-04-16T00:50:25.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="有监督学习-Supervised-Learning"><a href="#有监督学习-Supervised-Learning" class="headerlink" title="有监督学习(Supervised Learning)"></a>有监督学习(Supervised Learning)</h2><p>训练集数据为$\{ (x^r,\ \hat y^r) \}_{r=1}^R$，其中每组数据包括算法的输入与输出(标签)。</p><h2 id="半监督学习-Semi-supervised-Learning"><a href="#半监督学习-Semi-supervised-Learning" class="headerlink" title="半监督学习(Semi-supervised Learning)"></a>半监督学习(Semi-supervised Learning)</h2><p>训练集数据为$\{ (x^r,\ \hat y^r) \}_{r=1}^R+\{ x^u\}_{u=R+1}^{U+R}$，即其中部分数据有标签而大量数据没有标签($U&gt;&gt;R$)。</p><p>半监督学习可以分为以下2种情况</p><ol><li><p><strong>Transductive Learning</strong></p><p> unlabeled data is the testing data，只使用testing data中的feature，并没有使用testing data中的label，所以并没有cheating。</p><p> 适用于已知testing data的情况，比如kaggle比赛。</p></li><li><p><strong>Inductive Learning</strong></p><p> unlabeled data is not the testing data，完全不使用testing data。</p><p> 适用于testing data未知的情况，这是大多数情况。</p></li></ol><h2 id="为什么需要半监督学习"><a href="#为什么需要半监督学习" class="headerlink" title="为什么需要半监督学习"></a>为什么需要半监督学习</h2><p>其实缺的并不是数据，缺少的是有标签的数据。利用这些大量的没有标签的数据进行学习，这是非常有价值的。</p><h2 id="为什么半监督学习有用"><a href="#为什么半监督学习有用" class="headerlink" title="为什么半监督学习有用"></a>为什么半监督学习有用</h2><p>The <strong>distribution</strong> of the unlabeled data tell us something：无标注数据的分布可以告诉我们一些东西</p><p><img src="https://pic3.zhimg.com/80/v2-676d19916cb98d7f251ff22a08eec087_720w.png" alt="img"></p><p><strong>半监督学习往往伴随着假设，而该假设的合理与否决定了结果的好坏程度。</strong>如上图所示，在猫狗图片分类中一只狗被认为是一只猫，这很可能是由于这2张图片的背景都是绿色，因此假设的合理性至关重要。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;有监督学习-Supervised-Learning&quot;&gt;&lt;a href=&quot;#有监督学习-Supervised-Learning&quot; class=&quot;headerlink&quot; title=&quot;有监督学习(Supervised Learning)&quot;&gt;&lt;/a&gt;有监督学习(Supe
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="半监督学习" scheme="https://chouxianyu.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.6基于RNN和PyTorch的文本情感分类</title>
    <link href="https://chouxianyu.github.io/2021/04/15/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-6%E5%9F%BA%E4%BA%8ERNN%E5%92%8CPyTorch%E7%9A%84%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"/>
    <id>https://chouxianyu.github.io/2021/04/15/李宏毅机器学习课程笔记-9-6基于RNN和PyTorch的文本情感分类/</id>
    <published>2021-04-15T00:22:18.000Z</published>
    <updated>2021-04-15T00:45:28.163Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework4的记录。</p><p>全部课程PPT、数据和代码下载链接：</p><p>链接：<a href="https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg" target="_blank" rel="noopener">https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg</a> 提取码：tpmc</p><p>代码仓库：<a href="https://github.com/chouxianyu/LHY_ML2020_Codes" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes</a></p><ul><li><p>任务描述</p><p>  通过RNN实现文本情感分类(Text Sentiment Classification)。</p></li><li><p>数据集描述</p><p>  输入是1个句子，输出是0(负面)或1(正面)。</p><p>  训练集：标注数据20万，无标注数据120万</p><p>  测试集：20万(无标注)</p></li><li><p>数据格式</p><ul><li>training_label.txt：<code>label +++$+++ sentence</code>，其中<code>+++$+++</code>只是分隔符</li><li>training_nolabel.txt：每一行就是一个句子，没有label</li><li>testing_data.txt：</li></ul></li><li><p>数据预处理</p><p>  一个句子(sentence)中有多个word，我们需要通过<strong>Word Embedding</strong>(我的其它文章里有介绍)用一个vector表示一个word， 然后使用RNN得到一个表示该sentence的vector。</p></li><li><p>半监督学习</p><p>  这里使用一种半监督学习方法：<strong>Self-Training</strong>(我的其它文章里有介绍)。使用有标签数据训练好模型，然后对无标签数据进行预测，并根据预测结果对无标签数据进行标注(“伪标签”)并继续训练模型</p></li><li><p>第三方库</p><p>  使用Python第三方库<code>gensim</code>实现word2vec模型，以进行Word Embedding。</p></li><li><p>代码</p><p>  <a href="https://github.com/chouxianyu/LHY_ML2020_Codes/tree/master/hw4_RNN" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes/tree/master/hw4_RNN</a></p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework4的记录。&lt;/p&gt;
&lt;p&gt;全部课程PPT、数据和代码下载链接：&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg&quot; target
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="LSTM" scheme="https://chouxianyu.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.5详解基于LSTM的RNN</title>
    <link href="https://chouxianyu.github.io/2021/04/14/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-5%E8%AF%A6%E8%A7%A3%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84RNN/"/>
    <id>https://chouxianyu.github.io/2021/04/14/李宏毅机器学习课程笔记-9-5详解基于LSTM的RNN/</id>
    <published>2021-04-14T02:38:35.000Z</published>
    <updated>2021-04-14T02:40:15.998Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1层LSTM神经元的架构"><a href="#1层LSTM神经元的架构" class="headerlink" title="1层LSTM神经元的架构"></a>1层LSTM神经元的架构</h2><p>根据上述内容，你可能看不出LSTM与RNN有什么关系，接下来具体介绍LSTM在RNN中的应用。</p><p>假设我们现在有一些LSTM（下图中白色部分）作为神经元，每个LSTM的memory cell里都存了一个scalar值（下图中红框中内容），把这些scalar连接起来就组成了1个vector $c^{t-1}$，即关于上个input（时间点为t-1）的memory。</p><p><img src="https://pic4.zhimg.com/80/v2-d96ed6d363d633b87729a22badf815a9_720w.png" alt="img"></p><p><strong>在时间点t，输入为1个vector $x^t$，它会经过4个线性的transform得到$z^f,z^i,z,z^o$，$z^f,z^i,z,z^o$这4个vector的dimension数量和LSTM神经元的数量相等，这4个vector的1个dimension即为1个LSTM神经元的输入（4个vector的第1个dimension为第1个LSTM神经元的输入）。</strong></p><h2 id="1个LSTM神经元的运算方法"><a href="#1个LSTM神经元的运算方法" class="headerlink" title="1个LSTM神经元的运算方法"></a>1个LSTM神经元的运算方法</h2><p>下图是单个LSTM神经元的运算方法，其4个input分别是$z$、$z^i$、$z^f$和$z^o$的其中1维（1维为1个神经元的输入）。每个LSTM神经元的input是各不相同的，但它们可以共同运算。</p><p>1个LSTM神经元的运算方法如下图所示。</p><p><img src="https://pic3.zhimg.com/80/v2-bc5c546a495db0ef197f4527f841562c_720w.png" alt="img"></p><p>$f(z^f)$与上一个时间点的memory $c^{t-1}$对应的cell值相乘，加上$g(z)$与$f(z^i)$的乘积，得到该时刻该cell中的值$c^t$，最终再乘以output gate的信号$f(z^o)$，得到输出$y^t$。</p><h2 id="1个LSTM神经元在相邻时刻时的运算方法"><a href="#1个LSTM神经元在相邻时刻时的运算方法" class="headerlink" title="1个LSTM神经元在相邻时刻时的运算方法"></a>1个LSTM神经元在相邻时刻时的运算方法</h2><p><img src="https://pic1.zhimg.com/80/v2-993d9ee86e0180e96e405e6b3248b41f_720w.png" alt="img"></p><p>上图是同1个LSTM神经元在2个相邻时刻的运算方法，其中与前文描述略有不同的是，这里还需要把当前时刻该神经元的输出$y^t$以及该神经元中cell保存的值$c^t$（peephole）都连接到下一时刻的输入上。因此在$t+1$时刻，神经元不只是考虑当前的输入$x^{t+1}$，还要看前一时刻该神经元的输出$h^t$和cell保存值$c^t$。</p><p>如何考虑结合$t+1$时刻的输入$x^{t+1}$和上一时刻该神经元的信息$h^t,c^t$呢？====&gt;<strong>把$x^{t+1}$、$h^t$和$c^t$这3个vector并在一起</strong>，乘上4个不同的转换矩阵，得到该神经元$t+1$时刻的4个输入$z$、$z^i$、$z^f$、$z^o$。</p><h2 id="多层LSTM在相邻时刻的运算方法"><a href="#多层LSTM在相邻时刻的运算方法" class="headerlink" title="多层LSTM在相邻时刻的运算方法"></a>多层LSTM在相邻时刻的运算方法</h2><p><img src="https://pic2.zhimg.com/80/v2-a6435474533a8871ad59ed5443055159_720w.png" alt="img"></p><p>上图中左边一列的2个LSTM代表2层LSTM，右边一列的2个LSTM则代表它们在下一时刻的状态。即横向是时间轴，纵向是层轴。</p><p>虽然看起来很复杂，感觉不一定work，但LSTM在RNN中已成为了标准做法。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1层LSTM神经元的架构&quot;&gt;&lt;a href=&quot;#1层LSTM神经元的架构&quot; class=&quot;headerlink&quot; title=&quot;1层LSTM神经元的架构&quot;&gt;&lt;/a&gt;1层LSTM神经元的架构&lt;/h2&gt;&lt;p&gt;根据上述内容，你可能看不出LSTM与RNN有什么关系，接下来
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="LSTM" scheme="https://chouxianyu.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.4LSTM入门</title>
    <link href="https://chouxianyu.github.io/2021/04/13/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-4LSTM%E5%85%A5%E9%97%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/13/李宏毅机器学习课程笔记-9-4LSTM入门/</id>
    <published>2021-04-13T00:35:26.000Z</published>
    <updated>2021-04-13T00:47:38.903Z</updated>
    
    <content type="html"><![CDATA[<p>LSTM即Long Short-term Memory。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>前几篇文章提到的RNN都比较简单，可以任意读写memory，没有进一步对memory进行管理。<strong>现在常用的memory管理方式是LSTM</strong>。正如其名，LSTM是比较长的短期记忆，<code>-</code>是在short和term之间。<strong>前几篇提到的RNN在有新的输入时都会更新memory，这样的memory是非常短期的，而LSTM中可以有更久之前的memory</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-2d2d4f677d917a001fcf379e00472bbf_720w.png" alt="img"></p><p>如上图所示，LSTM中有3个gate、4个输入(3个gate控制信号和1个想要写入memory cell的值)和1个输出：</p><ul><li>input gate：当某个neuron的输出想要被写进memory cell，它要先经过input gate。如果input gate是关闭的，则任何内容都无法被写入。input gate的关闭与否、什么时候开闭是由神经网络学习到的。</li><li>output gate：output gate决定了外界是否可以从memory cell中读取数据。当output gate关闭的时候，memory里面的内容无法被读取。output gate的关闭与否、什么时候开闭也是由神经网络学习到的。</li><li>forget gate：forget gate决定什么时候需要把memory cell里存放的内容忘掉，什么时候要保存。这也是由神经网络学习到的。</li></ul><h2 id="LSTM计算式"><a href="#LSTM计算式" class="headerlink" title="LSTM计算式"></a>LSTM计算式</h2><p>下图展示了LSTM的计算式。</p><p><img src="https://pic1.zhimg.com/80/v2-de00669f5a90c477fb32b2fffb71571d_720w.png" alt="img"></p><ul><li>$z$是想要被存到memory cell里的值</li><li>$z_i$是input gate的控制信号</li><li>$z_o$是output gate的控制信号</li><li>$z_f$是forget gate的控制信号</li><li>$a$是综合上述4个输入得到的输出值</li></ul><p>$z$、$z_i$、$z_o$和$z_f$通过激活函数分别得到$g(z)$、$f(z_i)$、$f(z_o)$和$f(z_f)$，其中$z_i$、$z_o$和$z_f$的激活函数$f()$一般会选sigmoid函数，因为其输出在0~1之间，可表示gate的开启程度。</p><p>令$g(z)$与$f(z_i)$相乘得到$g(z)f(z_i)$，然后把原先存放在memory cell中的$c$与$f(z_f)$相乘得到$cf(z_f)$，两者相加得到存在memory cell中的新值$c’=g(z)f(z_i)+cf(z_f)$。</p><ul><li><p>若$f(z_i)=0$，则相当于并不使用输入$z$更新memory；若$f(z_i)=1$，则相当于直接输入$g(z)$。</p></li><li><p>若$f(z_f)=1$，则不忘记memory cell中的原值$c$；若$f(z_f)=0$，则原值$c$将被遗忘清除。</p><p>  可以看出，forget gate的逻辑与直觉是相反的，该控制信号打开表示记得原值，关闭却表示遗忘。这个gate取名为remember gate更好些。</p></li></ul><p>此后，$c’$通过激活函数得到$h(c’)$，与output gate的$f(z_o)$相乘，得到输出$a=h(c’)f(z_o)$。</p><h2 id="Apply-LSTM-to-NN"><a href="#Apply-LSTM-to-NN" class="headerlink" title="Apply LSTM  to NN"></a>Apply LSTM  to NN</h2><p>上述的LSTM应该如何应用于神经网络呢？其实直接把LSTM作为1个神经元就可以了。假设输入层有2个标量输入$x_1,x_2$，隐藏层中有2个神经元，每个神经元输出1个标量，则其结构如下图所示。</p><p><img src="https://pic3.zhimg.com/80/v2-af52398f38860c122b0741e07ebb3dbd_720w.png" alt="img"></p><ul><li><strong>标量输入$x_1,x_2$乘以4个参数得到4个值，这4个值作为LSTM的4个input</strong>。</li><li>在普通的神经元中，1个input对应1个output；而在LSTM中4个input才产生1个output，并且所有的input都是不相同的。</li><li>LSTM所需要的参数量是普通NN的4倍。</li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;LSTM即Long Short-term Memory。&lt;/p&gt;
&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;前几篇文章提到的RNN都比较简单，可以任意读写mem
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="LSTM" scheme="https://chouxianyu.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.3RNN的应用</title>
    <link href="https://chouxianyu.github.io/2021/04/12/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-3RNN%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/12/李宏毅机器学习课程笔记-9-3RNN的应用/</id>
    <published>2021-04-12T00:54:54.000Z</published>
    <updated>2021-04-12T01:02:36.510Z</updated>
    
    <content type="html"><![CDATA[<p>在Slot Filling中，输入是1个word vector，输出是每个word的label，<strong>输入和输出是等长的</strong>。</p><p>然而，RNN还可以实现多对1、多对多…</p><h2 id="Many-to-One"><a href="#Many-to-One" class="headerlink" title="Many to One"></a>Many to One</h2><p>Many to One：输入是1个vector sequence，输出是1个vector</p><ul><li><p>Sentiment Analysis</p><p>  输入1篇文章或1句话等（1个vector sequence），输出其情感倾向（分类或者回归，比如超好、好、普通、差、超差、[-1,1]）。</p></li><li><p>Key Term Extraction</p><p>  输入是1篇文章等，输出是几个关键词。</p></li></ul><h2 id="Many-to-Many"><a href="#Many-to-Many" class="headerlink" title="Many to Many"></a>Many to Many</h2><p><strong>Many to Many：输入和输出都是sequence，但输出更短</strong></p><p>比如Speech Recognition。输入是1段声音信号，每隔1小段时间（通常很短，比如0.01秒）就用1个vector表示，输出是1段文字。因此输入是1个vector sequence，而输出是1个charactor sequence，并且<strong>输入序列要比输出序列短</strong>。</p><p>如果仍然使用Slot Filling的方法，就只能做到输入的每个vector对应输出1个character，输入1句“好棒”的语音后可能输出文字“好好棒棒棒”，但其实应该输出文字“好棒”。我们可以通过<strong>Trimming</strong>去除输出中相同的character，但语音“好棒”和语音“好棒棒”是不同的，应该如何区分呢？可以用<strong>CTC(Connectionist Temporal Classification)</strong>，其基本思路是可以在输出中填充NULL，最终输出时删除NULL即可。</p><p><img src="https://pic2.zhimg.com/80/v2-c135e0952e8aa68cf0831e64c0cb0105_720w.png" alt="img"></p><p>如上图所示，输入中vector的数量多于label中character的数量，那CTC应该怎么训练呢？答案是假设所有的可能性都是对的。</p><h2 id="Many-to-Many-1"><a href="#Many-to-Many-1" class="headerlink" title="Many to Many"></a>Many to Many</h2><p><strong><a href="http://www.oalib.com/paper/4068742" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a>：输入和输出都是sequence，但两者长度不确定。</strong></p><p>以机器翻译为例，RNN要将英文的word sequence翻译成中文的character sequence（并不知道哪个sequence更长或更短）。</p><p><img src="https://pic1.zhimg.com/80/v2-67cb0355203c2db204efaba8eff6b2f2_720w.png" alt="img"></p><p>如上图所示，假设RNN的输入“machine learning”，在2个时间点分别输入”machine”和”learning”，在最后1个时间点时memory中就存储了整个word sequence的信息。接下来让RNN输出，得到“机”，然后把“机”当做input（这1步有很多极技巧，这里省略），并读取memory中的信息，就会输出“器”，以此类推，RNN会一直输出但不知道什么时候停止。那怎么让RNN停止输出呢？可以添加1个symbol<code>===</code>标志停止，当RNN输出这个symbol时就停止输出。</p><h2 id="Seq2Seq-for-Syntatic-Parsing"><a href="#Seq2Seq-for-Syntatic-Parsing" class="headerlink" title="Seq2Seq for Syntatic Parsing"></a>Seq2Seq for Syntatic Parsing</h2><p><a href="http://arxiv.org/abs/1412.7449" target="_blank" rel="noopener">Grammar as a Foreign Langauage</a>： 输入为1个word sequence，输出1个语法树（可以用sequence表示）。</p><h2 id="Seq2Seq-Auto-encoder-for-Text"><a href="#Seq2Seq-Auto-encoder-for-Text" class="headerlink" title="Seq2Seq Auto-encoder for Text"></a>Seq2Seq Auto-encoder for Text</h2><p>如果用bag-of-word来表示1段文本，就容易丢失word之间的联系和语序上的信息。比如“白血球消灭了感染病”和“感染病消灭了白血球”这2段文本语义完全相反但bag-of-word是相同的。</p><p><a href="https://arxiv.org/abs/1506.01057" target="_blank" rel="noopener">A Hierarchical Neural Autoencoder for Paragraphs and Documents</a>：可以使用Seq2Seq Autoencoder，在考虑语序的情况下把文章编码成vector，只需要将RNN作为编码器和解码器即可。</p><p><img src="https://pic2.zhimg.com/80/v2-58c2031f882fd46db6171384e461756d_720w.png" alt="img"></p><p>如上图所示，word sequence输入RNN后被编码成embedded vector，然后再通过另1个RNN解码，如果解码后能得到一模一样的句子，则编码得到的vector就表示了这个word sequence中最重要的信息。</p><p><img src="https://pic4.zhimg.com/80/v2-e67956b01d8f5c5c2bd4b209a9c816ff_720w.png" alt="img"></p><p>如上图所示，这个过程可以是分层的（hierarchical），可以将每1个sentence编码成1个vector然后将它们加起来得到表示整个document的vector，然后再通过它产生多个setence的vector，然后将多个setence的vector解码得到word sequence。这是1个4层的LSTM（word sequence-sentence sequence-document-sentence sequence-word sequence）。</p><p>Seq2Seq Auto-encoder比较容易得到文法的编码，而Skip Thought（输入1个句子，输出其下1句）更容易得到语义的意思。</p><h2 id="Seq2Seq-Auto-encoder-for-Speech"><a href="#Seq2Seq-Auto-encoder-for-Speech" class="headerlink" title="Seq2Seq Auto-encoder for Speech"></a>Seq2Seq Auto-encoder for Speech</h2><p><a href="https://arxiv.org/abs/1603.00982" target="_blank" rel="noopener">Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder</a></p><p>Seq2Seq Auto-encoder还可以用在语音上，它可以把1个audio segment(word-level)编码成1个fixed-length vector。这有什么用处呢？它可以基于语音之间的相似度做语音搜索。</p><p><img src="https://pic1.zhimg.com/80/v2-ac9ee7ee9e1c47683525266543036146_720w.png" alt="img"></p><p>那如何基于语音之间的相似度做语音搜索呢？如上图所示，假如有1个语音的database，可将其划分为audio segments（长度可变），然后使用Seq2Seq Auto-encoder将其编码为1个fixed-length vector。对于1段需要搜索的语音，通过Seq2Seq Auto-encoder将其编码成1个fixed-length vector，计算其与database中audio segments的vector的相似度。</p><p><img src="https://pic1.zhimg.com/80/v2-286be450d67ee1a810c5dac8889deba1_720w.png" alt="img"></p><p>那如何把1个audio segment编码成1个fixed-length vector呢？如上图所示，首先把audio segment转换为acoustic feature sequence，然后输入至RNN。该RNN作为Encoder，在最后1个时间点其memory中的值就代表整个acoustic feature sequence，这就是我们想要的vector。但是只有这个作为Encoder的RNN我们没有办法训练，所以还要训练1个作为Decoder的RNN。该RNN作为Decoder，以Encoder在最后1个时间点时memory中的vector为输入，然后输出1个acoustic feature sequence，训练目标是输出的acoustic feature sequence和输入的acoustic feature sequence越接近越好。由此可知，该例中Encoder和Decoder是要同时训练的。</p><h2 id="Attention-based-Model"><a href="#Attention-based-Model" class="headerlink" title="Attention-based Model"></a>Attention-based Model</h2><blockquote><p>專家發現，小兒失憶現象是由於動物的大腦在神經新生的過程中，處於不斷重組的狀態，為減少太多訊息的干擾，會不斷清除舊記憶，從而增加對新事物的學習能力。年幼小鼠的記憶保留能力所以低下，乃因其高度活躍的神經再生所致，而成年小鼠保留記憶能力的增加，也由於其大腦相對成熟，海馬體的神經再生活力已經下降。腦科學家既然可以抑制年幼小鼠海馬體的高度活躍神經再生活力，又可刺激成年小鼠海馬體增加其神經再生活力。</p><p>——————引自<a href="http://henrylo1605.blogspot.com/2015/05/blog-post_56.html" target="_blank" rel="noopener">http://henrylo1605.blogspot.com/2015/05/blog-post_56.html</a></p></blockquote><p>现在除了RNN之外，Attention-based Model也用到了memory的思想。机器也可以有记忆，神经网络通过操控读/写头去读/写信息，这个就是Neural Turing Machine。</p><h2 id="Reading-Comprehension"><a href="#Reading-Comprehension" class="headerlink" title="Reading Comprehension"></a>Reading Comprehension</h2><p>Attention-based Model常常用在Reading Comprehension上，让机器读1篇document，再把每个setence变成代表语义的vector，接下来让用户向机器提问，神经网络就会去调用读写头，取出memory中与查询语句相关的信息，综合处理之后，可以给出正确的回答。</p><h2 id="Visual-Question-Answering"><a href="#Visual-Question-Answering" class="headerlink" title="Visual Question Answering"></a>Visual Question Answering</h2><h2 id="Speech-Question-Answering"><a href="#Speech-Question-Answering" class="headerlink" title="Speech Question Answering"></a>Speech Question Answering</h2><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在Slot Filling中，输入是1个word vector，输出是每个word的label，&lt;strong&gt;输入和输出是等长的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;然而，RNN还可以实现多对1、多对多…&lt;/p&gt;
&lt;h2 id=&quot;Many-to-One&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.2如何训练RNN</title>
    <link href="https://chouxianyu.github.io/2021/04/11/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-2%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83RNN/"/>
    <id>https://chouxianyu.github.io/2021/04/11/李宏毅机器学习课程笔记-9-2如何训练RNN/</id>
    <published>2021-04-11T02:41:12.000Z</published>
    <updated>2021-04-11T02:43:24.928Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RNN的损失函数"><a href="#RNN的损失函数" class="headerlink" title="RNN的损失函数"></a>RNN的损失函数</h2><p>仍然以Slot Filling为例，如下图所示。</p><p><img src="https://pic2.zhimg.com/80/v2-bbab6148db7b998a52c988e160c3fd16_720w.png" alt="img"></p><p>对于1个word$x^i$，RNN输出1个one-hot编码的vector $y^i$，求$y^i$和对应label的交叉熵损失（Cross Entropy Loss），将多个word的loss求和即为RNN的损失函数。需要注意的是不能打乱word的语序，$x^{i+1}$要紧接着$x^i$输入。</p><p>确定RNN的损失函数后，RNN的训练其实也是用的梯度下降。训练前馈神经网络时我们使用有效的反向传播算法，为了方便地训练RNN，我们使用BPTT。基于BP，<strong>BPTT(Backpropagation Through Time)</strong>考虑了时间维度的信息。</p><h2 id="RNN的Error-Surface"><a href="#RNN的Error-Surface" class="headerlink" title="RNN的Error Surface"></a>RNN的Error Surface</h2><p>RNN的Error Surface如下图所示，其中$z$轴代表loss，$x$轴和$y$轴代表两个参数$w_1$和$w_2$。可以看出，RNN的Error Surface在某些地方非常平坦，在某些地方又非常的陡峭。<strong>这样的Error Surface导致在训练RNN时loss剧烈变化</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-9264ded1a5556fb9bce3cbd5f7d5a3aa_720w.png" alt="img"></p><h2 id="问题出现的原因"><a href="#问题出现的原因" class="headerlink" title="问题出现的原因"></a>问题出现的原因</h2><p>既然RNN的Error Surface中有这么平滑的地方，那会不会是sigmoid激活函数造成的梯度消失呢？原因并不是sigmoid，如果是的话，那换成ReLU就可以，但把sigmoid换成ReLU之后，效果反而更差了。那到底为什么会有非常陡峭和非常平滑的地方呢？</p><p><img src="https://pic4.zhimg.com/80/v2-e601dceb08a568fabb5e12ab811d310b_720w.png" alt="img"></p><p>如上图所示，假设某RNN只含1个神经元，并且该神经元是Linear的，input和output的weight都是1，没有bias，memory传递的weight是$w$，输入序列为[1, 0, 0, 0, …, 0]，所以$y^{1000}=w^{999}$。</p><p>现在我们考虑loss关于参数$w$的梯度，当$w:\ 1\ =&gt;\ 1.01$时，可知$y^{1000}:\ 1\ =&gt;\ 20000$，此时梯度很大；当$w:\ 0.99\ =&gt;\ 0.01$时，可知$y^{1000}$几乎没有变化，此时梯度很小。</p><p>从该例中可知，RNN的Error Surface中的“悬崖”出现的原因是，<strong>关于memory的参数$w$的作用随着时间增加不断增强，导致RNN出现梯度消失或梯度爆炸的问题</strong>。</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>如何解决RNN梯度消失或梯度爆炸的问题？可以通过<strong>Clipping</strong>进行处理，Clipping的效果是使梯度不超过某个阈值，即当梯度即将超过某个阈值（比如15）时，就将梯度赋值为该阈值。</p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>有什么更好的方法可以解决RNN的Error Surface中的问题呢？LSTM就是使用最广泛的技巧，它可以“删除”Error Surface中比较平坦的部分，也就解决了梯度消失的问题，但它无法解决梯度爆炸的问题。正因如此，训练LSTM时需要将学习率调得特别小。</p><p>LSTM为什么可以解决RNN中梯度消失的问题呢，因为RNN和LSTM对memory的处理是不同的（LSTM有forget gate）。<strong>在RNN中，每个时间点memory中的旧值都会被新值覆盖，导致参数$w$对memory的影响每次都被清除，进而引发梯度消失。在LSTM中，每个时间点memory里的旧值会乘以$f(g(f))$再与新值相加，只有在forget gate被关闭时参数$w$对memory的影响才会被清除，在forget gate被打开时参数$w$对memory的影响就会通过累加得到保留，因此不会出现梯度消失的问题。</strong></p><p>LSTM在1997年被提出，第1版的LSTM被提出就是为了解决梯度消失的问题，但这1版本是没有forget gate的，forget gate是后来才加上去的。也有1种说法是，在训练LSTM时需要给forget gate特别大的bias，以确保forget gate在多数情况下是开启的。</p><h3 id="GRU（Gated-Recurrent-Unit-Cho-EMNLP’14）"><a href="#GRU（Gated-Recurrent-Unit-Cho-EMNLP’14）" class="headerlink" title="GRU（Gated Recurrent Unit, Cho, EMNLP’14）"></a>GRU（Gated Recurrent Unit, Cho, EMNLP’14）</h3><p>GRU比LSTM更简单，GRU只有2个gate，因此需要更少的参数量、鲁棒性更好、更不容易过拟合。GRU的基本思路是“旧的不去，新的不来”，GRU把input和forget gate联动起来，当forget gate把memory中的值清空时，input gate才会打开然后放入新的值。</p><h3 id="Clockwise-RNN（Jan-Koutnik-JMLR’14）"><a href="#Clockwise-RNN（Jan-Koutnik-JMLR’14）" class="headerlink" title="Clockwise RNN（Jan Koutnik, JMLR’14）"></a>Clockwise RNN（Jan Koutnik, JMLR’14）</h3><h3 id="SCRN（Structrally-Constrained-Recurrent-Network-Tomas-Mikolov-ICLR’15）"><a href="#SCRN（Structrally-Constrained-Recurrent-Network-Tomas-Mikolov-ICLR’15）" class="headerlink" title="SCRN（Structrally Constrained Recurrent Network, Tomas Mikolov, ICLR’15）"></a>SCRN（Structrally Constrained Recurrent Network, Tomas Mikolov, ICLR’15）</h3><h3 id="Vanilla-RNN-Initialized-with-Identity-Matrix-ReLU（Quoc-V-Le-arXiv’15）"><a href="#Vanilla-RNN-Initialized-with-Identity-Matrix-ReLU（Quoc-V-Le-arXiv’15）" class="headerlink" title="Vanilla RNN Initialized with Identity Matrix + ReLU（Quoc V.Le, arXiv’15）"></a>Vanilla RNN Initialized with Identity Matrix + ReLU（Quoc V.Le, arXiv’15）</h3><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;RNN的损失函数&quot;&gt;&lt;a href=&quot;#RNN的损失函数&quot; class=&quot;headerlink&quot; title=&quot;RNN的损失函数&quot;&gt;&lt;/a&gt;RNN的损失函数&lt;/h2&gt;&lt;p&gt;仍然以Slot Filling为例，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.1循环神经网络RNN入门</title>
    <link href="https://chouxianyu.github.io/2021/04/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-1%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN%E5%85%A5%E9%97%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/10/李宏毅机器学习课程笔记-9-1循环神经网络RNN入门/</id>
    <published>2021-04-10T00:53:08.000Z</published>
    <updated>2021-04-10T01:02:16.365Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Slot-Filling"><a href="#Slot-Filling" class="headerlink" title="Slot Filling"></a>Slot Filling</h2><p>比如在订票系统中，输入“Arrive Taipei on November 2nd”这样一个序列，我们设置几个slot(槽位)，希望算法能够将关键词“Taipei”放入Destination这个slot，将”November”和”2nd”放入到达时间Time of Arrival这个slot，而“Arrive”和“on”不属于任何slot。那这个算法如何实现呢？</p><h2 id="Slot-Filling-with-FNN"><a href="#Slot-Filling-with-FNN" class="headerlink" title="Slot Filling with FNN"></a>Slot Filling with FNN</h2><p>可以用Feedforward Neural Network实现Slot Filling吗？可以，下面介绍这种FNN的输入和输出，但其存在问题。</p><p>输入是一个word（比如“Taipei”）并用vector来表示它；输出是1个probablity distribution，表示输入的word属于各个slot的概率。</p><p>如何用vector表示1个word呢？方法有很多。比如<strong>1-of-N Encoding</strong>(又名one-hot Encoding)，如下图所示。设定1个lexicon(词汇表)，那vector的size就和lexicon的size相同，vector中的每个维度对应lexicon中的word，vector中word对应的维度的值为1、其它维度的值为0。</p><p><img src="https://pic1.zhimg.com/80/v2-44be0e091d503721158e1bbee1cdb048_720w.png" alt="img"></p><p>如下图所示，只有1-of-N Encoding还不够，一些word不在lexicon中，对此我们需要在lexicon中添加1个”<strong>other</strong>“。除了1-of-N Encoding，还可以通过<strong>word hashing</strong>。可以用1个26×26×26的vector表示1个word，该vector中每个元素代表1个3字母序列。比如”apple”包括”app”、”ppl”、”ple”。</p><p><img src="https://pic4.zhimg.com/80/v2-828e73ac8019838e7667eeb033859d40_720w.png" alt="img"></p><p>使用FNN实现Slot Filling时会存在一个问题：假如有2个句子“Arrive Taipei on November 2nd”和“Leave Taipei on November 2nd”，在处理这2个句子时FNN会先处理“arrive”和“leave”这2个词汇然后再处理“Taipei”。<strong>这时FNN没有办法区分出“Taipei”是出发地还是目的地，而我们希望算法在处理序列时是有“记忆力”的（即在处理“Taipei”时，它还记得“Leave”或“Arrive”）</strong>，于是RNN诞生了。</p><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><p>如下图所示，<strong>将每1个隐藏层的输出保存在memory中，网络不仅考虑了input，还要考虑memory中的数据</strong>（merory中的数据是需要有初值的，比如0）。</p><p><img src="https://pic1.zhimg.com/80/v2-f151d27c684e3ae8e60cb9d55d4a78fe_720w.png" alt="img"></p><p>因为RNN会考虑memory中存储的临时值，而不同输入产生的临时值不一定相同，所以<strong>改变输入序列中元素的顺序会导致最终输出结果的改变</strong>（Changing the sequence order will change the output）。</p><h2 id="Slot-Filling-with-RNN"><a href="#Slot-Filling-with-RNN" class="headerlink" title="Slot Filling with RNN"></a>Slot Filling with RNN</h2><p>如下图所示，以“Arrive Taipei on November 2nd” 这个word sequence为例，将“Arrive”的vector$x^1$输入到RNN，隐藏层生成$a^1$，根据$a^1$生成$y^1$，表示“arrive”属于每个slot的概率，其中$a^1$会被存储到memory中；将“Taipei”的vector$x^2$输入到RNN，此时隐藏层同时考虑$x^2$和memory中的$a^1$生成$a^2$，根据$a^2$生成$y^2$，表示“Taipei”属于某个slot的概率，此时再把$a^2$存到memory中；以此类推根据$x_3$和$a_2$生成$a_3$进而得到$y^3$……</p><p><img src="https://pic2.zhimg.com/80/v2-e16243c99c04f52df6f52ae560fdfb03_720w.png" alt="img"></p><h2 id="RNN的变体"><a href="#RNN的变体" class="headerlink" title="RNN的变体"></a>RNN的变体</h2><h3 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h3><p>RNN也有不同的变形。<strong>Elman Network是把隐藏层的输出存到memory中，而Jordan Network是把输出层的输出保存到memory中</strong>。由于隐藏层没有明确的训练目标，而整个NN具有明确的目标，因此Jordan Network的表现会更好一些。</p><p><img src="https://pic4.zhimg.com/80/v2-7df1a297e9879628ea03333d79b06d22_720w.png" alt="img"></p><h3 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h3><p>如下图所示，<strong>RNN可以是双向的</strong>。训练2个方向的RNN，1个从前往后读取序列，1个从后往前读取序列，然后使用2个RNN的隐藏层得到最后的输出层。这样的好处是，<strong>输出层的感受野更大</strong>，因为RNN在得到$y^{t+1}$的时候，它不只看了从句首$x^1$开始到$x^{t+1}$的数据，还看了从句尾$x^{n}$一直到$x^{t+1}$的输入，这就相当于<strong>RNN是在看过整个句子之后才计算每个word属于哪个slot的概率</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-a591f7a3eda6fd1328ae36273451bff2_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Slot-Filling&quot;&gt;&lt;a href=&quot;#Slot-Filling&quot; class=&quot;headerlink&quot; title=&quot;Slot Filling&quot;&gt;&lt;/a&gt;Slot Filling&lt;/h2&gt;&lt;p&gt;比如在订票系统中，输入“Arrive Taipei on N
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-8.2图神经网络(Spatial-based Convolution)</title>
    <link href="https://chouxianyu.github.io/2021/04/05/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-8-2%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Spatial-based-Convolution/"/>
    <id>https://chouxianyu.github.io/2021/04/05/李宏毅机器学习课程笔记-8-2图神经网络-Spatial-based-Convolution/</id>
    <published>2021-04-05T12:06:39.000Z</published>
    <updated>2021-04-05T12:07:56.753Z</updated>
    
    <content type="html"><![CDATA[<h2 id="术语（Terminology）"><a href="#术语（Terminology）" class="headerlink" title="术语（Terminology）"></a>术语（Terminology）</h2><ul><li><p><strong>Aggregation</strong></p><p>  Aggregation是Convolution在GNN中的推广。Aggregation就是在某一个layer中用某node及其neighbor的feature得到下一个layer中该node的feature。</p></li><li><p><strong>Readout</strong></p><p>  Readout有点像是全连接在GNN中的推广。Readout就是汇总整个图的信息，最终得到一个特征来表示这整个图（Graph Representation）。</p></li></ul><h2 id="NN4G-Neural-Network-for-Graph"><a href="#NN4G-Neural-Network-for-Graph" class="headerlink" title="NN4G(Neural Network for Graph)"></a>NN4G(Neural Network for Graph)</h2><p>论文链接：<a href="https://ieeexplore.ieee.org/document/4773279" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/4773279</a></p><ul><li><p>输入层</p><p>  假如是一个化学分子，输入层的图中的结点就是一个原子。不同原子有不同的特征， 其特征可以是任何和原子相关的化学特征，所以需要<strong>embedding</strong>（将高维特征映射到低维特征），做完embedding也就得到了隐藏层$h^0$。</p></li><li><p>隐藏层$h^0$</p><p>  如何做embedding呢？让原特征乘以embedding matrix就得到隐藏层$h^0$。如下图所示，以1个结点为例，输入层中结点$v_3$的特征是$x_3$，该结点embedding时的计算式为$h^0_3=\bar w_0\cdot x_3$。<strong>embedding</strong>后就得到了隐藏层$h^0$，然后再对隐藏层$h^0$进行<strong>Aggregation</strong>就得到了隐藏层$h^1$。</p><p>  <img src="https://pic2.zhimg.com/80/v2-cff58a466296d02f900be31d89fca62a_720w.png" alt="img"></p></li><li><p>隐藏层$h^1$</p><p>  如何做Aggregation呢？如下图所示，以1个结点为例，在隐藏层$h^0$中，结点$h^0_3$和$h^0_0,h^0_2,h^0_4$3个结点相邻，则Aggregation时计算式为$h^1_3=\hat w_{1,0}(h^0_0+h^0_2+h^0_4)+\bar w_1\cdot x_3$。<strong>经过多次Aggregation，最后需要Readout</strong>。</p><p>  <img src="https://pic3.zhimg.com/80/v2-35dabe169d54b8815ad452c1c105cd75_720w.png" alt="img"></p></li><li><p>Readout</p><p>  如何做Readout呢？如下图所示，假设有3个隐藏层，那Readout的计算式为$y=MEAN(h^0)+MEAN(h^1)+MEAN(h^2)$。</p><p>  <img src="https://pic4.zhimg.com/80/v2-99a44d03b0f47295abfe6d62eaa77bde_720w.png" alt="img"></p></li></ul><h2 id="DCNN-Diffusion-Convolution-Neural-Network"><a href="#DCNN-Diffusion-Convolution-Neural-Network" class="headerlink" title="DCNN(Diffusion-Convolution Neural Network)"></a>DCNN(Diffusion-Convolution Neural Network)</h2><p>论文链接：<a href="https://arxiv.org/abs/1511.02136" target="_blank" rel="noopener">https://arxiv.org/abs/1511.02136</a></p><ul><li><p>输入层</p><p>  假如我们有1个和上例中（NN4G）一样的输入图。</p></li><li><p>隐藏层$h^0$</p><p>  如下图所示，从输入层到隐藏层$h^0$的计算式为$h^0_3=w^0_3MEAN(d(3,\cdot)=1)$，其中$d(3,\cdot)=1$表示所有与结点$x_3$距离为1的输入层结点的特征。</p><p>  <img src="https://pic4.zhimg.com/80/v2-97b6883501dbd6fca82612e42f9e02b4_720w.png" alt="img"></p></li><li><p>隐藏层$h^1$</p><p>  如下图所示，从隐藏层$h^0$到隐藏层$h^1$的计算式为$h^1_3=w^1_3MEAN(d(3,\cdot)=2)$，其中$d(3,\cdot)=2$表示所有与结点$x_3$距离为2的输入层结点的特征。</p><p>  <img src="https://pic2.zhimg.com/80/v2-8282b7f79e512aa9bc1a1b3a18a295aa_720w.png" alt="img"></p><p>  以此类推，<strong>叠加k个隐藏层后就可以获取各结点k范围内的信息</strong>。如下图所示，令1个隐藏层中多个结点的特征形成矩阵（1行是1个结点的特征），多个隐藏层的特征就形成多个通道$H^0,H^1,\dots,H^k$。</p><p>  <img src="https://pic2.zhimg.com/80/v2-ec751b3608ee583f1f6a4cd320ff2f0b_720w.png" alt="img"></p></li><li><p>Node features</p><p>  如何表达整个图的特征呢？如下图所示，将每个通道的特征flatten，然后再乘以参数$w$得到$y_1$即可。</p><p>  <img src="https://pic1.zhimg.com/80/v2-c226fe1e317a545f5c9c5b2c114a0a7b_720w.png" alt="img"></p><p>  也有其它做法，ICLR2018中<a href="https://arxiv.org/abs/1707.01926" target="_blank" rel="noopener">DGC(Diffusion Graph Convolution)</a>不是flatten，而是相加，如下图所示。</p><p>  <img src="https://pic4.zhimg.com/80/v2-7110f786304c6ed60fd4071e00e23047_720w.png" alt="img"></p></li></ul><h2 id="MoNET-Mixture-Model-Networks"><a href="#MoNET-Mixture-Model-Networks" class="headerlink" title="MoNET(Mixture Model Networks)"></a>MoNET(Mixture Model Networks)</h2><p>NN4G、DCNN都是将邻居结点的特征直接相加，并没有考虑各个邻居结点特征的重要性，而MoNET考虑了这个问题。</p><p>论文链接：<a href="https://arxiv.org/abs/1611.08402" target="_blank" rel="noopener">https://arxiv.org/abs/1611.08402</a></p><p>MoNET定义了结点距离的概念，基于结点距离表示各个邻居结点特征的重要性然后对各个邻居结点进行<strong>加权</strong>求和，而不是简单地取均值或求和。</p><p>如下图所示，假如我们有和上例一样的输入图，隐藏层$h^0$中结点$v_3$的特征为$h^0_3$，结点$v_3$和结点$v_0$的距离为$u_{3,0}$。</p><p>定义结点$x,y$的距离$u(x,y)=(\frac{1}{\sqrt{deg(x)}},\frac{1}{\sqrt{deg(y)}})^T$，其中$deg(x)$表示结点$x$的度（degree，度是连接到每个节点的边的数量）。</p><p><img src="https://pic1.zhimg.com/80/v2-454c766a5484d4fe7f79178e679fab55_720w.png" alt="img"></p><h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><p><strong>SA</strong>mple and aggre<strong>G</strong>at<strong>E</strong>(GraphSAGE)，在transductive和inductive setting上都能work。</p><p>论文链接：<a href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener">https://arxiv.org/abs/1706.02216</a></p><p>GraphSAGE的Aggregation除了mean，还有max pooling和LSTM。LSTM用来处理序列数据，但图中结点的邻居并没有序列关系，但如果每次在邻居中随机取样出不同顺序，那也许可以忽略顺序学习到顺序无关的信息。</p><h2 id="GAT-Graph-Attention-Networks"><a href="#GAT-Graph-Attention-Networks" class="headerlink" title="GAT(Graph Attention Networks)"></a>GAT(Graph Attention Networks)</h2><p>论文链接：<a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">https://arxiv.org/abs/1710.10903</a></p><p>GAT不只是做加权求和（weighted sum），而其中的weight是通过学习得到的，方法就是对邻居做attention。</p><p>假如我们有1个和上例中（NN4G）一样的输入图。在做aggregation时，我们通过函数$f$计算各个邻居结点$v_0,v_2,v_4$对结点$v_3$的重要性，然后做加权求和。</p><p><img src="https://pic4.zhimg.com/80/v2-d293babed38f57a74e8740be39dd69d8_720w.png" alt="img"></p><h2 id="GIN-Graph-Isomorphism-Network"><a href="#GIN-Graph-Isomorphism-Network" class="headerlink" title="GIN(Graph Isomorphism Network)"></a>GIN(Graph Isomorphism Network)</h2><p>这篇论文偏理论，证明出有些方法是work的，有些是不会work的。</p><p>比如提取特征时不要用mean或max（在一些情况下会fail），要用sum，如下图所示。</p><p><img src="https://pic4.zhimg.com/80/v2-ad2c1fd4a3043d56cd5420c03d3cfef7_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;术语（Terminology）&quot;&gt;&lt;a href=&quot;#术语（Terminology）&quot; class=&quot;headerlink&quot; title=&quot;术语（Terminology）&quot;&gt;&lt;/a&gt;术语（Terminology）&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="图神经网络" scheme="https://chouxianyu.github.io/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-7.4基于CNN和PyTorch的食物图片分类</title>
    <link href="https://chouxianyu.github.io/2021/04/05/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7-4%E5%9F%BA%E4%BA%8ECNN%E5%92%8CPyTorch%E7%9A%84%E9%A3%9F%E7%89%A9%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    <id>https://chouxianyu.github.io/2021/04/05/李宏毅机器学习课程笔记-7-4基于CNN和PyTorch的食物图片分类/</id>
    <published>2021-04-05T11:42:47.000Z</published>
    <updated>2021-04-13T08:52:03.190Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework3的记录。</p><p>全部课程PPT、数据和代码下载链接：</p><p>链接：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg" target="_blank" rel="noopener">https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg</a> 提取码：tpmc</p><p>代码仓库：<a href="https://github.com/chouxianyu/LHY_ML2020_Codes" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes</a></p><ul><li><p>任务描述</p><p>  通过CNN实现食物图片分类，数据集已提供</p></li><li><p>数据集描述</p><p>  11个图片类别，训练集中有9866张图片，验证集中有3430张图片，测试集中有3347张图片。</p><p>  训练集和验证集中图片命名格式为<code>类别_编号.jpg</code>，编号不重要。</p></li><li><p>代码</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.activation <span class="keyword">import</span> ReLU</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.batchnorm <span class="keyword">import</span> BatchNorm2d</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.pooling <span class="keyword">import</span> MaxPool1d, MaxPool2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""加载数据"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_files</span><span class="params">(dir_path)</span>:</span> <span class="comment"># 读取文件夹中的所有图片</span></span><br><span class="line">    filenames = sorted(os.listdir(dir_path))</span><br><span class="line">    x = np.zeros((len(filenames), <span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>), dtype=np.uint8) <span class="comment"># (N,H,W,C)</span></span><br><span class="line">    y = np.zeros((len(filenames)), dtype=np.uint8)</span><br><span class="line">    <span class="keyword">for</span> i, filename <span class="keyword">in</span> enumerate(filenames):</span><br><span class="line">        img = cv2.imread(os.path.join(dir_path, filename))</span><br><span class="line">        x[i, : , :] = cv2.resize(img, (<span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">        y[i] = int(filename.split(<span class="string">"_"</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line">train_x, train_y = read_files(<span class="string">"./data/training"</span>)</span><br><span class="line">val_x, val_y = read_files(<span class="string">"./data/validation"</span>)</span><br><span class="line">print(<span class="string">"Data Loaded"</span>)</span><br><span class="line">print(<span class="string">"Size of training data : %d"</span> % len(train_x))</span><br><span class="line">print(<span class="string">"Size of validation data : %d"</span> % len(val_x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""数据变换（训练时进行数据增强）"""</span></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(mode=<span class="keyword">None</span>), <span class="comment"># 将图片格式转换成PIL格式</span></span><br><span class="line">    transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>), <span class="comment"># 随机水平翻转</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">15</span>), <span class="comment"># 随机旋转图片</span></span><br><span class="line">    transforms.ToTensor(), <span class="comment"># 转换成torch中的tensor并将值normalize到[0.0,1.0]</span></span><br><span class="line">])</span><br><span class="line">val_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""加载数据"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImgDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y=None, transform=None)</span>:</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.y = torch.LongTensor(y)</span><br><span class="line">        self.transform = transform</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.x)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        X = self.x[index]</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            X = self.transform(X)</span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            Y = self.y[index]</span><br><span class="line">            <span class="keyword">return</span> X, Y</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">train_set = ImgDataset(train_x, train_y, train_transform)</span><br><span class="line">val_set = ImgDataset(val_x, val_y, val_transform)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""定义模型"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        <span class="comment"># torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)</span></span><br><span class="line">        <span class="comment"># torch.nn.MaxPool2d(kernel_size, stride, padding)</span></span><br><span class="line">        self.cnn = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [64, 128, 128]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [64, 64, 64]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [128, 64, 64]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [128, 32, 32]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [256, 32, 32]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [256, 16, 16]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [512, 16, 16]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [512, 8, 8]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [512, 8, 8]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [512, 4, 4]</span></span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">11</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.cnn(x)</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>) <span class="comment"># torch.nn只支持mini-batches而不支持单个sample，第1个维度是mini-batch中图片（特征）的索引，即将每张图片都展开</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""训练并测试模型"""</span></span><br><span class="line">model = Model() <span class="comment"># model = Model().cuda()</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train_acc = <span class="number">0.0</span></span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    val_acc = <span class="number">0.0</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># batch_loss.backward()的gradient会累加，所以每个batch都需要置零</span></span><br><span class="line">        pred = model(data[<span class="number">0</span>]) <span class="comment"># pred = model(data[0].cuda())</span></span><br><span class="line">        batch_loss = criterion(pred, data[<span class="number">1</span>]) <span class="comment"># batch_loss = criterion(pred, data[1].cuda())</span></span><br><span class="line">        batch_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        train_acc += np.sum(np.argmax(pred.detach().numpy(), axis=<span class="number">1</span>) == data[<span class="number">1</span>].numpy())</span><br><span class="line">        <span class="comment"># train_acc += np.sum(np.argmax(pred.cpu().detach().numpy(), axis=1) == data[1].numpy())</span></span><br><span class="line">        train_loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(val_loader):</span><br><span class="line">            pred = model[data[<span class="number">0</span>]] <span class="comment"># pred = model(data[0].cuda())</span></span><br><span class="line">            batch_loss = criterion(pred, data[<span class="number">1</span>]) <span class="comment"># batch_loss = criterion(pred, data[1].cuda())</span></span><br><span class="line">            val_acc += np.sum(np.argmax(pred.detach().numpy(), axis=<span class="number">1</span>) == data[<span class="number">1</span>].numpy())</span><br><span class="line">            <span class="comment"># val_acc += np.sum(np.argmax(pred.cpu().detach().numpy(), axis=1) == data[1].numpy())</span></span><br><span class="line">            val_loss += batch_loss.item()</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f'</span> % \</span><br><span class="line">            (epoch+<span class="number">1</span>, epochs, time.time()-epoch_start_time, \</span><br><span class="line">             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))</span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework3的记录。&lt;/p&gt;
&lt;p&gt;全部课程PPT、数据和代码下载链接：&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pan.baidu.com/
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
      <category term="图片分类" scheme="https://chouxianyu.github.io/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
      <category term="卷积神经网络" scheme="https://chouxianyu.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch入门：基于LeNet5和CIFAR10的图片分类</title>
    <link href="https://chouxianyu.github.io/2021/04/04/PyTorch%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8ELeNet5%E5%92%8CCIFAR10%E7%9A%84%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    <id>https://chouxianyu.github.io/2021/04/04/PyTorch入门：基于LeNet5和CIFAR10的图片分类/</id>
    <published>2021-04-04T05:45:03.000Z</published>
    <updated>2021-04-04T05:53:56.482Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.cnblogs.com/chouxianyu/p/14613460.html" target="_blank" rel="noopener">PyTorch入门：使用PyTorch搭建神经网络LeNet5</a>一文中，我们已经使用PyTorch实现了一个简单的神经网络LeNet5，本文将基于PyTorch使用LeNet5和CIFAR10实现图片分类模型的定义、训练和测试的全过程，代码(有详细注释)如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 构建神经网络模型：将LeNet5模型的输入改为3个通道</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## normalize：torchvision中数据集是元素值在[0,1]范围的PIL图片(C,H,W)，需将其数值范围转换为[-1,1]</span></span><br><span class="line">normalization = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 加载CIFAR10数据集</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>, transform=normalization, download=<span class="keyword">True</span>)</span><br><span class="line">train_set_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">0</span>) <span class="comment"># Windows系统中建议把num_workers设为0</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>, transform=normalization, download=<span class="keyword">True</span>)</span><br><span class="line">test_set_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">False</span>, num_workers=<span class="number">0</span>) <span class="comment"># Windows系统中建议把num_workers设为0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 显示CIFAR10数据集中的一些图片</span></span><br><span class="line"><span class="comment"># def imshow(img):</span></span><br><span class="line"><span class="comment">#     # print(img.size())</span></span><br><span class="line"><span class="comment">#     img = img / 2 + 0.5 # unnormalize: [-1,1] =&gt; [0,1]</span></span><br><span class="line"><span class="comment">#     img = img.numpy()</span></span><br><span class="line"><span class="comment">#     plt.imshow(np.transpose(img, (1, 2, 0))) # PIL的(C,H,W) =&gt; matplotlib的(H,W,C)</span></span><br><span class="line"><span class="comment">#     plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')</span></span><br><span class="line"><span class="comment"># data_iter = iter(train_set_loader)</span></span><br><span class="line"><span class="comment"># images, labels = data_iter.next() # images, labels都是tensor</span></span><br><span class="line"><span class="comment"># # print(images.size())</span></span><br><span class="line"><span class="comment"># # print(labels.size())</span></span><br><span class="line"><span class="comment"># imshow(torchvision.utils.make_grid(images))</span></span><br><span class="line"><span class="comment"># print(' '.join('%s' % classes[labels[j]] for j in range(len(labels))))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义神经网络、损失函数和优化器</span></span><br><span class="line">net = Net()</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(params=net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>) <span class="comment"># SGD with momentum</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练神经网络</span></span><br><span class="line">print(<span class="string">'Training Started'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>): <span class="comment"># 1个epoch会将所有数据训练一次</span></span><br><span class="line">    running_loss = <span class="number">0.0</span> <span class="comment"># 用来在控制台输出loss，以观察训练情况</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(iterable=train_set_loader, start=<span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 获取数据</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line">        <span class="comment"># 清空梯度缓存</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 输出每2000个mini-batch的平均loss</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>: <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            print(<span class="string">'[%3d, %5d] loss: %.3f'</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 保存模型参数</span></span><br><span class="line">torch.save(net.state_dict(), <span class="string">'./data/LeNet5.pt'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 测试模型</span></span><br><span class="line">print(<span class="string">'Testing Started'</span>)</span><br><span class="line">net_new = Net()</span><br><span class="line">net_new.load_state_dict(torch.load(<span class="string">'./data/LeNet5.pt'</span>))</span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_set_loader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        _, predictions = torch.max(net_new(images), <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predictions==labels).sum().item()</span><br><span class="line">print(<span class="string">'Accuracy: %d/%d = %.2f%%'</span> % (correct, total, correct/total*<span class="number">100</span>) )</span><br><span class="line"></span><br><span class="line"><span class="string">"""Explore:</span></span><br><span class="line"><span class="string">使用GPU后会发现速度并没有增加很多，原因是LeNet这个模型非常小。</span></span><br><span class="line"><span class="string">如果将模型宽度增大（增加2个卷积层的卷积核数量），GPU对模型的加速效果会是怎么样的呢？</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在&lt;a href=&quot;https://www.cnblogs.com/chouxianyu/p/14613460.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PyTorch入门：使用PyTorch搭建神经网络LeNet5&lt;/a&gt;一文中，我们已经使
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
      <category term="图片分类" scheme="https://chouxianyu.github.io/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch入门：使用PyTorch搭建神经网络LeNet5</title>
    <link href="https://chouxianyu.github.io/2021/04/03/PyTorch%E5%85%A5%E9%97%A8%EF%BC%9A%E4%BD%BF%E7%94%A8PyTorch%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CLeNet5/"/>
    <id>https://chouxianyu.github.io/2021/04/03/PyTorch入门：使用PyTorch搭建神经网络LeNet5/</id>
    <published>2021-04-03T01:47:13.000Z</published>
    <updated>2021-04-03T04:17:40.842Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在本文中，我们基于PyTorch构建一个简单的神经网络LeNet5。</p><p>在阅读本文之前，建议您了解一些卷积神经网络的前置知识，比如卷积、Max Pooling和全连接层等等，可以看我写的相关文章：<a href="https://zhuanlan.zhihu.com/p/360796545" target="_blank" rel="noopener">李宏毅机器学习课程笔记-7.1CNN入门详解</a>。</p><p>通过阅读本文，您可以学习到如何使用PyTorch构建神经网络LeNet5。</p><h1 id="模型说明"><a href="#模型说明" class="headerlink" title="模型说明"></a>模型说明</h1><p>在本例中，我们使用如下图所示的神经网络模型：LeNet5。</p><p><img src="https://pic1.zhimg.com/80/v2-e3852bfd8a716bda7cddac603628affa_720w.png" alt="img"></p><p>该模型有1个输入层、2个卷积层、2次Max Pooling、2个全连接层和1个输出层。</p><ul><li><p>输入层INPUT</p><p>  1个channel，图片size是32×32。</p></li><li><p>卷积层C1</p><p>  6个channel，特征图的size是28×28，即每个卷积核的size为(5,5)，stride为1。</p></li><li><p>下采样操作S2</p><p>  6个channel，特征图的size是14×14，即Max Pooling窗口size为(2,2)。</p></li><li><p>卷积层C3</p><p>  16个channel，特征图的size是10×10，即每个卷积核的size为(5,5)，stride为1。</p></li><li><p>下采样操作S4</p><p>  16个channel，特征图的size是5×5，即Max Pooling窗口size为(2,2)。</p></li><li><p>全连接层F5</p><p>  120个神经元。</p></li><li><p>全连接层F6</p><p>  84个神经元。</p></li><li><p>输出层OUTPUT</p><p>  10个神经元。</p></li></ul><p>另外，除了输入层和输出层，剩下的卷积层、最大池化操作和全连接层后面都要加上Relu激活函数，下采样操作S4之后需要进行Flatten以和全连接层F5衔接起来。</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet5</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet5, self).__init__()</span><br><span class="line">        <span class="comment"># 卷积层</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        self.fc5 = nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc6 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.OUTPUT = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.max_pool2d(x, (<span class="number">2</span>, <span class="number">2</span>)) <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>) <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>) <span class="comment"># Flatten</span></span><br><span class="line">        x = F.relu(self.fc5(x))</span><br><span class="line">        x = F.relu(self.fc6(x))</span><br><span class="line">        x = self.OUTPUT(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = LeNet5()</span><br><span class="line">output = net(torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"><span class="comment"># print(output)</span></span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</a></p><p>其实本文内容主要是PyTorch的官方教程。</p><p>PyTorch官方教程中代码实现与图片所示的LeNet5不符（PyTorch官方教程代码中是3×3的卷积核，而图片中LeNet5是5×5的卷积核），本文中我是按照图片所示模型结构实现的。</p><p>其实PyTorch开发者和其他开发者也注意到了这一问题，详见：</p><p><a href="https://github.com/pytorch/tutorials/pull/515" target="_blank" rel="noopener">https://github.com/pytorch/tutorials/pull/515</a></p><p><a href="https://github.com/pytorch/tutorials/commit/630802450c13c78f02f744af1c47d1033b6fe206" target="_blank" rel="noopener">https://github.com/pytorch/tutorials/commit/630802450c13c78f02f744af1c47d1033b6fe206</a></p><p><a href="https://github.com/pytorch/tutorials/pull/1257" target="_blank" rel="noopener">https://github.com/pytorch/tutorials/pull/1257</a></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;在本文中，我们基于PyTorch构建一个简单的神经网络LeNet5。&lt;/p&gt;
&lt;p&gt;在阅读本文之前，建议您了解一些卷积神经网络的前置知识，比
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
  </entry>
  
</feed>
