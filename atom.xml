<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>臭咸鱼的缺氧瓶</title>
  
  <subtitle>快给我氧气！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://chouxianyu.github.io/"/>
  <updated>2021-01-28T13:41:15.107Z</updated>
  <id>https://chouxianyu.github.io/</id>
  
  <author>
    <name>臭咸鱼</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PAT甲级1014Waiting in Line</title>
    <link href="https://chouxianyu.github.io/2021/01/28/PAT%E7%94%B2%E7%BA%A71014Waiting-in-Line/"/>
    <id>https://chouxianyu.github.io/2021/01/28/PAT甲级1014Waiting-in-Line/</id>
    <published>2021-01-28T13:38:59.000Z</published>
    <updated>2021-01-28T13:41:15.107Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目介绍"><a href="#题目介绍" class="headerlink" title="题目介绍"></a>题目介绍</h1><ul><li><p>题目链接</p><p>  <a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805498207911936" target="_blank" rel="noopener">https://pintia.cn/problem-sets/994805342720868352/problems/994805498207911936</a></p></li><li><p>题目考点</p><p>  队列和模拟，重难点在于模拟而非队列</p></li><li><p>题目难度</p><p>  PAT甲级30分</p></li><li><p>题目大意</p><ul><li>有N个窗口，每个窗口最多允许M个人站在黄线前排队，剩下的客户在黄线外等候。</li><li>客户进入黄线时会选择人数最少的窗口排队，如果有重复，就选择索引最小的窗口</li><li>客户$C_i$办理业务需要$T_i$分钟。</li><li>前N个客户在8:00开始服务。</li><li>请计算某个客户结束服务的时间。</li></ul></li><li><p>输入</p><ul><li>N：正整数，不超过20，服务窗口的数量</li><li>M：正整数，不超过10，每个服务窗口前最多有几个人等待</li><li>K：正整数，不超过1000，顾客的数量，顾客索引为[1,K]</li><li>Q：要查询的顾客服务完成时间的数量</li><li>K个正整数：每个顾客完成服务需要几分钟</li><li>Q个正整数：需要查询的Q个顾客</li></ul></li><li><p>输出</p><p>  按<code>HH:MM</code>的格式输出Q个顾客完成服务的时间，如果在17:00之前还没有开始服务，则输出Sorry。</p></li></ul><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><ul><li><p>题目中有一个信息并没有十分明确地表达出来：黄线外的客户进入黄线以内时，索引小的人优先。根据这一点，所以从1到K遍历客户就可以。</p></li><li><p>整体思路：计算出所有客户完成服务的时间，最后进行Q个查询并根据分钟数输出完成服务的时间（或sorry）</p></li><li><p>如何计算出所有客户完成服务的时间？其实是个模拟题，<strong>核心在于入队，入队分2个阶段</strong>。</p></li><li><p>为什么核心是入队？<strong>某客户完成服务的时间取决于它前面1个客户完成服务的时间。而在某客户入队时，该客户前面就是队尾客户，所以该客户完成服务的时间=队尾客户完成服务的时间+该客户的服务时长。如果入队时队伍是空的，那该客户完成服务的时间就等于其服务所需时长。</strong></p></li><li><p>入队第1阶段是前N×M个客户入队</p><p>  无需等待黄线，只需按照客户索引顺序依次入队，其中前N个客户是在8:00开始服务。</p><p>  如何让这些客户正确地排队呢？用2层循环。内循环：在n个窗口中选择窗口（从1到N）入队。外循环：使内循环重复m次达到n×m的效果。这样就实现了“选择人数最少、索引最小的窗口”的要求。</p><p>  在客户入队时，我们需要不断更新客户索引，所以当已经没有客户时（即客户索引超过K时）要停止入队；另外所有窗口排满时也要停止入队，这通过双循环的判断条件实现。</p></li><li><p>入队第2阶段是剩下的客户入队</p><p>  假如还有剩下的客户（即客户索引还未超过K时），那就要找出队伍最短的窗口，让下一个客户入队。</p><p>  如何找到队伍最短的窗口呢？在该方法中，并不是通过比较n个窗口的排队长度。因为如果还剩有客户就说明每个窗口都是满的，那队伍最短就没有意义了。而最快有客户结束服务的窗口（如果有重复，就选索引最小的窗口）就是队伍最短的窗口，也就是找到哪个窗口队首客户完成服务的时间最早。</p></li><li><p>其它</p><ul><li><p>关于题目</p><p>  好久没做题目了，脑子和手都很生疏。看到这道题，我就想起来数学建模里做过的题、排队论什么的。思路很乱，我很自闭，这份题解的核心在于抓住了入队的规律。如果是更复杂的排队论问题，也许就需要模拟，模拟出一个时钟，然后某一分钟遍历所有窗口？这种思路确实可以，但效率不高。</p></li><li><p>关于算法题</p><p>  脱离这道题来讲，模拟题（可能大多数算法题也是）都是<strong>找规律</strong>，然后用规律去求结果。</p></li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Problem: PAT Advanced 1014</span></span><br><span class="line"><span class="comment">// URL: https://pintia.cn/problem-sets/994805342720868352/problems/994805498207911936</span></span><br><span class="line"><span class="comment">// Tags: 队列</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; window[<span class="number">20</span>]; <span class="comment">// n个窗口，每个窗口是一个排队队列</span></span><br><span class="line"><span class="keyword">int</span> processingTime[<span class="number">1001</span>]; <span class="comment">// k个客户需要的分钟数，index为[1,k]</span></span><br><span class="line"><span class="keyword">int</span> finishTime[<span class="number">1001</span>]; <span class="comment">// k个客户结束服务的时间，index为[1,k]</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 变量定义并初始化</span></span><br><span class="line">    <span class="keyword">int</span> customerIndex = <span class="number">1</span>; <span class="comment">// 客户索引，后面用来遍历客户</span></span><br><span class="line">    <span class="keyword">int</span> n, m, k, q; <span class="comment">// 窗口个数、单个窗口队伍最大长度、客户数量、查询数量</span></span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d %d %d %d"</span>, &amp;n, &amp;m, &amp;k, &amp;q);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i &lt;= k; i++)</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, processingTime+i);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 处理前n×m个客户（内循环：在n个窗口中选择1个窗口入队。外循环：使内循环重复m次达到n×m的效果）</span></span><br><span class="line">    <span class="keyword">while</span>(m--)&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j &lt; n &amp;&amp; customerIndex &lt;= k; j++)&#123; <span class="comment">// 客户优先选择队伍最短、索引最小的窗口</span></span><br><span class="line">            <span class="keyword">if</span> (window[j].empty())&#123; <span class="comment">// 如果队伍是空的，那该客户在8:00开始服务，其完成服务的时间就等于其服务时长</span></span><br><span class="line">                finishTime[customerIndex] = processingTime[customerIndex];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                finishTime[customerIndex] = finishTime[window[j].back()] + processingTime[customerIndex]; <span class="comment">// 该客户完成服务的时间=队尾客户完成服务的时间+该客户的服务时长</span></span><br><span class="line">            &#125;</span><br><span class="line">            window[j].push(customerIndex); <span class="comment">// 当前客户入队</span></span><br><span class="line">            customerIndex++; <span class="comment">// 更新客户索引（考虑下一位客户）</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果还剩有客户的话，处理剩下的客户</span></span><br><span class="line">    <span class="keyword">while</span>(customerIndex &lt;= k)&#123;</span><br><span class="line">        <span class="keyword">int</span> quickestFinishTime = finishTime[window[<span class="number">0</span>].front()], quickestWindow = <span class="number">0</span>; <span class="comment">// 找出队伍最短的窗口</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (finishTime[window[i].front()] &lt; quickestFinishTime)&#123;</span><br><span class="line">                quickestFinishTime = finishTime[window[i].front()];</span><br><span class="line">                quickestWindow = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        finishTime[customerIndex] = finishTime[window[quickestWindow].back()] + processingTime[customerIndex]; <span class="comment">// 记录当前客户结束服务的时间</span></span><br><span class="line">        window[quickestWindow].pop();</span><br><span class="line">        window[quickestWindow].push(customerIndex);</span><br><span class="line">        </span><br><span class="line">        customerIndex++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> query_customer;</span><br><span class="line">    <span class="keyword">while</span>(q--)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;query_customer);</span><br><span class="line">        <span class="keyword">if</span> (finishTime[query_customer] - processingTime[query_customer] &lt; <span class="number">540</span>)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%02d:%02d\n"</span>, finishTime[query_customer]/<span class="number">60</span>+<span class="number">8</span>, finishTime[query_customer]%<span class="number">60</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Sorry\n"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://blog.csdn.net/liuchuo/article/details/54561626" target="_blank" rel="noopener">https://blog.csdn.net/liuchuo/article/details/54561626</a></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目介绍&quot;&gt;&lt;a href=&quot;#题目介绍&quot; class=&quot;headerlink&quot; title=&quot;题目介绍&quot;&gt;&lt;/a&gt;题目介绍&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;题目链接&lt;/p&gt;
&lt;p&gt;  &lt;a href=&quot;https://pintia.cn/problem-sets
      
    
    </summary>
    
    
      <category term="算法" scheme="https://chouxianyu.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="队列" scheme="https://chouxianyu.github.io/tags/%E9%98%9F%E5%88%97/"/>
    
      <category term="PAT" scheme="https://chouxianyu.github.io/tags/PAT/"/>
    
  </entry>
  
  <entry>
    <title>python批量处理邮件：poplib和email快速上手教程</title>
    <link href="https://chouxianyu.github.io/2021/01/27/python%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86%E9%82%AE%E4%BB%B6%EF%BC%9Apoplib%E5%92%8Cemail%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%95%99%E7%A8%8B/"/>
    <id>https://chouxianyu.github.io/2021/01/27/python批量处理邮件：poplib和email快速上手教程/</id>
    <published>2021-01-27T13:16:36.000Z</published>
    <updated>2021-01-27T13:22:32.595Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><ul><li><p><code>poplib</code>是一个python第三方库，基于它我们可以连接POP3服务器。</p></li><li><p><code>email</code>是一个python内置的模块，基于它我们可以管理邮箱中的邮件。</p></li><li><p>Some Story</p><p>  我现在需要下载很多邮件的附件。我之前有一份相同功能的代码（<a href="https://www.cnblogs.com/chouxianyu/p/11270101.html" target="_blank" rel="noopener">点击这里</a>），发布出来之后博客访问量也挺高。然而，当时由于时间原因我对那份代码只是一知半解，运行起来后就没具体研究。所以趁这个机会我又写了一份代码，原因有很多：一是于己我要把代码细节给搞懂，二是针对这次的需求进行修改，三是水一篇博客（bushi，<strong>这份代码十分简要并且注释十分详细</strong>）。</p></li></ul><h1 id="快速上手代码"><a href="#快速上手代码" class="headerlink" title="快速上手代码"></a>快速上手代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> poplib</span><br><span class="line"><span class="keyword">from</span> email.parser <span class="keyword">import</span> Parser</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">    功能：下载某一个邮件的所有附件</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_attachment</span><span class="params">(email)</span>:</span></span><br><span class="line">    subject = email.get(<span class="string">'Subject'</span>)</span><br><span class="line">    <span class="keyword">for</span> part <span class="keyword">in</span> email.walk():  <span class="comment"># 深度优先遍历整个email的内容</span></span><br><span class="line">        attachment_name = part.get_filename()  <span class="comment"># 获取附件名称</span></span><br><span class="line">        <span class="keyword">if</span> attachment_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># print(attachment_name)</span></span><br><span class="line">            attachment_content = part.get_payload(decode=<span class="keyword">True</span>)  <span class="comment"># 下载附件</span></span><br><span class="line">            attachment_file = open(<span class="string">'./'</span> + attachment_name, <span class="string">'wb'</span>) <span class="comment"># 在指定目录下创建文件，注意二进制文件需要用wb模式打开</span></span><br><span class="line">            attachment_file.write(attachment_content)  <span class="comment"># 将附件保存到本地</span></span><br><span class="line">            attachment_file.close()</span><br><span class="line">    print(<span class="string">'Done'</span>, subject)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""连接到POP3服务器"""</span></span><br><span class="line">    server = poplib.POP3(host=<span class="string">'pop.163.com'</span>) <span class="comment"># 创建一个POP3对象，参数host是指定服务器</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""身份验证"""</span></span><br><span class="line">    server.user(<span class="string">'xxxx@163.com'</span>)  <span class="comment"># 参数是你的邮箱地址</span></span><br><span class="line">    server.pass_(<span class="string">'xxxxx'</span>)  <span class="comment"># 参数是你的邮箱密码</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""获取邮箱状态"""</span></span><br><span class="line">    email_count, email_size = server.stat()</span><br><span class="line">    <span class="comment"># print('邮件数量：%s, 邮件总大小：%s' % (email_count, email_size))</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""获取邮件列表"""</span></span><br><span class="line">    <span class="comment"># response, email_sizes, octets = server.list() # 3个结果分别是响应结果（1个包含是否请求成功、邮件数量、邮件总大小的字符串），邮件信息列表（一个字符串列表，每个元素内容包含邮件index及该邮件的大小），字节包含比特数量（octet专指8bit的字节）</span></span><br><span class="line">    <span class="comment"># print(response)</span></span><br><span class="line">    <span class="comment"># print(email_sizes)</span></span><br><span class="line">    <span class="comment"># print(octets)</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""遍历邮件"""</span></span><br><span class="line">    email_parser = Parser()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(email_count):</span><br><span class="line">        <span class="string">"""获取邮件内容：POP3.retr(which)检索index为which的整个邮件，并将其设为已读"""</span></span><br><span class="line">        response, lines, octets = server.retr(i+<span class="number">1</span>) <span class="comment"># 3个结果分别是响应结果（1个包含是否请求成功和该邮件大小的字符串），邮件内容（一个字符串列表，每个元素是邮件内容的一行），邮件大小（即有多少个octets，octet特指8bit的字节）</span></span><br><span class="line">        <span class="comment"># print(response)</span></span><br><span class="line">        <span class="comment"># print(lines)</span></span><br><span class="line">        <span class="comment"># print(octets)</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">"""将邮件内容拼接成1个字符串，并使用utf-8解码"""</span></span><br><span class="line">        email_raw_content = <span class="string">b'\r\n'</span>.join(lines).decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        <span class="comment"># print(email_raw_content)</span></span><br><span class="line"></span><br><span class="line">        <span class="string">"""将字符串格式的邮件内容转换为email模块支持的格式（&lt;class 'email.message.Message'&gt;）"""</span></span><br><span class="line">        email = email_parser.parsestr(email_raw_content)</span><br><span class="line">        <span class="comment"># for item in email.items():</span></span><br><span class="line">        <span class="comment">#     print(item)</span></span><br><span class="line"></span><br><span class="line">        <span class="string">"""下载邮件中的附件"""</span></span><br><span class="line">        download_attachment(email)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><p><a href="https://docs.python.org/zh-cn/3/library/poplib.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/library/poplib.html</a></p></li><li><p><a href="https://docs.python.org/zh-cn/3/library/email.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/library/email.html</a></p><p>  <strong>自定义修改</strong>：如果需要对上面的代码进行修改，比如需要获取邮件正文而非邮件附件，则可以进一步去看文档（特别是<a href="https://docs.python.org/zh-cn/3/library/email.message.html#module-email.message" target="_blank" rel="noopener"><code>email.message</code></a>）。</p><p>  我注释的一些代码也很重要，建议读者自己写几封测试邮件，把代码看懂，然后再去看文档，就可以啦！</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;poplib&lt;/code&gt;是一个python第三方库，基于它我们可以连接POP3服务
      
    
    </summary>
    
    
      <category term="python" scheme="https://chouxianyu.github.io/tags/python/"/>
    
      <category term="下载邮件附件" scheme="https://chouxianyu.github.io/tags/%E4%B8%8B%E8%BD%BD%E9%82%AE%E4%BB%B6%E9%99%84%E4%BB%B6/"/>
    
      <category term="poplib" scheme="https://chouxianyu.github.io/tags/poplib/"/>
    
      <category term="email" scheme="https://chouxianyu.github.io/tags/email/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-5.3神经网络中的反向传播算法</title>
    <link href="https://chouxianyu.github.io/2021/01/27/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-5-3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"/>
    <id>https://chouxianyu.github.io/2021/01/27/李宏毅机器学习课程笔记-5-3神经网络中的反向传播算法/</id>
    <published>2021-01-27T03:37:36.000Z</published>
    <updated>2021-01-27T03:52:15.446Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="链式法则（Chain-Rule）"><a href="#链式法则（Chain-Rule）" class="headerlink" title="链式法则（Chain Rule）"></a>链式法则（Chain Rule）</h1><ul><li>$z=h(y),y=g(x)\to\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}$</li><li>$z=k(x,y),x=g(s),y=h(s)\to\frac{dz}{ds}=\frac{dz}{dx}\frac{dx}{ds}+\frac{dz}{dy}\frac{dy}{ds}$</li></ul><h1 id="反向传播算法（Backpropagation）"><a href="#反向传播算法（Backpropagation）" class="headerlink" title="反向传播算法（Backpropagation）"></a>反向传播算法（Backpropagation）</h1><h2 id="变量定义"><a href="#变量定义" class="headerlink" title="变量定义"></a>变量定义</h2><p>如下图所示，设神经网络的输入为$x^n$，该输入对应的label是$\hat y^n$，神经网络的参数是$\theta$，神经网络的输出是$y^n$。</p><p>整个神经网络的Loss为$L(\theta)=\sum_{n=1}^{N}C^n(\theta)$。假设$\theta$中有一个参数$w$，那$\frac{\partial L(\theta)}{\partial w}=\sum^N_{n=1}\frac{\partial C^n(\theta)}{\partial w}$。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201104063357Backpropagation1.png" alt="Backpropagation1" style="zoom: 50%;"></p><h2 id="一个神经元的情况"><a href="#一个神经元的情况" class="headerlink" title="一个神经元的情况"></a>一个神经元的情况</h2><p>如下图所示，$z=x_1w_1+x_2w_x+b$，根据链式法则可知$\frac{\partial C}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial C}{\partial z}$，其中为所有参数$w$计算$\frac{\partial z}{\partial w}$是Forward Pass、为所有激活函数的输入$z$计算$\frac{\partial C}{\partial z}$是Backward Pass。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201104064536Backpropagation2.png" alt="Backpropagation2.png" style="zoom: 50%;"></p><h2 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h2><p>Forward Pass是为所有参数$w$计算$\frac{\partial z}{\partial w}$，它的方向是从前往后算的，所以叫Forward Pass。</p><p>以一个神经元为例，因为$z=x_1w_1+x_2w_x+b$，所以$\frac{\partial z}{\partial w_1}=x_1,\frac{\partial z}{\partial w_2}=x_2$，如下图所示。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201108071142ForwardPass1.jpg" alt="Forward Pass1" style="zoom:50%;"></p><p>规律是：该权重乘以的那个输入的值。所以当有多个神经元时，如下图所示。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201108071151ForwardPass2.jpg" alt="Forward Pass2" style="zoom:50%;"></p><h2 id="Backward-Pass"><a href="#Backward-Pass" class="headerlink" title="Backward Pass"></a>Backward Pass</h2><p>Backward Pass是为所有激活函数的输入$z$计算$\frac{\partial C}{\partial z}$，它的方向是从后往前算的，要先算出输出层的$\frac{\partial C}{\partial z}$，再往前计算其它神经元的$\frac{\partial C}{\partial z}$，所以叫Backward Pass。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201108074342BackwardPass1.jpg" alt="BackwardPass1" style="zoom:50%;"></p><p>如上图所示，令$a=\sigma(z)$，根据链式法则，可知$\frac{\partial C}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial C}{\partial a}$，其中$\frac{\partial a}{\partial z}=\sigma’(z)$是一个常数，因为在Forward Pass时$z$的值就已经确定了，而$\frac{\partial C}{\partial a}=\frac{\partial z’}{\partial a}\frac{\partial C}{\partial z’}+\frac{\partial z’’}{\partial a}\frac{\partial C}{\partial z’’}=w_3\frac{\partial C}{\partial z’}+w_4\frac{\partial C}{\partial z’’}$，所以$\frac{\partial C}{\partial z}=\sigma’(z)[w_3\frac{\partial C}{\partial z’}+w_4\frac{\partial C}{\partial z’’}]$。</p><p>对于式子$\frac{\partial C}{\partial z}=\sigma’(z)[w_3\frac{\partial C}{\partial z’}+w_4\frac{\partial C}{\partial z’’}]$，我们可以发现两点：</p><ol><li><p>$\frac{\partial C}{\partial z}$的计算式是递归的，因为在计算$\frac{\partial C}{\partial z}$的时候需要计算$\frac{\partial C}{\partial z’}$和$\frac{\partial C}{\partial z’’}$。</p><p> 如下图所示，输出层的$\frac{\partial C}{\partial z’}$和$\frac{\partial C}{\partial z’’}$是容易计算的。</p><p> <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201108075906BackwardPass3.jpg" alt="BackwardPass3" style="zoom:50%;"></p></li><li><p>$\frac{\partial C}{\partial z}$的计算式$\frac{\partial C}{\partial z}=\sigma’(z)[w_3\frac{\partial C}{\partial z’}+w_4\frac{\partial C}{\partial z’’}]$是一个神经元的形式</p><p> 如下图所示，只不过没有嵌套sigmoid函数而是乘以一个常数$\sigma’(z)$，每个$\frac{\partial C}{\partial z}$都是一个神经元的形式，所以可以通过神经网络计算$\frac{\partial C}{\partial z}$。</p><p> <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201108075902BackwardPass2.jpg" alt="BackwardPass2" style="zoom:50%;"></p></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>通过Forward Pass，为所有参数$w$计算$\frac{\partial z}{\partial w}$；</li><li>通过Backward Pass，为所有激活函数的输入$z$计算$\frac{\partial C}{\partial z}$；</li><li>最后$\frac{\partial C}{\partial w}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial w}$，也就求出了梯度。</li></ol><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;链式法则（Chain-Rule）&quot;&gt;&lt;a href=&quot;#链式法则（Chain-Rule）&quot; class=&quot;headerlink&quot; title=&quot;链式法则（Chain Rule）&quot;&gt;&lt;/a&gt;链式法则（Chain Rule）&lt;/h1&gt;&lt;ul&gt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="反向传播算法" scheme="https://chouxianyu.github.io/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-5.2为什么是深度神经网络</title>
    <link href="https://chouxianyu.github.io/2021/01/27/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-5-2%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://chouxianyu.github.io/2021/01/27/李宏毅机器学习课程笔记-5-2为什么是深度神经网络/</id>
    <published>2021-01-27T03:36:51.000Z</published>
    <updated>2021-01-27T03:52:26.543Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="为什么是“深度”神经网络？"><a href="#为什么是“深度”神经网络？" class="headerlink" title="为什么是“深度”神经网络？"></a>为什么是“深度”神经网络？</h1><h2 id="问题与答案"><a href="#问题与答案" class="headerlink" title="问题与答案"></a>问题与答案</h2><ul><li><p>矮胖的神经网络和高瘦的神经网络，假设它们参数量相同，哪一个更好呢？</p><p>  2011年有一个实验，证明在参数量相当的情况下，高瘦的神经网络（即深度神经网络）的准确度更高，因为深度可以实现模块化。</p></li><li><p>只用一个神经元足够多的隐藏层，这个模型就包括了任意函数，那为什么不这么做呢？</p><p>  这样确实可以包括任意函数，但实现的效率不高。</p><p>  相关网址<a href="http://neuralnetworksanddeeplearning.com/chap4.html，也可以通过谷歌等找找其它答案。" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/chap4.html，也可以通过谷歌等找找其它答案。</a></p></li></ul><h2 id="“深度”的好处"><a href="#“深度”的好处" class="headerlink" title="“深度”的好处"></a>“深度”的好处</h2><ul><li><p><strong>模块化（Modularization）</strong></p><ul><li><p>就像写程序一样，我们不能把所有代码写在main函数里，而需要通过定义函数等方式将程序模块化。</p><p>  如下图所示，假如要做一个图片的四分类，两个维度分别是头发长短和性别，如果使用矮胖的神经网络会遇到一个问题，就是短头发的女生样本和长头发的男生样本会比较少，那这两个类别的分类器就会比较差。</p><p>  <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201110021709WhyDeepModularization1.png" alt="WhyDeepModularization1" style="zoom: 33%;"></p><p>  如下图所示，我们可以先定义各属性的分类器（Classifiers for the attributes），即先定义性别和头发长短的分类器，然后再做四分类。这样第一层分类器就不会遇到样本少的问题，第二层的分类器也容易训练，整体上也需要更少的训练集。</p><p>  <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201110021748WhyDeepModularization2.png" alt="WhyDeepModularization2" style="zoom:33%;"></p></li><li><p>在深度神经网络中，每层网络都可以作为下一层网络使用的一个模块，并且这个模块化是通过机器学习自动得到的。</p><p>  常有“人工智能=机器学习+大数据”的说法，但实际上“深度”使得需要的数据更少，如果数据集无限大，根本就不需要机器学习，只要去数据库里拿就好了。深度学习也并不是通过大量参数暴力拟合出一个模型，反而是在通过模块化有效利用数据。</p><p>  这里只是一个图像分类的例子，“深度”产生的模块化在语音识别任务中也有体现，与逻辑电路也有相似的问题和结论，具体可以看<a href="https://www.bilibili.com/video/BV1JE411g7XF?p=15" target="_blank" rel="noopener">李宏毅视频</a>。</p></li></ul></li><li><p><strong>端到端学习（End-to-end Learning）</strong></p><p>  深度神经网络模型就像是把一个个函数串接在一起，每个函数负责某个功能，每个函数负责什么功能是通过机器学习根据数据自动确定的。<a href="https://www.bilibili.com/video/BV1JE411g7XF?p=15" target="_blank" rel="noopener">李宏毅视频</a>中有讲这一点在语音识别、CV任务中的体现。</p></li><li><p><strong>处理复杂任务</strong></p><p>  有时类似的输入要输出差别很大的结果，比如白色的狗和北极熊看起来差不多，但分类结果非常不同；有时差别很大的输入要输出相同的结果，比如火车正面和侧面的图片都应该被分类成火车。</p><p>  只有一个隐藏层的网络是无法处理这种任务的。<a href="https://www.bilibili.com/video/BV1JE411g7XF?p=15" target="_blank" rel="noopener">李宏毅视频</a>中有讲这一点在语音识别、CV任务中的体现。</p></li><li><p>其它</p><p>  <a href="http://www.oalib.com/paper/4042915" target="_blank" rel="noopener">Do deep nets really need to be deep?</a></p><p>  <a href="https://www.bilibili.com/video/BV1JE411g7XF?p=15" target="_blank" rel="noopener">李宏毅视频</a>里也还有很多关于“深度”的探讨。</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;为什么是“深度”神经网络？&quot;&gt;&lt;a href=&quot;#为什么是“深度”神经网络？&quot; class=&quot;headerlink&quot; title=&quot;为什么是“深度”神经网络？&quot;&gt;&lt;/a&gt;为什么是“深度”神经网络？&lt;/h1&gt;&lt;h2 id=&quot;问题与答案&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-5.1深度学习之引言</title>
    <link href="https://chouxianyu.github.io/2021/01/27/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-5-1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%BC%95%E8%A8%80/"/>
    <id>https://chouxianyu.github.io/2021/01/27/李宏毅机器学习课程笔记-5-1深度学习之引言/</id>
    <published>2021-01-27T03:35:22.000Z</published>
    <updated>2021-01-27T03:52:57.765Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h1 id="深度学习的历史"><a href="#深度学习的历史" class="headerlink" title="深度学习的历史"></a>深度学习的历史</h1><ul><li><p>1958年：心理学家Rosenblatt提出感知机（Perceptron）</p><p>  它是一个线性模型。</p></li><li><p>1969年：有人说感知机是线性模型，具有局限性。</p></li><li><p>1980年代：多层感知机（Multi-layer Perceptron）</p><p>  和当今的神经网络是没有本质差别的。</p></li><li><p>1986年：Hinton提出反向传播算法（Backpropagation）</p><p>  但是超过3个隐藏层的神经网络，还是训练不出好的结果。</p></li><li><p>1989年：有人提出一个隐藏层就可以得到任何函数，为什么要多层？</p><p>  多层感知机慢慢淡出大家的视野。</p></li><li><p>2006年：受限玻尔兹曼机初始化（RBM Initialization）</p><p>  Hinton提出用受限玻尔兹曼机做初始化，很多人觉得这是个大突破，但实际上用处并不大。</p><p>  至少让多层感知机回到大家的视野。</p></li><li><p>2009年：GPU</p></li><li><p>2011年：神经网络用于语音识别</p></li><li><p>2012年：神经网络技术赢得ILSVRC（ImageNet Large Scale Visual Recognition Challenge）</p></li></ul><h1 id="深度学习的三个步骤"><a href="#深度学习的三个步骤" class="headerlink" title="深度学习的三个步骤"></a>深度学习的三个步骤</h1><p>和机器学习一样：</p><ol><li><p>确定模型（Model）/函数集（Function Set），在深度学习中就是定义一个神经网络。</p><p> 不同的连接会构成多样的网络结构。</p></li><li><p>确定如何评价函数的好坏</p><p> 如果是多分类，那和Classification一章中一样，计算每个样本预测结果与Ground Truth的交叉熵，然后求和，即为Loss。</p></li><li><p>确定如何找到最好的函数</p><p> 还是Gradient Descent。</p><p> 神经网络模型对应的函数比较复杂，而反向传播算法（Backpropagation）是一个很有效的计算神经网络梯度的方法。</p></li></ol><h1 id="神经网络的结构"><a href="#神经网络的结构" class="headerlink" title="神经网络的结构"></a>神经网络的结构</h1><ul><li><p>输入层（Input Layer）</p><p>  实际上就是输入，并不是真正的“层”。</p></li><li><p>隐藏层（Hidden Layers）</p><p>  输入层和输出层之间的层。Deep指有很多隐藏层，多少层才算Deep并没有统一标准。</p><p>  可以看成特征提取器（Feature Extractor），作用是代替特征工程（Feature Engineering）。</p></li><li><p>输出层（Output Layer）</p><p>  最后一层。</p><p>  可以看成分类器</p></li></ul><h1 id="全连接前馈神经网络"><a href="#全连接前馈神经网络" class="headerlink" title="全连接前馈神经网络"></a>全连接前馈神经网络</h1><p>即Fully Connected Feedforward Neural Network，FFN。</p><ul><li>全连接是指每个神经元与上一层的所有神经元相连。</li><li>前馈神经网络（FNN，Feedforward Neural Network）是指各神经元分层排列，每个神经元只与前一层的神经元相连，接收前一层的输出，并输出给下一层，各层间没有反馈。</li></ul><h1 id="一些网络"><a href="#一些网络" class="headerlink" title="一些网络"></a>一些网络</h1><p>其中Residual Net并不是一般的全连接前馈神经网络</p><div class="table-container"><table><thead><tr><th style="text-align:center">网络结构</th><th style="text-align:center">提出年份</th><th style="text-align:center">层数</th><th style="text-align:center">ImageNet错误率</th></tr></thead><tbody><tr><td style="text-align:center">AlexNet</td><td style="text-align:center">2012</td><td style="text-align:center">8</td><td style="text-align:center">16.4%</td></tr><tr><td style="text-align:center">VGGNet</td><td style="text-align:center">2014</td><td style="text-align:center">19</td><td style="text-align:center">7.3%</td></tr><tr><td style="text-align:center">GoogleNet</td><td style="text-align:center">2014</td><td style="text-align:center">22</td><td style="text-align:center">6.7%</td></tr><tr><td style="text-align:center">Residual Net</td><td style="text-align:center">2015</td><td style="text-align:center">152</td><td style="text-align:center">3.57%</td></tr></tbody></table></div><h1 id="机器学习和深度学习面对的不同问题"><a href="#机器学习和深度学习面对的不同问题" class="headerlink" title="机器学习和深度学习面对的不同问题"></a>机器学习和深度学习面对的不同问题</h1><ul><li>在机器学习中，人类需要手工做特征工程（Feature Engineering），人类需要思考如何提取特征。</li><li>有了深度学习以后，人类可以不做特征工程，但也遇到了新的问题：人类需要设计合适的网络结构。</li></ul><p>这两个问题哪个更容易呢？可能后者更容易些，比如在图像识别、语音识别任务中，人类可能并不知道自己是如何识别图像和语音的，就无法通过符号主义进行特征工程。</p><h1 id="关于深度学习的一些疑问"><a href="#关于深度学习的一些疑问" class="headerlink" title="关于深度学习的一些疑问"></a>关于深度学习的一些疑问</h1><ul><li><p>虽然深度学习的的准确度很高，但是它使用的参数更多，参数多、准确度高也是很正常的事，所以有什么特别之处呢？</p></li><li><p>只用一个神经元足够多的隐藏层，这个模型就包括了任意函数，那为什么不这么做而非要深度呢？为什么要是Deep而不是Fat呢？</p></li><li><p>如何设计神经网络的结构？</p><p>  多少层？每一层有多少个神经元？</p><p>  只能凭经验（实验结果）和直觉，当然可以让机器自己去找网络结构，即网络架构搜索（NAS，Network Architecture Search）。</p></li><li><p>必须用全连接前馈神经网络吗？</p><p>  不是。比如卷积神经网络（Convolutional Neural Networks, CNN）。</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;深度学习的历史&quot;&gt;&lt;a href=&quot;#深度学习的历史&quot; class=&quot;headerlink&quot; title=&quot;深度学习的历史&quot;&gt;&lt;/a&gt;深度学习的历史&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1958年：心理学家Rosenblatt提出感知机（Pe
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-4.3分类模型之逻辑回归</title>
    <link href="https://chouxianyu.github.io/2021/01/17/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4-3%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>https://chouxianyu.github.io/2021/01/17/李宏毅机器学习课程笔记-4-3分类模型之逻辑回归/</id>
    <published>2021-01-17T06:31:35.000Z</published>
    <updated>2021-01-17T06:52:51.280Z</updated>
    
    <content type="html"><![CDATA[<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>假设训练集如下，有2个类别$C_1$和$C_2$，表格中的每列为一个样本。</p><p>例如，第一列表示样本$x^1$的类别为$C_1$，所以它的标签$\hat y^1$是1。</p><div class="table-container"><table><thead><tr><th style="text-align:center">$x^1$</th><th style="text-align:center">$x^2$</th><th style="text-align:center">$x^2$</th><th style="text-align:center">$\dots$</th><th style="text-align:center">$x^N$</th></tr></thead><tbody><tr><td style="text-align:center">$C_1$</td><td style="text-align:center">$C_1$</td><td style="text-align:center">$C_2$</td><td style="text-align:center">$\dots$</td><td style="text-align:center">$C_1$</td></tr><tr><td style="text-align:center">$\hat y^1=1$</td><td style="text-align:center">$\hat y^2=1$</td><td style="text-align:center">$\hat y^3=0$</td><td style="text-align:center">$\dots$</td><td style="text-align:center">$\hat y^N=1$</td></tr></tbody></table></div><h2 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h2><p>在分类（Classification）一节中，我们要找到一个模型$P_{w,b}(C_1|x)$，如果$P_{w,b}(C_1|x)\geq0.5$，则$x$属于类别$C_1$，否则属于类别$C_2$。</p><p>可知$P_{w,b}(C_1|x)=\sigma(z)$，其中$\sigma(z)=\frac{1}{1+e^{-z}}$（Sigmoid Function），$z=w\cdot x+b=\sum_{i=1}^Nw_ix_i+b$。</p><p>最终我们找到了模型$f_{w,b}(x)=\sigma(\sum_{i=1}^Nw_ix_i+b)$，<strong>这其实就是逻辑回归（Logistic Regression）</strong>。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>从模型$f_{w,b}(x)=P_{w,b}(C_1|x)$中取样得到训练集的概率为</strong>：$L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\dots f_{w,b}(x^N)$（似然函数）。</p><p>我们要求$w^<em>,b^</em>=arg\ max_{w,b}L(w,b)$，等同于$w^<em>,b^</em>=arg\ min_{w,b}-lnL(w,b)$（对数似然方程，Log-likelihood Equation）。</p><p>而$-lnL(w,b)=-lnf_{w,b}(x^1)-lnf_{w,b}(x^2)-ln(1-f_{w,b}(x^3))\dots$，其中$lnf_{w,b}(x^n)=\hat y^nlnf_{w,b}(x^n)+(1-\hat y^n)ln(1-f(x^n))$，所以$-lnL(w,b)=\sum_{n=1}^N-[\hat y^nlnf_{w,b}(x^n)+(1-\hat y^n)ln(1-f_{w,b}(x^n))]$，式中$n$用来选择某个样本。</p><p>假设有两个伯努利分布$p$和$q$，在$p$中有$p(x=1)=\hat y^n,p(x=0)=1-\hat y^n$，在$q$中有$q(x=1)=f(x^n),q(x=0)=1-f(x^n)$，则$p$和$q$的<strong>交叉熵</strong>（Cross Entropy，代表两个分布有多接近，两个分布一模一样时交叉熵为0）为$H(p,q)=-\sum_xp(x)ln(q(x))$。</p><p>所以损失函数$L(f)=\sum_{n=1}^NC(f(x^n),\hat y^n)$，其中$C(f(x^n),\hat y^n)=-[\hat y^nlnf_{w,b}(x^n)+(1-\hat y^n)ln(1-f_{w,b}(x^n))]$，即损失函数为所有样本的$f(x^n)$与$\hat y^n$的交叉熵之和，式中$n$用来选择某个样本。</p><h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>$\frac{-lnL(w,b)}{\partial w_i}=\sum_{n=1}^{N}-(\hat y^n-f_{w,b}(x^n))x_i^n$（推导过程省略，具体见<a href="https://www.bilibili.com/video/BV1JE411g7XF?p=11" target="_blank" rel="noopener">李宏毅机器学习视频</a>14分56秒），其中$i$用来选择数据的某个维度，$n$用来选择某个样本，$N$为数据集中样本个数。</p><p>该式表明，预测值与label相差越大时，参数更新的步幅越大，这符合常理。</p><h1 id="逻辑回归VS线性回归"><a href="#逻辑回归VS线性回归" class="headerlink" title="逻辑回归VS线性回归"></a>逻辑回归VS线性回归</h1><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101070942LogisticRegressionVSLinearRegression.png" alt="LogisticRegressionVSLinearRegression" style="zoom: 33%;"></p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>逻辑回归模型比线性回归模型多了一个sigmoid函数；</p><p>逻辑回归输出是[0,1]，而线性回归的输出是任意值。</p><h2 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h2><p>逻辑回归模型使用的训练集中label的值必须是0或1，而线性回归模型训练集中label的值是真实值。</p><p>图中线性回归损失函数中的$\frac{1}{2}$是为了方便求导</p><p>这里有一个问题，为什么逻辑回归模型中不使用Square Error呢？这个问题的答案见下文</p><h2 id="梯度-1"><a href="#梯度-1" class="headerlink" title="梯度"></a>梯度</h2><p>逻辑回归模型和线性回归模型的梯度公式一样</p><h1 id="为什么逻辑回归模型中不使用Square-Error"><a href="#为什么逻辑回归模型中不使用Square-Error" class="headerlink" title="为什么逻辑回归模型中不使用Square Error"></a>为什么逻辑回归模型中不使用Square Error</h1><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071052LogisticRegressionWithSquareError.png" alt="LogisticRegressionWithSquareError" style="zoom:33%;"></p><p>由上图可知，当label的值为1时，不管预测值是0还是1，梯度都为0，当label值为0时也是这样。</p><p>如下图所示，如果在逻辑回归中使用Square Error，当梯度接近0时，我们无法判断目前与最优解的距离，也就无法调节学习率；并且在大多数时候梯度都是接近0的，收敛速度会很慢。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071124CrossEntropyVSSquareError.png" alt="CrossEntropyVSSquareError" style="zoom:33%;"></p><h1 id="判别模型VS生成模型"><a href="#判别模型VS生成模型" class="headerlink" title="判别模型VS生成模型"></a>判别模型VS生成模型</h1><h2 id="形式对比"><a href="#形式对比" class="headerlink" title="形式对比"></a>形式对比</h2><p>逻辑回归是一个判别模型（Discriminative Model），用正态分布描述后验概率（Posterior Probability）则是生成模型（Generative Model）。</p><p><strong>如果生成模型中共用协方差矩阵，那两个模型/函数集其实是一样的，都是$P(C_1|x)=\sigma(w\cdot x+b)$。</strong></p><p><strong>因为做了不同的假设，即使是使用同一个数据集、同一个模型，找到的函数是不一样的。</strong></p><h2 id="优劣对比"><a href="#优劣对比" class="headerlink" title="优劣对比"></a>优劣对比</h2><ul><li>如果现在数据很少，当假设了概率分布之后，就可以需要更少的数据用于训练，受数据影响较小；而判别模型就只根据数据来学习，易受数据影响，需要更多数据。</li><li>当假设了概率分布之后，生成模型受数据影响较小，对噪声的鲁棒性更强。</li><li>对于生成模型来讲，先验的和基于类别的概率（Priors and class-dependent probabilities），即$P(C_1)$和$P(C_2)$，可以从不同的来源估计得到。以语音识别为例，如果使用生成模型，可能并不需要声音的数据，网上的文本也可以用来估计某段文本出现的概率。</li></ul><h1 id="多分类（Multi-class-Classification）"><a href="#多分类（Multi-class-Classification）" class="headerlink" title="多分类（Multi-class Classification）"></a>多分类（Multi-class Classification）</h1><p>以3个类别$C_1$、$C_2$和$C_3$为例，分别对应参数$w^1,b_1$、$w^2,b_2$和$w^3,b_3$，即$z_1=w^1\cdot x+b_1$、$z_2=w^2\cdot x+b_2$和$z_3=w^3\cdot x+b_3$。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071155Multi-class%20Classification.png" alt="Multi-class Classification" style="zoom:33%;"></p><h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>使用Softmax（$y_i=\frac{e^{z_i}}{\sum_{j=1}^3e^{z_j}}$），使得$0&lt;y_i&lt;1$以及$\sum_{i=1}^3y_i=1$，其中$y_i=P(C_i|x)$，即一个样本$x$属于类别$C_i$的概率不超过1，属于所有类别的概率之和为1。==Softmax公式中的z是一个参数吗？怎么确定或求得？==</p><p>Softmax公式中为什么要用$e$？这是有原因/可解释的，可以看下PRML，也可以搜下最大熵。</p><p>最大熵（Maximum Entropy）其实也是一种分类器，和逻辑回归一样，只是从<strong>信息论</strong>的角度来看待。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071220Softmax.png" alt="Softmax" style="zoom:33%;"></p><h2 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h2><p>计算预测值$y$和$\hat y$的交叉熵，$y$和$\hat y$都是一个向量，即$-\sum_{i=1}^3\hat y^ilny^i$。</p><p>这时需要使用one-hot编码：如果$x\in C_1$，则$y=\begin{bmatrix}1\\0\\0\end{bmatrix}$；如果$x\in C_2$，则$y=\begin{bmatrix}0\\1\\0\end{bmatrix}$；如果$x\in C_3$，则$y=\begin{bmatrix}0\\0\\1\end{bmatrix}$。</p><h2 id="梯度-2"><a href="#梯度-2" class="headerlink" title="梯度"></a>梯度</h2><p>和逻辑回归的思路一样。</p><h1 id="逻辑回归的局限"><a href="#逻辑回归的局限" class="headerlink" title="逻辑回归的局限"></a>逻辑回归的局限</h1><p>如下图所示，假如有2个类别，数据集中有4个样本，每个样本有2维特征，将这4个样本画在图上。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071300LimitationOfLogisticRegression1.png" alt="LimitationOfLogisticRegression1" style="zoom:33%;"></p><p>如下图所示，假如用逻辑回归做分类，即$y=\sigma(z)=\sigma(w_1x_1+w_2x_2+b)$，我们找不到一个可以把“蓝色”样本和“红色”样本间隔开的函数。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071327LimitationOfLogisticRegression2.png" alt="LimitationOfLogisticRegression2" style="zoom:33%;"></p><p>假如一定要用逻辑回归，那我们可以怎么办呢？我们可以尝试特征变换（Feature Transformation）。</p><h1 id="特征变换（Feature-Transformation）"><a href="#特征变换（Feature-Transformation）" class="headerlink" title="特征变换（Feature Transformation）"></a>特征变换（Feature Transformation）</h1><p>在上面的例子中，我们并不能找到一个能将蓝色样本和红色样本间隔开的函数。</p><p>如下图所示，我们可以把原始的数据/特征转换到另外一个空间，在这个新的特征空间中，找到一个函数将“蓝色”样本和“红色”样本间隔开。</p><p>比如把原始的两维特征变换为与$\begin{bmatrix}0\\0\end{bmatrix}$和$\begin{bmatrix}1\\1\end{bmatrix}$的距离，在这个新的特征空间中，“蓝色”样本和“红色”样本是可分的。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071357LimitationOfLogisticRegression3.png" alt="LimitationOfLogisticRegression3" style="zoom:33%;"></p><p>但有一个问题是，我们并不一定知道怎么进行特征变换。或者说我们想让机器自己学会特征变换，这可以通过级联逻辑回归模型实现，即把多个逻辑回归模型连接起来，如下图所示。</p><p>下图中有3个逻辑回归模型，根据颜色称它们为小蓝、小绿和小红。小蓝和小绿的作用是分别将原始的2维特征变换为新的特征$x_1’$和$x_2’$，小红的作用是在新的特征空间$\begin{bmatrix}x_1’\\x_2’\end{bmatrix}$上将样本分类。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071425CascadingLogisticRegressionModels.png" alt="CascadingLogisticRegressionModels" style="zoom:33%;"></p><p>如下图所示，举一个例子。小蓝的功能是（下图左上角），离$(1,0)$越远、离$(0,1)$越近，则$x_1’$越大；小蓝的功能是（下图左下角），离$(1,0)$越远、离$(0,1)$越近，则$x_2’$越小。小蓝和小绿将特征映射到新的特征空间$\begin{bmatrix}x_1’\\x_2’\end{bmatrix}$中，结果见下图右下角，然后小红就能找到一个函数将“蓝色”样本和“红色”样本间隔开。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071449CascadingLogisticRegressionModels2.png" alt="CascadingLogisticRegressionModels2" style="zoom:33%;"></p><h1 id="神经网络（Neural-Network）"><a href="#神经网络（Neural-Network）" class="headerlink" title="神经网络（Neural Network）"></a>神经网络（Neural Network）</h1><p>假如把上例中的一个逻辑回归叫做神经元（Neuron），那我们就形成了一个神经网络。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;逻辑回归&quot;&gt;&lt;a href=&quot;#逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归&quot;&gt;&lt;/a&gt;逻辑回归&lt;/h1&gt;&lt;p&gt;假设训练集如下，有2个类别$C_1$和$C_2$，表格中的每列为一个样本。&lt;/p&gt;
&lt;p&gt;例如，第一列表示样本$x^1$的类
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="https://chouxianyu.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="https://chouxianyu.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="分类" scheme="https://chouxianyu.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Softmax" scheme="https://chouxianyu.github.io/tags/Softmax/"/>
    
      <category term="特征变换" scheme="https://chouxianyu.github.io/tags/%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-4.2分类模型之概率生成模型</title>
    <link href="https://chouxianyu.github.io/2021/01/17/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4-2%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E4%B9%8B%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    <id>https://chouxianyu.github.io/2021/01/17/李宏毅机器学习课程笔记-4-2分类模型之概率生成模型/</id>
    <published>2021-01-17T06:28:04.000Z</published>
    <updated>2021-01-17T07:22:19.037Z</updated>
    
    <content type="html"><![CDATA[<h1 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h1><p>$P(A\cap B)=P(A)P(B|A)=P(B)P(A|B)$</p><p>$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</p><h1 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h1><p>$P(B)=\sum_{i=1}^nP(A_i)P(B|A_i)$</p><h1 id="概率生成模型（Probalitity-Generative-Model）"><a href="#概率生成模型（Probalitity-Generative-Model）" class="headerlink" title="概率生成模型（Probalitity Generative Model）"></a>概率生成模型（Probalitity Generative Model）</h1><h2 id="理论与定义"><a href="#理论与定义" class="headerlink" title="理论与定义"></a>理论与定义</h2><p>假设有两个类别$C_1$和$C_2$，要判断对象$x$属于哪个类别，即计算$x$属于类别$C_1$的概率，这样把分类问题变成了概率计算问题。</p><ol><li><p>根据贝叶斯公式（Bayes’ theorem）和全概率公式（Total Probability Theorem）可以知道，$x$属于类别$C_1$的概率为$P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x)}=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}$，如果$P(C_1|x)&gt;0.5$则类别为$C_1$，否则类别为$C_2$。</p></li><li><p>概率生成模型的意思就是可以通过这个模型生成一个$x$。</p><p> 具体来讲就是，根据$P(x)=P(x|C_1)P(C_1)+P(x|C_2)P(C_2)$计算出$P(x)$，就可以知道$x$的分布进而生成$x$。如果想要计算出$P(x)$，就要根据训练集估计出$P(C_1)$、$P(x|C_1)$、$P(C_2)$、$P(x|C_2)$这四个值。</p><p> 更直观一点地讲，每个类别就是一个多元正态分布，其中多元是因为每个样本有多个维度的特征。</p></li><li><p>可以根据数据集中属于两个类别的对象的数量计算$P(C_1)$和$P(C_2)$这两个先验概率（Prior Probability）。</p><p> 如果有2个样本属于类别$C_1$，4个样本属于类别$C_2$，那$P(C_1)=\frac{1}{3}$、$P(C_2)=\frac{2}{3}$。</p></li><li><p>要计算后验概率（Posterior Probability）$P(x|C_1)$和$P(x|C_2)$，可以假设训练集中的各类别样本的特征分别是从某个多元正态分布（多元对应特征的多维）中取样得到的，或者说是假设训练集中各类别样本的特征分别符合某多元正态分布。</p><p> <strong>该正态分布的输入是一个样本的特征$x$，输出为样本$x$是从这个正态分布取样得到（或者说该样本属于某类别）的概率密度，然后通过积分就可以求得$P(x|C_1)$和$P(x|C_2)$。</strong></p></li><li><p>正态分布公式为$f_{\mu,\Sigma}(x)=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$。</p><p> 正态分布有2个参数，即均值$\mu$（代表正态分布的中心位置）和协方差矩阵（Covariance Matrix）$\Sigma$（代表正态分布的离散程度），计算出均值$\mu$和协方差$\Sigma$即可得到该正态分布。</p><p> 公式中的$D$为多维特征的维度。</p></li><li><p>实际上从任何一个正态分布中取样都有可能得到训练集中的特征，只是概率不同而已。通过极大似然估计（Maximum Likelihood Estimate，MLE），我们可以<strong>找到取样得到训练集特征的概率最大的那个正态分布</strong>，假设其均值和协方差矩阵为$\mu^<em>$和$\Sigma^</em>$。</p><ol><li><p>根据某正态分布的均值$\mu$和协方差$\Sigma$，可以计算出从该正态分布取样得到训练集的概率。$L(\mu,\Sigma)=f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)\dots f_{\mu,\Sigma}(x^N)$，这就是似然函数（Likelihood Function），其中$N$是训练集中某个类别样本的数量。</p></li><li><p>$\mu^<em>,\Sigma^</em>=arg\ max_{\mu,\Sigma}L(\mu,\Sigma)$。</p><p> 当然可以求导。</p><p> 直觉：$\mu^<em>=\frac{1}{N}\sum_{i=1}^Nx^i$，$\Sigma^</em>=\frac{1}{N}\sum_{i=1}^N(x^i-\mu^<em>)(x^i-\mu^</em>)T$。</p></li></ol></li></ol><h2 id="协方差矩阵共享"><a href="#协方差矩阵共享" class="headerlink" title="协方差矩阵共享"></a>协方差矩阵共享</h2><p>每个类别的特征符合一个多元正态分布，每个多元正态分布也有不同的均值和协方差矩阵。让每个类别对应的多元正态分布共享一个协方差矩阵（各个协方差矩阵的加权平均和），公式为$\Sigma=\frac{N_1}{N_1+N_2}\Sigma^1+\frac{N_2}{N_1+N_2}\Sigma^2$，可以减少模型参数，缓解过拟合。</p><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><ul><li><p>定义</p><p>  极大似然估计指已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，然后通过若干次试验，观察其结果，利用结果推出参数的大概值。一般说来，在一次试验中如果事件A发生了，则认为此时的参数值会使得$P(A|\theta)$最大，极大似然估计法就是要这样估计出的参数值，使所选取的样本在被选的总体中出现的可能性为最大。</p></li><li><p>求极大似然函数估计值的一般步骤：</p><ol><li>写出似然函数</li><li>对似然函数取对数，并整理</li><li>求导数</li><li>解似然方程</li></ol></li><li><p>当共享协方差矩阵时</p><p>  此时似然函数是$L(\mu^1,\mu^2,\Sigma)=f_{\mu^1,\Sigma}(x^1)f_{\mu^2,\Sigma}(x^2)\dots f_{\mu^1,\Sigma}(x^{N_1})\times f_{\mu^2,\Sigma}(x^{N_1+1})f_{\mu^2,\Sigma}(x^{N_1+2})\dots f_{\mu^2,\Sigma}(x^{N_1+N_2})$，其中$N_1$为训练集中类别$C_1$的样本数、$N_2$为训练集中类别$C_2$的样本数。</p><p>  当只有两个类别、两个特征时，如果共享协方差矩阵，那最终得到的两个类别的分界线是直线（横纵轴是两个特征），这一点可以在下文解释。</p><p>  <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201029081008ClassificationLinearBoundary.png" alt="ClassificationLinearBoundary.png" style="zoom: 50%;"></p></li><li><p>除了正态分布，还可以用其它的概率模型。</p><p>  比如对于二值特征，可以使用伯努利分布（Bernouli Distribution）。</p></li><li><p>朴素贝叶斯分类</p><p>  如果假设样本各个维度的数据是互相独立的，那这就是朴素贝叶斯分类器（Naive Bayes Classfier）。</p></li></ul><h1 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h1><p>由上面我们知道$P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1}{1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}$，</p><p>令$z=ln\frac{P(x|C_1P(C_1)}{P(x|C_2P(C_2))}$，则$P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1}{1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}=\frac{1}{1+e^{-z}}=\sigma(z)$，这就是Sigmoid函数。</p><p>如果共享协方差矩阵，经过运算可以得到$z=w^T\cdot x+b$的形式，其中常量$w^T=(\mu^1-\mu^2)^T\Sigma^{-1}$，常量$b=-\frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2+ln\frac{N_1}{N_2}$，即形如$P(C_1|x)=\sigma(w\cdot x+b)$。</p><p>我们最终得到了一个这么简单的一个式子，有一个问题是，我们假设了分布、用了一堆概率，为什么不能直接定义线性模型呢？该问题的答案在下一篇文章<code>李宏毅机器学习课程笔记-4.3分类模型之逻辑回归</code>中的<code>判别模型VS生成模型</code>部分。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;贝叶斯公式&quot;&gt;&lt;a href=&quot;#贝叶斯公式&quot; class=&quot;headerlink&quot; title=&quot;贝叶斯公式&quot;&gt;&lt;/a&gt;贝叶斯公式&lt;/h1&gt;&lt;p&gt;$P(A\cap B)=P(A)P(B|A)=P(B)P(A|B)$&lt;/p&gt;
&lt;p&gt;$P(A|B)=\frac{P(
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://chouxianyu.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="贝叶斯" scheme="https://chouxianyu.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
      <category term="概率生成模型" scheme="https://chouxianyu.github.io/tags/%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="Sigmoid" scheme="https://chouxianyu.github.io/tags/Sigmoid/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-4.1分类简介及其与回归的区别</title>
    <link href="https://chouxianyu.github.io/2021/01/17/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4-1%E5%88%86%E7%B1%BB%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%85%B6%E4%B8%8E%E5%9B%9E%E5%BD%92%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://chouxianyu.github.io/2021/01/17/李宏毅机器学习课程笔记-4-1分类简介及其与回归的区别/</id>
    <published>2021-01-17T06:27:42.000Z</published>
    <updated>2021-01-17T07:16:13.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分类模型应用案例（Classification-Cases）"><a href="#分类模型应用案例（Classification-Cases）" class="headerlink" title="分类模型应用案例（Classification Cases）"></a>分类模型应用案例（Classification Cases）</h1><ul><li>信用评分（Credit Scoring）<ul><li>输入：收入、储蓄、职业、年龄、信用历史等等</li><li>输出：是否贷款</li></ul></li><li>医疗诊断（Medical Diagnosis）<ul><li>输入：现在症状、年龄、性别、病史</li><li>输出：哪种疾病</li></ul></li><li>手写文字识别（Handwritten Character Recognition）<ul><li>输入：文字图片</li><li>输出：是哪一个汉字</li></ul></li><li>人脸识别（Face Recognition）<ul><li>输入：面部图片</li><li>输出：是哪个人</li></ul></li></ul><h1 id="把分类当成回归去做？"><a href="#把分类当成回归去做？" class="headerlink" title="把分类当成回归去做？"></a>把分类当成回归去做？</h1><p>不行。</p><ul><li><p>假设有两个类别，其中类别1的标签为1，类别2的标签为-1，那0就是分界线，大于0就是类别1，小于0就是类别2。</p><p>  回归模型会惩罚那些太正确的样本。如果结果远远大于1，它的分类应该是类别1还是类别2？这时为了降低整体误差，需要调整已经找到的回归函数，就会导致结果的不准确。</p><p>  <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201028115009image-20201028194917163.png" alt="image-20201028194917163" style="zoom:50%;"></p></li><li><p>假设有多个类别，类别1的标签是1，类别2的标签是2，类别3的标签是3。</p><p>  这样的话，标签间具有2和3相近、3大于2这种本来不存在的数字关系。</p></li></ul><h1 id="理想替代方案（Ideal-Alternatives）"><a href="#理想替代方案（Ideal-Alternatives）" class="headerlink" title="理想替代方案（Ideal Alternatives）"></a>理想替代方案（Ideal Alternatives）</h1><ul><li><p>模型</p><p>  模型可以根据特征判断类型，输入是特征，输出是类别</p></li><li><p>损失函数</p><p>  预测错误的次数，即$L(f)=\sum_n\delta(f(x^n)\neq\hat y^n)$。</p><p>  这个函数不可微</p></li><li><p>如何找到最好的函数</p><p>  比如感知机（Perceptron）、支持向量机（SVM）</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;分类模型应用案例（Classification-Cases）&quot;&gt;&lt;a href=&quot;#分类模型应用案例（Classification-Cases）&quot; class=&quot;headerlink&quot; title=&quot;分类模型应用案例（Classification Cases）&quot;&gt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://chouxianyu.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="回归" scheme="https://chouxianyu.github.io/tags/%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-3.梯度下降精讲</title>
    <link href="https://chouxianyu.github.io/2020/12/27/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%B2%BE%E8%AE%B2/"/>
    <id>https://chouxianyu.github.io/2020/12/27/李宏毅机器学习课程笔记-3-梯度下降精讲/</id>
    <published>2020-12-27T10:03:48.000Z</published>
    <updated>2020-12-27T10:26:29.552Z</updated>
    
    <content type="html"><![CDATA[<h1 id="梯度下降伪代码"><a href="#梯度下降伪代码" class="headerlink" title="梯度下降伪代码"></a>梯度下降伪代码</h1><p>梯度下降可以优化损失函数的值，使其尽量小，即可找到最好（在数据集上拟合效果最好）的模型参数。</p><p>现在假设模型$f$中只有一个参数$w$，则损失函数为$L(f)=L(w)$，梯度下降算法如下（若模型有多个参数，按相同方法更新各参数）</p><ol><li><p>初始化参数</p><p> 随机选取一个$w^0$（$w^0$并不一定是随机选取），令$w=w^0$。</p></li><li><p>计算梯度</p><p> $\frac{dL(f)}{dw}|_{w=w^0}$</p><p> 如果小于0，此时$w$增大则$L(f)$会减小；如果大于0，此时$w$减小则$L(w)$会减小。</p><p> 如果模型有多个参数，则计算损失函数在各个参数方向上的偏导数。</p></li><li><p>更新模型参数</p><p> $w^1=w^0-lr\frac{dL(f)}{dw}|_{w=w^0}$</p><p> $w$的变化量取决于梯度和学习率（Learning Rate）的大小：梯度绝对值或学习率越大，则$w$变化量越大。</p><p> 如果模型有多个参数，则用上一步计算出的偏导数对应更新各参数。</p></li><li><p>重复第2步和第3步</p><p> 经过多次参数更新/迭代（iteration），可以使损失函数的值达到局部最小（即局部最优，Local Optimal），但不一定是全局最优。</p></li></ol><h1 id="自适应学习率（Adaptive-Learning-Rate）"><a href="#自适应学习率（Adaptive-Learning-Rate）" class="headerlink" title="自适应学习率（Adaptive Learning Rate）"></a>自适应学习率（Adaptive Learning Rate）</h1><p>梯度下降过程中，固定学习率并不合理。学习率太大，可能导致loss不减小反而增大；学习率太小，loss会减小得很慢。</p><p>基本原则是随着参数迭代更新，学习率应该越来越小，比如$\eta^{t}=\frac{\eta}{\sqrt{t+1}}$。</p><p>更好的方法：每个参数有各自的学习率，比如Adagrad。</p><h1 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h1><p>Adaptive Gradient Descent，自适应梯度下降。2011年提出，核心是每个参数（parameter）有不同的学习率</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>每次迭代中，学习率要除以它对应参数的之前梯度的均方根（RMS） 。</p><p>即$w^{t+1}=w^t-\frac{\eta}{\sqrt{\sum_{i=0}^t(g^i)^2}}g^t$，其中$t$是迭代次数，$w$是参数，$g$是梯度，$\eta$是初始学习率。</p><p>随着参数迭代，$t$越来越大，$\sqrt{\sum_{i=0}^t(g^i)^2}$也越来越大，因此学习率的变化趋势是越来越小。</p><h2 id="Adagrad的矛盾（Contradiction）"><a href="#Adagrad的矛盾（Contradiction）" class="headerlink" title="Adagrad的矛盾（Contradiction）"></a>Adagrad的矛盾（Contradiction）</h2><p>一般的梯度下降方法$w^{t+1}=w^t-\eta^tg^t$中，$\eta^t$是常量，梯度越大时，则参数更新的步幅越大，这是由$g^t$项决定的。</p><p>在Adagrad中，$\eta$是常量，梯度$g^t$越大时会使得参数更新的步幅越大，但$\sqrt{\sum_{i=0}^t(g^i)^2}$越大会使得参数更新的步幅越小，这是一个矛盾吗？</p><h2 id="为什么要除以之前梯度的均方根？"><a href="#为什么要除以之前梯度的均方根？" class="headerlink" title="为什么要除以之前梯度的均方根？"></a>为什么要除以之前梯度的均方根？</h2><ul><li><p>一种直观的解释：增强参数更新步幅变化的惯性</p><p>  与之前梯度相比如果现在的梯度更大，则现在梯度除以之前梯度会使参数更新的步幅更大；如果现在的梯度更小，则会使步幅更新的步幅更小。</p><p>  这样就相当于增强了参数更新步幅变化的惯性，即如果参数更新的步幅突然变大或变小，就扩大这个趋势。</p></li><li><p>同时考虑一次梯度和二次梯度</p><p>  在Adagrad中，之前梯度的均方根是用来通过一次梯度估计二次梯度（虽然可以直接使用二次梯度，但其很难计算）。</p><ul><li><p>只考虑一个参数</p><p>  当参数只有一个或只考虑一个参数时，梯度越大，离最优点就越远，参数更新的步幅应该越大。</p></li><li><p>考虑多个参数</p><p>  当参数有多个或者考虑多个参数时，上述内容不一定成立。如果参数1的梯度比参数2的梯度大，但如果损失函数关于参数1的曲线比关于参数2的曲线更陡峭（即二次梯度更大），那参数1离最优点的距离可能比参数2更近。</p><p>  所以当参数有多个或者考虑多个参数时，我们既要考虑一次梯度又要考虑二次梯度。</p><p>  结论是一次梯度越大、二次梯度越小，离最优点就越远，参数更新的步幅应该越大。</p></li></ul></li></ul><h1 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h1><p>Stochastic Gradient Descent，随机梯度下降，1847年提出，可以让训练过程更快。</p><p>普通梯度下降中需要计算所有样本的Loss，而SGD只计算一个样本的Loss，然后进行梯度下降。</p><h1 id="梯度下降的数学理论"><a href="#梯度下降的数学理论" class="headerlink" title="梯度下降的数学理论"></a>梯度下降的数学理论</h1><p>建议直接看<a href="https://www.bilibili.com/video/BV1JE411g7XF?p=5" target="_blank" rel="noopener">李宏毅老师的本节视频</a>，从42分27秒开始看，老师讲得很好。</p><ol><li>初始化一组参数后，我们找到邻域中另一个使损失函数值最小的一组参数并更新参数（然后不断重复这一步骤）。</li><li><strong>在极小的邻域中</strong>，可以利用泰勒级数将损失函数简化，然后求其最小值，损失函数简化后，要使其最小即是让其中两个向量的內积最小，由此可以得出新的一组参数的值（具体过程略），这就是梯度下降。</li><li>学习率的作用是限制邻域大小，学习率太大可能使邻域太大，导致损失函数展开成泰勒级数时的误差较大。</li><li>当然也可以将损失函数展开成2次（比如牛顿迭代法），但这并不实用，因为要计算二次微分，甚至可能要求出海森矩阵（Hessian Matrix）逆矩阵等等，这些在做深度学习时是不实用的。</li></ol><h1 id="梯度下降的局限性"><a href="#梯度下降的局限性" class="headerlink" title="梯度下降的局限性"></a>梯度下降的局限性</h1><p>梯度下降过程中，每次参数更新不一定都会使损失函数的值更小。</p><p>求出的只是局部最小值（Local Minima）甚至是鞍点（Saddle Point），不一定是全局最优解。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;梯度下降伪代码&quot;&gt;&lt;a href=&quot;#梯度下降伪代码&quot; class=&quot;headerlink&quot; title=&quot;梯度下降伪代码&quot;&gt;&lt;/a&gt;梯度下降伪代码&lt;/h1&gt;&lt;p&gt;梯度下降可以优化损失函数的值，使其尽量小，即可找到最好（在数据集上拟合效果最好）的模型参数。&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="梯度下降" scheme="https://chouxianyu.github.io/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
      <category term="学习率" scheme="https://chouxianyu.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
      <category term="Adagrad" scheme="https://chouxianyu.github.io/tags/Adagrad/"/>
    
      <category term="SGD" scheme="https://chouxianyu.github.io/tags/SGD/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-2.5线性回归Python实战</title>
    <link href="https://chouxianyu.github.io/2020/12/25/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-2-5%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Python%E5%AE%9E%E6%88%98/"/>
    <id>https://chouxianyu.github.io/2020/12/25/李宏毅机器学习课程笔记-2-5线性回归Python实战/</id>
    <published>2020-12-25T01:03:09.000Z</published>
    <updated>2020-12-25T01:07:05.128Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework1的记录。</p><ul><li><p>任务描述（Task Description）</p><p>  现在有某地空气质量的观测数据，请使用线性回归拟合数据，预测PM2.5。</p></li><li><p>数据集描述（Dataset Description）</p><ul><li><p>train.csv</p><p>  该文件中是2014年每月前20天每小时的观察数据，每小时的数据是18个维度的（其中之一是PM2.5）。</p></li><li><p>test.csv</p><p>  该文件中包含240组数据，每组数据是连续9个小时的所有观测数据（同样是18个维度）。</p><p>  请预测每组数据对应的第10个小时的PM2.5数值。</p></li></ul></li><li><p>结果格式</p><p>  要求上交结果的格式为CSV文件。</p><p>  第一行必须是<code>id,value</code>。</p><p>  从第二行开始每行分别为id值及预测的PM2.5数值，两者用逗号间隔</p></li><li><p>总结</p><ul><li>数据处理<ul><li>将数据处理、转换成什么形式，要根据数据集格式、任务来确定。</li><li>要熟练掌握pandas、numpy等数据处理工具，特别是要知道它们能实现什么功能。</li></ul></li></ul></li><li><p>参考链接</p><p>  <a href="https://colab.research.google.com/drive/131sSqmrmWXfjFZ3jWSELl8cm0Ox5ah3C" target="_blank" rel="noopener">https://colab.research.google.com/drive/131sSqmrmWXfjFZ3jWSELl8cm0Ox5ah3C</a></p></li><li><p>Python代码</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment">## 读取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">'./train.csv'</span>, encoding = <span class="string">'big5'</span>) <span class="comment"># 读取训练集</span></span><br><span class="line"><span class="comment"># print(data.describe())</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 数据预处理</span></span><br><span class="line">data = data.iloc[:, <span class="number">3</span>:] <span class="comment"># 不需要使用前三列的表头，所以删除</span></span><br><span class="line">data[data == <span class="string">'NR'</span>] = <span class="number">0</span> <span class="comment"># 将非数值NR改为0</span></span><br><span class="line">raw_data = data.to_numpy() <span class="comment"># pandas转numpy数组，形状是4320(=18*20*12)*24</span></span><br><span class="line"><span class="comment"># print(raw_data.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改数据格式</span></span><br><span class="line"><span class="comment"># 数据格式为12(month)*18(features)*480(=24*20hours)，即12个月、每个月有480小时的数据（18维）</span></span><br><span class="line">month_data = &#123;&#125; <span class="comment"># 字典</span></span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    sample = np.empty([<span class="number">18</span>, <span class="number">480</span>])</span><br><span class="line">    <span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        sample[ : , <span class="number">24</span> * day : <span class="number">24</span> * (day + <span class="number">1</span>)] = raw_data[(month * <span class="number">20</span> + day) * <span class="number">18</span> : (month * <span class="number">20</span> + day + <span class="number">1</span>) * <span class="number">18</span>, : ]</span><br><span class="line">    month_data[month] = sample</span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改数据格式</span></span><br><span class="line"><span class="comment"># 数据格式为每个月有连续的480个小时，每10个小时形成1个object，每个月就有471个object，12个月就有471*12个oeject，每个object包括x(18*9的featrues)和y(1个PM2.5数值)。</span></span><br><span class="line">x = np.empty([<span class="number">471</span>*<span class="number">12</span>, <span class="number">18</span>*<span class="number">9</span>], dtype=float) <span class="comment"># 471*12行，一行是一个object的x</span></span><br><span class="line">y = np.empty([<span class="number">471</span>*<span class="number">12</span>, <span class="number">1</span>], dtype=float) <span class="comment"># 471*12行，一行是一个object的y</span></span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    <span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        <span class="keyword">for</span> hour <span class="keyword">in</span> range(<span class="number">24</span>):</span><br><span class="line">            <span class="keyword">if</span> day == <span class="number">19</span> <span class="keyword">and</span> hour &gt; <span class="number">14</span>: <span class="comment"># 最后一个10小时从第20天14小时开始，防止越界</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            x[month * <span class="number">471</span> + day * <span class="number">24</span> + hour, :] = month_data[month][:,day * <span class="number">24</span> + hour : day * <span class="number">24</span> + hour + <span class="number">9</span>].reshape(<span class="number">1</span>, <span class="number">-1</span>) <span class="comment"># reshape时的(1, -1)指：1行、列数自动计算</span></span><br><span class="line">            y[month * <span class="number">471</span> + day * <span class="number">24</span> + hour, <span class="number">0</span>] = month_data[month][<span class="number">9</span>, day * <span class="number">24</span> + hour + <span class="number">9</span>] <span class="comment"># 取对应的第10个小时的PM2.5的值</span></span><br><span class="line"><span class="comment"># print(x, y)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 标准化</span></span><br><span class="line"><span class="comment">#关于标准化，可以看这篇文章https://www.cnblogs.com/chouxianyu/p/13872444.html</span></span><br><span class="line">mean_x = np.mean(x, axis=<span class="number">0</span>) <span class="comment"># 平均值，axis=0指沿着列计算平均值，即计算每列的平均值</span></span><br><span class="line">std_x = np.std(x, axis=<span class="number">0</span>) <span class="comment"># 标准差，axis=0指沿着列计算平均值，即计算每列的标准差</span></span><br><span class="line"><span class="comment"># print(mean_x.shape, std_x.shape)</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x[<span class="number">0</span>])):</span><br><span class="line">        <span class="keyword">if</span> std_x[j] != <span class="number">0</span>:</span><br><span class="line">            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]</span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line">dim = <span class="number">18</span> * <span class="number">9</span> + <span class="number">1</span> <span class="comment"># 这个+1是为了保存偏置</span></span><br><span class="line">w = np.zeros([dim, <span class="number">1</span>])</span><br><span class="line">x = np.concatenate((np.ones([<span class="number">471</span> * <span class="number">12</span>, <span class="number">1</span>]), x), axis=<span class="number">1</span>).astype(float) <span class="comment"># axis=1表示将两个数组按行拼接，向x中添加1是为了让其与weight中的偏置相乘</span></span><br><span class="line">learning_rate = <span class="number">100</span> <span class="comment"># 学习率</span></span><br><span class="line">iter_time = <span class="number">1000</span> <span class="comment"># 迭代次数</span></span><br><span class="line">adagrad = np.zeros([dim, <span class="number">1</span>])</span><br><span class="line">eps = <span class="number">1e-10</span>  <span class="comment"># eps是避免Adagrad分母为0而加的</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(iter_time):</span><br><span class="line">    loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, <span class="number">2</span>)) / <span class="number">471</span> / <span class="number">12</span>) <span class="comment"># RMSE</span></span><br><span class="line">    <span class="keyword">if</span> (t % <span class="number">100</span> == <span class="number">0</span>):</span><br><span class="line">        print(t, loss)</span><br><span class="line">    gradient = <span class="number">2</span> * np.dot(x.transpose(), np.dot(x, w) - y) <span class="comment"># dim*1</span></span><br><span class="line">    adagrad += gradient ** <span class="number">2</span></span><br><span class="line">    w -= learning_rate * gradient / np.sqrt(adagrad + eps)</span><br><span class="line">np.save(<span class="string">'weight.npy'</span>, w)</span><br><span class="line">print(<span class="string">'Training Done'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 测试</span></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">test_data = pd.read_csv(<span class="string">'./test.csv'</span>,header=<span class="keyword">None</span>, encoding=<span class="string">'big5'</span>)</span><br><span class="line">test_data = test_data.iloc[ : , <span class="number">2</span>:] <span class="comment"># 去除表头（前两列）</span></span><br><span class="line">test_data[test_data == <span class="string">'NR'</span>] = <span class="number">0</span></span><br><span class="line">test_data = test_data.to_numpy()</span><br><span class="line">test_x = np.empty([<span class="number">240</span>, <span class="number">18</span> * <span class="number">9</span>]) <span class="comment"># 240个object，一行是一个object的x</span></span><br><span class="line"><span class="comment"># 修改数据格式</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">240</span>):</span><br><span class="line">    test_x[i, :] = test_data[i * <span class="number">18</span> : (i + <span class="number">1</span>) * <span class="number">18</span>, : ].reshape(<span class="number">1</span>, <span class="number">-1</span>) <span class="comment"># 格式和训练集一样</span></span><br><span class="line"><span class="comment"># 标准化</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(test_x)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(test_x[<span class="number">0</span>])):</span><br><span class="line">        <span class="keyword">if</span> std_x[j] != <span class="number">0</span>:</span><br><span class="line">            test_x[i, j] = (test_x[i, j] - mean_x[j]) / std_x[j]</span><br><span class="line">test_x = np.concatenate((np.ones([<span class="number">240</span>, <span class="number">1</span>]), test_x), axis=<span class="number">1</span>).astype(float) <span class="comment"># axis=1表示将两个数组按行拼接，向x中添加1是为了让其与weight中的偏置相乘</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 预测</span></span><br><span class="line">w = np.load(<span class="string">'weight.npy'</span>)</span><br><span class="line">ans_y = np.dot(test_x, w)</span><br><span class="line"><span class="comment"># print('ans_y.shape', ans_y.shape)</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'answer.csv'</span>, mode=<span class="string">'w'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> answer_file:</span><br><span class="line">    csv_writer = csv.writer(answer_file)</span><br><span class="line">    csv_writer.writerow([<span class="string">'id'</span>, <span class="string">'value'</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">240</span>):</span><br><span class="line">        row = [<span class="string">'id_'</span> + str(i), ans_y[i][<span class="number">0</span>]]</span><br><span class="line">        csv_writer.writerow(row)</span><br><span class="line">        <span class="comment"># print(row)</span></span><br></pre></td></tr></table></figure></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework1的记录。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;任务描述（Task Description）&lt;/p&gt;
&lt;p&gt;  现在有某地空气质量的观测数据，请使用线性回归拟合数据，预测PM2.5。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="https://chouxianyu.github.io/tags/python/"/>
    
      <category term="线性回归" scheme="https://chouxianyu.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="adgrad" scheme="https://chouxianyu.github.io/tags/adgrad/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-2.4交叉验证</title>
    <link href="https://chouxianyu.github.io/2020/12/24/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-2-4%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/"/>
    <id>https://chouxianyu.github.io/2020/12/24/李宏毅机器学习课程笔记-2-4交叉验证/</id>
    <published>2020-12-24T11:10:34.000Z</published>
    <updated>2020-12-24T11:34:54.806Z</updated>
    
    <content type="html"><![CDATA[<h1 id="交叉验证（Cross-Validation）"><a href="#交叉验证（Cross-Validation）" class="headerlink" title="交叉验证（Cross Validation）"></a>交叉验证（Cross Validation）</h1><p>在机器学习中，通常不能将全部数据用于模型训练，否则将没有数据集可以用来评估模型。</p><h2 id="The-Validation-Set-Approach"><a href="#The-Validation-Set-Approach" class="headerlink" title="The Validation Set Approach"></a>The Validation Set Approach</h2><ul><li><p>定义</p><p>  将数据集划分成训练集（Training Set）和测试集（Test Set）两部分。</p></li><li><p>缺点</p><p>  这种方法的缺点是依赖于训练集和测试集的划分方法，并且只用了部分数据进行模型的训练。</p></li></ul><h2 id="LOOCV（Leave-One-Out-Cross-Validation）"><a href="#LOOCV（Leave-One-Out-Cross-Validation）" class="headerlink" title="LOOCV（Leave One Out Cross Validation）"></a>LOOCV（Leave One Out Cross Validation）</h2><ul><li><p>定义</p><p>  假设数据集中有N个数据，取其中1个数据作为测试集，将剩下的N-1个数据作为训练集，这样重复N次就得到N个模型以及N个误差值，最终使用这N个误差值的平均值评估该模型。</p></li><li><p>优点</p><p>  该方法不受训练集和测试集划分方法的影响，因为每个数据都单独做过测试集；同时该方法用了N-1个数据训练模型，也几乎用到了所有的数据，保证了模型的Bias更小。</p></li><li><p>缺点</p><p>  该方法的缺点是计算量过大，是The Validation Set Approach耗时的N-1倍。</p></li></ul><h2 id="K折交叉验证（K-fold-Cross-Validation）"><a href="#K折交叉验证（K-fold-Cross-Validation）" class="headerlink" title="K折交叉验证（K-fold Cross Validation）"></a>K折交叉验证（K-fold Cross Validation）</h2><ul><li><p>定义</p><p>  该方法是LOOCV的折中，即将数据集分成K份。</p></li><li><p>如何选取K的值</p><p>  K的选取是一个Bias和Variance的trade-off。一般选择K=5或10。</p><p>  K越大，每次训练时训练集的数据量就越大，则Bias越小；但每次训练时的训练集之间的相关性越大（考虑最极端的情况K=N，也就是LOOCV，每次训练使用的数据几乎是一样的），这种大相关性会导致最终的误差具有更大的Variance。</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;交叉验证（Cross-Validation）&quot;&gt;&lt;a href=&quot;#交叉验证（Cross-Validation）&quot; class=&quot;headerlink&quot; title=&quot;交叉验证（Cross Validation）&quot;&gt;&lt;/a&gt;交叉验证（Cross Validatio
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="交叉验证" scheme="https://chouxianyu.github.io/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-2.3欠拟合与过拟合</title>
    <link href="https://chouxianyu.github.io/2020/12/24/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-2-3%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>https://chouxianyu.github.io/2020/12/24/李宏毅机器学习课程笔记-2-3欠拟合与过拟合/</id>
    <published>2020-12-24T11:10:08.000Z</published>
    <updated>2020-12-24T11:16:33.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="欠拟合（Underfitting）"><a href="#欠拟合（Underfitting）" class="headerlink" title="欠拟合（Underfitting）"></a>欠拟合（Underfitting）</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Bias较大、Variance较小。</p><p>如果模型在训练集上的误差很大，则此时Bias是大的，情况为欠拟合。</p><h2 id="Bias大时如何处理"><a href="#Bias大时如何处理" class="headerlink" title="Bias大时如何处理"></a>Bias大时如何处理</h2><p>使用更复杂的模型，比如添加考虑更多维度的输入、把线性模型换成非线性模型。</p><h1 id="过拟合（Overfitting）"><a href="#过拟合（Overfitting）" class="headerlink" title="过拟合（Overfitting）"></a>过拟合（Overfitting）</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>Bias较小、Variance较大。</p><p>如果模型在训练集上的误差很小，但是在测试集上的误差很大，则此时Variance是大的，情况为过拟合。</p><h2 id="Variance大时如何处理"><a href="#Variance大时如何处理" class="headerlink" title="Variance大时如何处理"></a>Variance大时如何处理</h2><ul><li><p>使用更复杂的数据集</p><p>  比如添加数据（很有效，但不一定做得到）、数据增强等方法。</p></li><li><p>使用更简单的模型（不是根本方法）</p><p>  可能是模型过于复杂导致了过拟合，因此可以简化模型缓解过拟合。</p></li><li><p>正则化（Regularization）</p><p>  正则化可能会使Bias增大，所以需要调整正则化的参数。</p><p>  如$L_{new}=L_{old}+\lambda \sum(w_i)^2$，其中$\lambda$是一个常数。</p><p>  加上正则项$\lambda \sum(w_i)^2$的目的是让函数参数的值尽可能地接近0，使函数变得更平滑。</p></li></ul><h1 id="平滑（Smooth）"><a href="#平滑（Smooth）" class="headerlink" title="平滑（Smooth）"></a>平滑（Smooth）</h1><h2 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h2><p>平滑是指输入变化影响输出变化的程度（输出对输入的敏感程度）。</p><p>假设输入变化，如果函数越不平滑，则输出变化程度越大。</p><p>函数参数越接近0，这个函数就越平滑（smooth）。 </p><h2 id="我们为什么喜欢一个平滑的函数？"><a href="#我们为什么喜欢一个平滑的函数？" class="headerlink" title="我们为什么喜欢一个平滑的函数？"></a>我们为什么喜欢一个平滑的函数？</h2><p>适度平滑的函数可以缓解函数输入中包含的噪声对函数输出的影响。</p><p>如果输入中包含一些噪声/干扰（noise），那平滑函数的输出受输入中包含的噪声干扰的程度更小。</p><h2 id="我们为什么不喜欢过于平滑的函数？"><a href="#我们为什么不喜欢过于平滑的函数？" class="headerlink" title="我们为什么不喜欢过于平滑的函数？"></a>我们为什么不喜欢过于平滑的函数？</h2><p>函数过于平滑，就无法有效地提取数据的特征，这不是我们想要的函数。</p><p>假设有一个极限平滑的函数，即该函数的输出不受输入的影响，那当然不是个好的函数。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;欠拟合（Underfitting）&quot;&gt;&lt;a href=&quot;#欠拟合（Underfitting）&quot; class=&quot;headerlink&quot; title=&quot;欠拟合（Underfitting）&quot;&gt;&lt;/a&gt;欠拟合（Underfitting）&lt;/h1&gt;&lt;h2 id=&quot;定义&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="欠拟合" scheme="https://chouxianyu.github.io/tags/%E6%AC%A0%E6%8B%9F%E5%90%88/"/>
    
      <category term="过拟合" scheme="https://chouxianyu.github.io/tags/%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    
      <category term="Bias" scheme="https://chouxianyu.github.io/tags/Bias/"/>
    
      <category term="Variance" scheme="https://chouxianyu.github.io/tags/Variance/"/>
    
      <category term="正则化" scheme="https://chouxianyu.github.io/tags/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-2.2如何选择模型、减小误差</title>
    <link href="https://chouxianyu.github.io/2020/12/24/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-2-2%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B%E3%80%81%E5%87%8F%E5%B0%8F%E8%AF%AF%E5%B7%AE/"/>
    <id>https://chouxianyu.github.io/2020/12/24/李宏毅机器学习课程笔记-2-2如何选择模型、减小误差/</id>
    <published>2020-12-24T11:09:41.000Z</published>
    <updated>2020-12-24T11:40:32.890Z</updated>
    
    <content type="html"><![CDATA[<h1 id="模型选择（How-to-select-model）"><a href="#模型选择（How-to-select-model）" class="headerlink" title="模型选择（How to select model）"></a>模型选择（How to select model）</h1><ul><li><p>模型越复杂，一般其在训练集上的误差（Error）越小。</p><p>  因为更复杂的模型（函数集）包含了更多的函数。比如二次模型包含了线性（一次）模型。</p></li><li><p>模型越复杂，其在测试集上的误差（Error）不一定越小。</p><p>  因为模型过于复杂时，越容易被数据影响，可能导致过拟合。</p></li></ul><h1 id="误差（Error）"><a href="#误差（Error）" class="headerlink" title="误差（Error）"></a>误差（Error）</h1><h2 id="误差的来源"><a href="#误差的来源" class="headerlink" title="误差的来源"></a>误差的来源</h2><p>暂时称通过机器学习得到的函数为人工函数，它其实是对“上帝函数”的估计（Estimator），和“上帝函数”之间是有误差的。</p><p>误差来源于两方面：一是Bias，二是Variance，需要权衡（trade-off）两者以使总误差最小。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071519VarianceAndBias.png" alt="VarianceAndBias" style="zoom: 50%;"></p><p>如上图所示，Bias是指人工函数（的期望）和上帝函数之间的距离，Variance是指人工函数的离散程度（或者说是不稳定程度）。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071542BiasVSVariance.png" alt="BiasVSVariance" style="zoom:50%;"></p><p>如上图所示，横轴是模型的复杂程度（1次幂、2次幂、……），纵轴是误差大小。模型越复杂，Bias越小，Variance越大。</p><h2 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>使用相同模型在不同数据上拟合得到的函数是不同的，这些函数之间的离散程度就是Variance。</p><p>以射箭为例，Variance衡量的就是射得稳不稳。</p><h3 id="模型越复杂，Variance越大。"><a href="#模型越复杂，Variance越大。" class="headerlink" title="模型越复杂，Variance越大。"></a>模型越复杂，Variance越大。</h3><p>因为模型越简单，越不容易被数据影响（对数据不敏感，感知数据变化的能力较差），那Variance就越小。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071610Variance.png" alt="Variance" style="zoom:50%;"></p><h2 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>使用相同模型在不同数据上拟合得到的函数是不同的，取这些函数的“期望”，该期望与“真理”的差距就是Bias。</p><p>以射箭为例，Bias衡量的就是射得准不准（这里的“准”的含义有待商榷）。</p><h3 id="模型越简单，Bias越大。"><a href="#模型越简单，Bias越大。" class="headerlink" title="模型越简单，Bias越大。"></a>模型越简单，Bias越大。</h3><p>因为模型就是个函数集（Function Set）。模型越简单，则其包含的函数就越少、包含“上帝函数”的几率就越小，甚至可能不包括上帝函数。</p><p>在函数集很小的情况下，即使是其中最好的函数，它与“上帝函数”的差距也还是很大的。</p><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201101071635Bias.png" alt="Bias" style="zoom:50%;"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;模型选择（How-to-select-model）&quot;&gt;&lt;a href=&quot;#模型选择（How-to-select-model）&quot; class=&quot;headerlink&quot; title=&quot;模型选择（How to select model）&quot;&gt;&lt;/a&gt;模型选择（How to
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Bias" scheme="https://chouxianyu.github.io/tags/Bias/"/>
    
      <category term="Variance" scheme="https://chouxianyu.github.io/tags/Variance/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-2.1线性回归模型</title>
    <link href="https://chouxianyu.github.io/2020/12/24/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-2-1%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>https://chouxianyu.github.io/2020/12/24/李宏毅机器学习课程笔记-2-1线性回归模型/</id>
    <published>2020-12-24T11:09:06.000Z</published>
    <updated>2020-12-24T11:54:21.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="回归模型应用案例（Regression-Cases）"><a href="#回归模型应用案例（Regression-Cases）" class="headerlink" title="回归模型应用案例（Regression Cases）"></a>回归模型应用案例（Regression Cases）</h1><ul><li><p>股票市场预测（Stock Market Forecast）</p><p>  预测某个公司明天的股票情况</p></li><li><p>自动驾驶车（Self-Driving Car）</p><p>  预测方向盘转动角度</p></li><li><p>推荐系统（Recommendation）</p><p>  预测某用户购买某商品的可能性</p></li></ul><h1 id="线性回归模型（Linear-Regression-Model）"><a href="#线性回归模型（Linear-Regression-Model）" class="headerlink" title="线性回归模型（Linear Regression Model）"></a>线性回归模型（Linear Regression Model）</h1><p>如$y=f(x)=w\cdot x+b$</p><ul><li><p>$y$是输出；</p><p>  $\hat y$是真实值/标签（label）</p></li><li><p>$w$是权重（weight）；</p></li><li><p>$b$是偏置（bias）；</p></li><li><p>$x$是输入（input），也可叫做特征（feature）</p><p>  数据集中一般包含多个object，每个object一般包含多个component。此时，上标是object的索引，下标是component的索引。</p></li><li><p>损失函数（Loss Function）</p><p>  如果不考虑模型的好坏，衡量一个函数的好坏，其实是衡量模型参数的好坏。</p><p>  以线性模型为例，就是衡量参数$w$和$b$的好坏。如$L(f)=L(w,b)=\sum_{n=1}^{10}(\hat y-(b+w\cdot x^n))^2$，把所有样本误差的平方和作为损失函数</p><ul><li><p>输入</p><p>  一个函数</p></li><li><p>输出</p><p>  多么地不好（how bad it is）。损失函数值越大，则这个函数越差、与数据集中内容越不相符。</p></li></ul></li></ul><h1 id="梯度下降（Gradient-Descent）"><a href="#梯度下降（Gradient-Descent）" class="headerlink" title="梯度下降（Gradient Descent）"></a>梯度下降（Gradient Descent）</h1><p>梯度下降可以优化损失函数的值，使其尽量小，即可找到最好（在数据集上拟合效果最好）的模型参数。</p><p>现在假设模型$f$中只有一个参数$w$，则损失函数为$L(f)=L(w)$，梯度下降算法如下（若模型有多个参数，按相同方法更新各参数）</p><ol><li><p>初始化参数</p><p> 随机选取一个$w^0$（$w^0$并不一定是随机选取），令$w=w^0$。</p></li><li><p>计算梯度</p><p> $\frac{dL(f)}{dw}|_{w=w^0}$</p><p> 如果小于0，此时$w$增大则$L(f)$会减小；如果大于0，此时$w$减小则$L(w)$会减小。</p><p> 如果模型有多个参数，则计算损失函数在各个参数方向上的偏导数。</p></li><li><p>更新模型参数</p><p> $w^1=w^0-lr\frac{dL(f)}{dw}|_{w=w^0}$</p><p> $w$的变化量取决于梯度和学习率（Learning Rate）的大小：梯度绝对值或学习率越大，则$w$变化量越大。</p><p> 如果模型有多个参数，则用上一步计算出的偏导数对应更新各参数。</p></li><li><p>重复第2步和第3步</p><p> 经过多次参数更新/迭代（iteration），可以使损失函数的值达到局部最小（即局部最优，Local Optimal），但不一定是全局最优。</p></li></ol><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;回归模型应用案例（Regression-Cases）&quot;&gt;&lt;a href=&quot;#回归模型应用案例（Regression-Cases）&quot; class=&quot;headerlink&quot; title=&quot;回归模型应用案例（Regression Cases）&quot;&gt;&lt;/a&gt;回归模型应用案例
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归，梯度下降" scheme="https://chouxianyu.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-1.机器学习概论</title>
    <link href="https://chouxianyu.github.io/2020/12/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/"/>
    <id>https://chouxianyu.github.io/2020/12/22/李宏毅机器学习课程笔记-1-机器学习概论/</id>
    <published>2020-12-22T02:06:27.000Z</published>
    <updated>2020-12-22T03:13:10.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习是什么"><a href="#机器学习是什么" class="headerlink" title="机器学习是什么"></a>机器学习是什么</h1><p><strong>机器学习就是让机器能自动找到一个函数（function）</strong></p><ul><li><p>语音识别（Speech Recognition）</p><p>  输入是音频，输出是音频对应的文字。</p></li><li><p>图像分类</p><p>  输入是图片，输出是类别（比如猫、狗）。</p></li><li><p>AlphaGo下围棋</p><p>  输入是当前棋盘的状态，输出是下一步落棋的位置。</p></li><li><p>对话/问答系统</p></li></ul><h1 id="机器能够找到哪些函数？"><a href="#机器能够找到哪些函数？" class="headerlink" title="机器能够找到哪些函数？"></a>机器能够找到哪些函数？</h1><p>为解决不同的问题、完成不同的任务，需要找到不同的函数，那机器学习能找到哪些函数呢？</p><ul><li><p>回归（Regression）</p><p>  输出是一个连续的数值、标量，比如PM2.5预测。</p></li><li><p>分类（Classification）</p><p>  输出是一个离散的值。</p><p>  二分类（Binary Classification）的输出就是0或1、Yes或No、…，比如文本情感分析的输出可以是正面和负面。</p><p>  多分类（Multi-Category Classification）的输出就是[1,2,3,…,N]，比如图像分类里判断一张图片是猫还是狗还是杯子。</p></li><li><p>生成（Generation）</p><p>  很多教科书把机器学习划分为回归问题和分类问题，但其实不止这两种问题，还有其它问题，比如生成（Generation）。</p><p>  生成（Generation）指让机器学习如何创造/生成，比如生成文本、图片等。</p></li></ul><h1 id="如何告诉机器我们希望找到什么函数"><a href="#如何告诉机器我们希望找到什么函数" class="headerlink" title="如何告诉机器我们希望找到什么函数"></a>如何告诉机器我们希望找到什么函数</h1><p>我们该如何为机器提供学习资料？</p><ul><li><p>有监督学习（Supervised Learning）</p><p>  可以把有监督学习中的“监督”理解为“标签（Label）”，即数据集中不仅包括特征还包括标签。</p><p>  有了标签，我们就可以评价一个函数的好坏，进而优化这个函数。</p><p>  使用Loss判断函数的好坏，Loss越小，函数越好。<strong>个人想法：值得一提的是，Loss/评价指标是多样的、优化方法也是多样的。</strong></p></li><li><p>强化学习（Reinforcement Learning）</p><p>  原始的AlpahGo是先通过有监督学习优化到一定程度，然后用强化学习继续优化。</p><p>  新版本的AlphaGo是完全通过强化学习实现的，优于原始的AlphaGo。 </p></li><li><p>无监督学习（Unsupervised Learning）</p><p>  只给机器提供数据特征，但不提供数据标签。==那机器能学到什么呢？==</p></li></ul><p>下面以让机器学习下围棋为例：有监督学习VS强化学习。</p><ul><li><p>有监督学习</p><p>  函数的输入（数据特征）就是棋盘状态，函数的输出（数据标签）就是下一步落棋的位置。</p><p>  此时，我们需要为机器提供的数据就类似棋谱（如果现在棋局是这样，那下一步怎么落棋最好），<strong>但其实人类不一定知道怎么落棋最好</strong>。</p><p>  <strong>个人想法：理论上，通过这样的有监督学习，机器是无法超越人类的。因为这样的有监督学习的本质是人类把自己的下棋策略教给机器，机器学习的内容仅仅是人类的下棋策略而无法“自主进行思考”，所以理论上机器是无法超越人类的。同时要注意，这里的人类指全人类。</strong></p></li><li><p>强化学习</p><p>  让机器跟自己、别人下棋，把结果（赢或输）作为Reward，引导机器学习如何下棋。</p><p>  如果它赢了，那它就知道这一盘里有几步棋下得好，但不知道是哪几步；如果它输了，它就知道这一盘里有几步棋下得不好，但不知道是哪几步。</p><p>  <strong>个人想法：理论上，通过这样的强化学习，机器是可以超过人类的。因为两者的学习材料没有本质区别，但机器的机能却优于人类，这里讲的机能包括信息共享能力、记忆能力、执行能力等方面</strong></p></li></ul><h1 id="机器如何找出我们想找到的函数"><a href="#机器如何找出我们想找到的函数" class="headerlink" title="机器如何找出我们想找到的函数"></a>机器如何找出我们想找到的函数</h1><ul><li><p>我们要给定函数形式/范围（模型）</p><p>  比如假定函数是线性模型、神经网络等等。<strong>模型就是一个函数集，模型的参数确定以后，才得到一个函数。</strong></p></li><li><p>找到更好的函数：</p><p>  使用梯度下降（Gradient Descent），找到更好的函数。</p></li></ul><h1 id="前沿研究"><a href="#前沿研究" class="headerlink" title="前沿研究"></a>前沿研究</h1><ul><li><p>AI的可解释性（Explainable AI）</p><p>  比如，机器为什么认为这张图片里有一只猫？</p></li><li><p>对抗攻击（Adversarial Attack）</p><p>  对输入故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出。</p></li><li><p>模型压缩（Network Compression）</p><p>  把模型压缩以减少模型对计算资源消耗。</p></li><li><p>异常检测（Anomaly Detection）</p><p>  使机器知道它遇到了自己不知道的东西。</p></li><li><p>迁移学习（Transfer Learning/Domain Adversarial Learning）</p><p>  一个模型已经学到了一些知识，将这些知识应用到另一个任务中。</p></li><li><p>元学习（Meta Learning）</p><p>  让机器学习如何学习。</p><p>  机器学习是我们教机器学习某种知识，元学习是我们教机器如何学习。</p></li><li><p>终身学习（Life-Long Learning）</p><p>  让机器终身学习，学习完任务1、再继续学任务2、……</p></li></ul><h1 id="机器学习的三个步骤"><a href="#机器学习的三个步骤" class="headerlink" title="机器学习的三个步骤"></a>机器学习的三个步骤</h1><ol><li>确定模型（Model）/函数集（Function Set）</li><li>确定如何评价函数的好坏</li><li>确定如何找到最好的函数</li></ol><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="">@臭咸鱼的快乐生活</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习是什么&quot;&gt;&lt;a href=&quot;#机器学习是什么&quot; class=&quot;headerlink&quot; title=&quot;机器学习是什么&quot;&gt;&lt;/a&gt;机器学习是什么&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;机器学习就是让机器能自动找到一个函数（function）&lt;/strong&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浙江理工大学计算机科学与技术专业课程笔记、试题、课件等复习资料大礼包</title>
    <link href="https://chouxianyu.github.io/2020/11/26/%E6%B5%99%E6%B1%9F%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E3%80%81%E8%AF%95%E9%A2%98%E3%80%81%E8%AF%BE%E4%BB%B6%E7%AD%89%E5%A4%8D%E4%B9%A0%E8%B5%84%E6%96%99%E5%A4%A7%E7%A4%BC%E5%8C%85/"/>
    <id>https://chouxianyu.github.io/2020/11/26/浙江理工大学计算机科学与技术专业课程笔记、试题、课件等复习资料大礼包/</id>
    <published>2020-11-26T06:53:20.000Z</published>
    <updated>2020-12-12T02:24:38.609Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>模拟电路与数字电路</p><p>链接：<a href="https://pan.baidu.com/s/1KTUmyqRAG0ilQrBEB72eFQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1KTUmyqRAG0ilQrBEB72eFQ</a><br>提取码：5n4y<br>复制这段内容后打开百度网盘手机App，操作更方便哦</p><p><strong>百度网盘不让分享，请尽快保存</strong></p></li><li><p>C++试题</p><p>  链接：<a href="https://pan.baidu.com/s/1QtMMzJxFOl6fsDHVzKIeyw" target="_blank" rel="noopener">https://pan.baidu.com/s/1QtMMzJxFOl6fsDHVzKIeyw</a><br>  提取码：os8k</p><p>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>C语言试题</p><p>  链接：<a href="https://pan.baidu.com/s/1OsE0BsLJ-kXGibNTg1GfgQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1OsE0BsLJ-kXGibNTg1GfgQ</a><br>  提取码：kaso<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>JAVA试题</p><p>  链接：<a href="https://pan.baidu.com/s/11EpvCdIeQokMbMDR2MdZCQ" target="_blank" rel="noopener">https://pan.baidu.com/s/11EpvCdIeQokMbMDR2MdZCQ</a><br>  提取码：knly<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>汇编语言（试题、笔记、实验、参考书）</p><p>  链接：<a href="https://pan.baidu.com/s/1OxCLyAbOB2Egp6jJLdN2kg" target="_blank" rel="noopener">https://pan.baidu.com/s/1OxCLyAbOB2Egp6jJLdN2kg</a><br>  提取码：44eg<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>概率论与数理统计</p><p>  链接：<a href="https://pan.baidu.com/s/1qgyFsn-2JHO0hwxK08uUQw" target="_blank" rel="noopener">https://pan.baidu.com/s/1qgyFsn-2JHO0hwxK08uUQw</a><br>  提取码：e0is<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>高等数学</p><p>  链接：<a href="https://pan.baidu.com/s/1n7sTD9tutaQTuca-eI1jKg" target="_blank" rel="noopener">https://pan.baidu.com/s/1n7sTD9tutaQTuca-eI1jKg</a><br>  提取码：95xp<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>离散数学</p><p>  链接：<a href="https://pan.baidu.com/s/1ltfwSqB5or90wRFQVOYncQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1ltfwSqB5or90wRFQVOYncQ</a><br>  提取码：yagu<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>线性代数</p><p>  链接：<a href="https://pan.baidu.com/s/19DgVokKrFj9aA2KMC7HVtw" target="_blank" rel="noopener">https://pan.baidu.com/s/19DgVokKrFj9aA2KMC7HVtw</a><br>  提取码：mac6<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>普通物理</p><p>  链接：<a href="https://pan.baidu.com/s/13OtioAw5SX1sA_fmO-n4Zw" target="_blank" rel="noopener">https://pan.baidu.com/s/13OtioAw5SX1sA_fmO-n4Zw</a><br>  提取码：gkwr<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>电子服务信任与信誉</p><p>  链接：<a href="https://pan.baidu.com/s/1WMWf8tz_QYySrZ7E5wq25w" target="_blank" rel="noopener">https://pan.baidu.com/s/1WMWf8tz_QYySrZ7E5wq25w</a><br>  提取码：zusz<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>编译原理</p><p>  链接：<a href="https://pan.baidu.com/s/1QNAigX7gtjNxVE5hdgysWg" target="_blank" rel="noopener">https://pan.baidu.com/s/1QNAigX7gtjNxVE5hdgysWg</a><br>  提取码：5n5g<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>操作系统</p><p>  链接：<a href="https://pan.baidu.com/s/1cHQ7yfFFmKLWSmhTp-FdrQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1cHQ7yfFFmKLWSmhTp-FdrQ</a><br>  提取码：qcjn<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>服务外包概论</p><p>  链接：<a href="https://pan.baidu.com/s/1ICG6Z6tiIa7ViL2JMdFYPQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1ICG6Z6tiIa7ViL2JMdFYPQ</a><br>  提取码：jaa2<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>软件工程</p><p>  链接：<a href="https://pan.baidu.com/s/1YoT2-vTHM_tX_UysTpwlHw" target="_blank" rel="noopener">https://pan.baidu.com/s/1YoT2-vTHM_tX_UysTpwlHw</a><br>  提取码：8csu<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>数据库系统与应用设计</p><p>  链接：<a href="https://pan.baidu.com/s/1s1Y92c00PPjtAl5pEjjlSQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1s1Y92c00PPjtAl5pEjjlSQ</a><br>  提取码：ql1g<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>计算机网络</p><p>  链接：<a href="https://pan.baidu.com/s/1YJ8vW7aHVjFYhkWRL0kCbQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1YJ8vW7aHVjFYhkWRL0kCbQ</a><br>  提取码：jh0j<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>计算机系统结构</p><p>  链接：<a href="https://pan.baidu.com/s/1-LZ4RFh5tOOxukwXIyO7OQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1-LZ4RFh5tOOxukwXIyO7OQ</a><br>  提取码：kxkq<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>计算机组成原理</p><p>  链接：<a href="https://pan.baidu.com/s/1Uk08FMucWBFotmjD0cIKgA" target="_blank" rel="noopener">https://pan.baidu.com/s/1Uk08FMucWBFotmjD0cIKgA</a><br>  提取码：ea1t<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li><li><p>数据结构与算法</p><p>  链接：<a href="https://pan.baidu.com/s/1rxLY0hKenxFBd2692LG1vQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1rxLY0hKenxFBd2692LG1vQ</a><br>  提取码：xyfz<br>  复制这段内容后打开百度网盘手机App，操作更方便哦</p></li></ul><hr><p>作者：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">https://www.cnblogs.com/chouxianyu/</a></p><p>欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;模拟电路与数字电路&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1KTUmyqRAG0ilQrBEB72eFQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://pan.baidu.c
      
    
    </summary>
    
    
      <category term="期末大礼包" scheme="https://chouxianyu.github.io/tags/%E6%9C%9F%E6%9C%AB%E5%A4%A7%E7%A4%BC%E5%8C%85/"/>
    
  </entry>
  
  <entry>
    <title>一文详解特征缩放、标准化、归一化的定义、区别、特点和作用</title>
    <link href="https://chouxianyu.github.io/2020/10/25/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E3%80%81%E6%A0%87%E5%87%86%E5%8C%96%E3%80%81%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%AE%9A%E4%B9%89%E3%80%81%E5%8C%BA%E5%88%AB%E3%80%81%E7%89%B9%E7%82%B9%E5%92%8C%E4%BD%9C%E7%94%A8/"/>
    <id>https://chouxianyu.github.io/2020/10/25/一文详解特征缩放、标准化、归一化的定义、区别、特点和作用/</id>
    <published>2020-10-25T02:30:54.000Z</published>
    <updated>2020-10-28T07:52:20.192Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我在学李宏毅的机器学习课程，助教给的回归作业代码中有数据标准化的操作。</p><p>我听过数据标准化，还有归一化、批量归一化等等，但不是很懂，不知道他们具体是什么、有什么区别。</p><p>百度上找了挺多文章，讲得都不是很系统，比如大多文章都没讲懂标准化和归一化的区别或者是不同文章讲的内容矛盾了。</p><p>用谷歌一搜，就找到了很多很有价值的相关文章，然后我也写了这篇文章做个记录。</p><p>相对来讲，中文社区要比英文社区差些，部分原因是名词滥用或中英翻译问题，比如标准化和归一化、常量指针和指针常量。emmm</p><h1 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h1><p>本文主要讲解了以下内容</p><ol><li>特征缩放是什么/特征缩放的定义</li><li>归一化是什么/归一化的定义</li><li>标准化是什么/标准化的定义</li><li>归一化和标准化的区别</li><li>为什么要进行特征缩放/特征缩放有什么作用</li><li>有哪些常见的特征缩放方法</li><li>什么时候适合进行特征缩放/特征缩放方法的应用</li></ol><h1 id="名词定义"><a href="#名词定义" class="headerlink" title="名词定义"></a>名词定义</h1><p>这几个词是有区别的，特别是标准化和归一化，不要滥用（在本文中也请区分这几个词）。</p><div class="table-container"><table><thead><tr><th style="text-align:center">中文</th><th style="text-align:center">英文</th></tr></thead><tbody><tr><td style="text-align:center">特征缩放</td><td style="text-align:center">Feature Scaling</td></tr><tr><td style="text-align:center">标准化</td><td style="text-align:center">Standardization(Z-Score Normalization)</td></tr><tr><td style="text-align:center">归一化</td><td style="text-align:center">Normalization</td></tr></tbody></table></div><p>可以认为Feature Scaling包括Standardization和Normalization，其中Standardization可以叫做Z-Score Normalization。</p><h1 id="为什么需要进行特征缩放"><a href="#为什么需要进行特征缩放" class="headerlink" title="为什么需要进行特征缩放"></a>为什么需要进行特征缩放</h1><h2 id="背景：多指标量纲和数量级不同"><a href="#背景：多指标量纲和数量级不同" class="headerlink" title="背景：多指标量纲和数量级不同"></a>背景：多指标量纲和数量级不同</h2><p>使用单一指标对某事物进行评价并不合理，因此需要多指标综合评价方法。多指标综合评价方法，就是把描述某事物不同方面的多个指标综合起来得到一个综合指标，并通过它评价、比较该事物。</p><p>由于性质不同，不同评价指标通常具有不同的量纲和数量级。当各指标相差很大时，如果直接使用原始指标值计算综合指标，就会突出数值较大的指标在分析中的作用、削弱数值较小的指标在分析中的作用。</p><p>为消除各评价指标间量纲和数量级的差异、保证结果的可靠性，就需要对各指标的原始数据进行特征缩放（也有数据标准化、数据归一化的说法，但这些叫法不准确，所以不推荐）。</p><p>由于量纲和数量级不同，所以需要特征缩放。特征缩放可以显著提升部分机器学习算法的性能，但它对部分算法没有帮助。</p><h2 id="不进行特征缩放会有什么后果"><a href="#不进行特征缩放会有什么后果" class="headerlink" title="不进行特征缩放会有什么后果"></a>不进行特征缩放会有什么后果</h2><p>假如特征$x_1$的数值是100左右，特征$x_2$的数值是1左右，方程为$y=b+w_1x_1+w_2x_2$，那$w_1$对$y$的影响就更大，对Loss的影响也更大，损失函数关于$w_1$的梯度也更大，而损失函数关于$w_2$的梯度却很小，因此两个特征就不能使用相同的学习率。</p><p>不进行特征缩放的话，Error Surface就是一个椭圆，梯度下降时不一定是朝着最优点（圆心），速度就慢。</p><p>如果进行了特征缩放，Error Surface会尽可能趋近于圆，因此梯度下降时会一直朝着最优点（圆心），所以速度快。</p><h1 id="各类算法是否需要进行特征缩放"><a href="#各类算法是否需要进行特征缩放" class="headerlink" title="各类算法是否需要进行特征缩放"></a>各类算法是否需要进行特征缩放</h1><h2 id="基于梯度下降的算法（Gradient-Descent-Based-Algorithms）"><a href="#基于梯度下降的算法（Gradient-Descent-Based-Algorithms）" class="headerlink" title="基于梯度下降的算法（Gradient Descent Based Algorithms）"></a>基于梯度下降的算法（Gradient Descent Based Algorithms）</h2><p>在基于梯度下降进行优化的算法中，需要进行特征缩放，比如线性回归、逻辑回归、神经网络等。</p><p>因为计算梯度时会使用特征的值，如果各特征的的取值范围差异很大，不同特征对应梯度的值就会差异很大。</p><p>为保证平滑走到最优点、按相同速率更新各特征的权重，需要进行特征放缩。</p><p>通过特征放缩，可以使数值范围变小，进而加速梯度下降。</p><h2 id="基于距离的算法（Distance-Based-Algorithms）"><a href="#基于距离的算法（Distance-Based-Algorithms）" class="headerlink" title="基于距离的算法（Distance-Based Algorithms）"></a>基于距离的算法（Distance-Based Algorithms）</h2><p>在基于距离进行优化的算法中，需要进行特征缩放，比如K近邻、K-Means、SVM、PCA等。</p><p>因为这些算法是基于数据点的特征值计算它们的距离，距离越小则两者越相似。</p><h2 id="基于树的算法（Tree-Based-Algorithms）"><a href="#基于树的算法（Tree-Based-Algorithms）" class="headerlink" title="基于树的算法（Tree-Based Algorithms）"></a>基于树的算法（Tree-Based Algorithms）</h2><p>基于树的算法（比如决策树）对特征（features）的数值范围并不敏感，不需要进行特征缩放。</p><p>决策树仅基于单个feature拆分节点，并不受其它feature的影响。</p><h2 id="线性判别分析、朴素贝叶斯等算法"><a href="#线性判别分析、朴素贝叶斯等算法" class="headerlink" title="线性判别分析、朴素贝叶斯等算法"></a>线性判别分析、朴素贝叶斯等算法</h2><p>这两个算法处理了特征数量级差异大的问题，因此不需要进行特征缩放。</p><h1 id="四种特征缩放的方法"><a href="#四种特征缩放的方法" class="headerlink" title="四种特征缩放的方法"></a>四种特征缩放的方法</h1><p>标准化和归一化都可以实现特征缩放，但两者是有区别的。</p><p>假设我们有一份数据$x$，它有$N$行$M$列，即有$N$个对象，每个对象有$M$个特征，$x^i_j$表示对象$i$的特征$j$。</p><ul><li>Standardization（Z-Score Normalization）</li><li>Mean Normalization</li><li>Min-Max Normalization</li><li>Unit Vector Normalization/Scaling to unit length</li></ul><h1 id="标准化（Standardization-Z-Score-Normalization）"><a href="#标准化（Standardization-Z-Score-Normalization）" class="headerlink" title="标准化（Standardization/Z-Score Normalization）"></a>标准化（Standardization/Z-Score Normalization）</h1><ul><li><p>定义</p><p>  公式为$\hat x[:,j]=\frac{x[:,j]-mean(x[:,j])}{std(x[:,j])}$，其中$mean$代表平均值，$std$代表标准差，</p><p>  $mean(x[:,j])=\frac{1}{N}\cdot\sum_{i=1}^Nx[i,j]$，</p><p>  $std(x[:,j])=\sqrt{\frac{1}{N-1}\cdot\sum^N_{i=1}(x[i,j]-\mu_j)^2}$。</p></li><li><p>特点</p><p>  使数据的平均值变为0、标准差变为1，不改变数据的分布类型，数值范围不一定，消除了数据的量纲差异。</p></li><li><p>假设</p><p>  标准化假设数据是正态分布，但这个要求并不十分严格，如果数据是正态分布则该技术会更有效。</p></li><li><p>何时使用</p><p>  当我们使用的算法假设数据是正态分布时，可以使用Standardization，比如线性回归、逻辑回归、线性判别分析。</p><p>  因为Standardization使数据平均值为0，也可以在一些假设数据中心为0（zero centric data）的算法中使用，比如主成分分析（PCA）。</p></li></ul><h1 id="归一化（Normalization）"><a href="#归一化（Normalization）" class="headerlink" title="归一化（Normalization）"></a>归一化（Normalization）</h1><ul><li><p>特点</p><p>  把数据调整到[0,1]，并且消除了数据的量纲差异。</p><p>  也可以把数据调到[-1,1]，在使用SVM和Adaboost时就需要这样。</p></li><li><p>何时使用</p><p>  当我们不知道数据分布时或者我们知道数据不是正态分布时，这是一个很好的方法。</p><p>  换种说法就是，当我们使用的算法没有假设数据的分布类型时，就可以使用Normalization，比如K近邻算法和人工神经网络。</p></li></ul><h2 id="Mean-Normalization"><a href="#Mean-Normalization" class="headerlink" title="Mean Normalization"></a>Mean Normalization</h2><ul><li><p>定义</p><p>  $\hat x[:,j]=\frac{x[:,j]-mean(x[:,j])}{max(x[:,j])-min(x[:,j])}$</p><p>  其中$mean$代表平均值，$max$代表最大值，$min$代表最小值。</p></li><li><p>特点</p><p>  把数据调到[-1,1]，平均值为0</p></li><li><p>何时使用</p><p>  一些假设数据中心为0（zero centric data）的算法，比如主成分分析（PCA）。</p></li></ul><h2 id="Min-Max-Normalization"><a href="#Min-Max-Normalization" class="headerlink" title="Min-Max Normalization"></a>Min-Max Normalization</h2><ul><li><p>定义</p><p>  $\hat x[:,j]=\frac{x[:,j]-min(x[:,j])}{max(x[:,j])-min(x[:,j])}$</p><p>  其中$max$代表最大值，$min$代表最小值。</p></li><li><p>特点</p><p>  把数据调到[0,1]</p></li><li><p>何时使用</p><p>  当处理具有严格数值范围要求的数据（比如图片）时，这非常有用。</p></li></ul><h2 id="Unit-Vector-Normalization-Scaling-to-unit-length"><a href="#Unit-Vector-Normalization-Scaling-to-unit-length" class="headerlink" title="Unit Vector Normalization/Scaling to unit length"></a>Unit Vector Normalization/Scaling to unit length</h2><ul><li><p>定义</p><p>  $\hat x[i,:]=\frac{x[i,:]}{||x[i,:]||}$</p><p>  把每个对象的特征向量变成单位长度。</p></li><li><p>特点</p><p>  把数据调到[0,1]</p></li><li><p>何时使用</p><p>  当处理具有严格数值范围要求的数据（比如图片）时，这非常有用。</p></li></ul><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>下面几篇文章确实讲得很清晰、精准，作为机器学习新手的我都能看懂。</p><p>这篇文章的内容也都是从下面这些文章里提取总结出来的，所以如果我上面哪里讲得不清楚、不好的话，建议直接查看下面几篇文章或者直接谷歌搜索相关名词。</p><ol><li><p><a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96" target="_blank" rel="noopener">https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96</a></p></li><li><p><a href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/</a></p><p> 这篇文章讲得很不错，建议参考。</p></li><li><p><a href="https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e" target="_blank" rel="noopener">https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e</a></p><p> 这篇文章讲得很不错，建议参考。</p></li><li><p><a href="https://machinelearningmastery.com/normalize-standardize-machine-learning-data-weka/" target="_blank" rel="noopener">https://machinelearningmastery.com/normalize-standardize-machine-learning-data-weka/</a></p><p> 这篇文章讲了Weka的使用，但我们并不需要看Weka怎么用，看文章里其它理论部分即可。</p></li><li><p><a href="https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0" target="_blank" rel="noopener">https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0</a></p><p> 这篇文章更偏向于实践一些，也很不错。</p></li><li><p><a href="https://en.wikipedia.org/wiki/Feature_scaling" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Feature_scaling</a></p></li></ol><hr><p>作者：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">https://www.cnblogs.com/chouxianyu/</a></p><p>欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;我在学李宏毅的机器学习课程，助教给的回归作业代码中有数据标准化的操作。&lt;/p&gt;
&lt;p&gt;我听过数据标准化，还有归一化、批量归一化等等，但不是很
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征缩放" scheme="https://chouxianyu.github.io/tags/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE/"/>
    
      <category term="标准化" scheme="https://chouxianyu.github.io/tags/%E6%A0%87%E5%87%86%E5%8C%96/"/>
    
      <category term="归一化" scheme="https://chouxianyu.github.io/tags/%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
      <category term="梯度下降" scheme="https://chouxianyu.github.io/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>ModuleNotFoundError-No module named jupyter_nbextensions_configurator</title>
    <link href="https://chouxianyu.github.io/2020/10/24/ModuleNotFoundError-No-module-named-jupyter-nbextensions-configurator/"/>
    <id>https://chouxianyu.github.io/2020/10/24/ModuleNotFoundError-No-module-named-jupyter-nbextensions-configurator/</id>
    <published>2020-10-24T00:50:46.000Z</published>
    <updated>2020-10-24T00:53:37.452Z</updated>
    
    <content type="html"><![CDATA[<p>启动Anaconda中的Jupyter Notebook的时候，出现如下错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[W 08:37:25.964 NotebookApp] Error loading server extension jupyter_nbextensions_configurator</span><br><span class="line">    Traceback (most recent call last):</span><br><span class="line">      File &quot;D:\WorkingSoftware\Anaconda3\lib\site-packages\notebook\notebookapp.py&quot;, line 1615, in init_server_extensions</span><br><span class="line">        mod = importlib.import_module(modulename)</span><br><span class="line">      File &quot;D:\WorkingSoftware\Anaconda3\lib\importlib\__init__.py&quot;, line 127, in import_module</span><br><span class="line">        return _bootstrap._gcd_import(name[level:], package, level)</span><br><span class="line">      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import</span><br><span class="line">      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load</span><br><span class="line">      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 965, in _find_and_load_unlocked</span><br><span class="line">    ModuleNotFoundError: No module named &apos;jupyter_nbextensions_configurator&apos;</span><br></pre></td></tr></table></figure><p>经查询，在Anaconda Prompt某Python环境（我使用的是Anaconda默认的base环境）中运行如下命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install jupyter_nbextensions_configurator</span><br></pre></td></tr></table></figure><p>再次打开Jupyter Notebook时就可以发现问题已经解决。</p><hr><p>作者：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">https://www.cnblogs.com/chouxianyu/</a></p><p>欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;启动Anaconda中的Jupyter Notebook的时候，出现如下错误&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;
      
    
    </summary>
    
    
      <category term="Bug" scheme="https://chouxianyu.github.io/tags/Bug/"/>
    
      <category term="python" scheme="https://chouxianyu.github.io/tags/python/"/>
    
      <category term="Jupyter" scheme="https://chouxianyu.github.io/tags/Jupyter/"/>
    
      <category term="Anaconda" scheme="https://chouxianyu.github.io/tags/Anaconda/"/>
    
  </entry>
  
  <entry>
    <title>VSCode运行Python代码:文件存在但出现FileNotFoundError</title>
    <link href="https://chouxianyu.github.io/2020/10/23/VSCode%E8%BF%90%E8%A1%8CPython%E4%BB%A3%E7%A0%81-%E6%96%87%E4%BB%B6%E5%AD%98%E5%9C%A8%E4%BD%86%E5%87%BA%E7%8E%B0FileNotFoundError/"/>
    <id>https://chouxianyu.github.io/2020/10/23/VSCode运行Python代码-文件存在但出现FileNotFoundError/</id>
    <published>2020-10-23T13:25:39.000Z</published>
    <updated>2020-10-23T13:28:51.229Z</updated>
    
    <content type="html"><![CDATA[<p>今天使用VSCode运行Python代码，读取当前目录下的CSV文件，代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">'./train.csv'</span>)</span><br></pre></td></tr></table></figure><p>我已确认过我的Python代码文件和<code>train.csv</code>在同一个目录，正常情况下这份代码不应该报错的。</p><p>但我运行这份代码时，终端内容如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">E:\Study\LHYMachineLearning&gt;python -u "e:\Study\LHYMachineLearning\LHYMLCode\hw1_regression\hw1_regression.py"</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">………………&lt;此处内容省略&gt;</span><br><span class="line">FileNotFoundError: [Errno 2] File b'./train.csv' does not exist: b'./train.csv'</span><br></pre></td></tr></table></figure><p>但是如果我修改VSCode终端的所在路径，<strong>这样运行就不会报错</strong>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E:\Study\LHYMachineLearning\LHYMLCode\hw1_regression&gt;python -u hw1_regression.py</span><br></pre></td></tr></table></figure><p>所以问题就在于运行命令时VSCode终端的所在路径，这里不再过多解释，上面两条命令的对比十分明显了。</p><hr><p>作者：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">https://www.cnblogs.com/chouxianyu/</a></p><p>欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天使用VSCode运行Python代码，读取当前目录下的CSV文件，代码如下&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/spa
      
    
    </summary>
    
    
      <category term="Bug" scheme="https://chouxianyu.github.io/tags/Bug/"/>
    
      <category term="python" scheme="https://chouxianyu.github.io/tags/python/"/>
    
      <category term="VSCode" scheme="https://chouxianyu.github.io/tags/VSCode/"/>
    
  </entry>
  
  <entry>
    <title>(hexo)YAMLException can not read a block mapping entry; a multiline key may not be an implicit key</title>
    <link href="https://chouxianyu.github.io/2020/10/19/hexo-YAMLException-can-not-read-a-block-mapping-entry-a-multiline-key-may-not-be-an-implicit-key/"/>
    <id>https://chouxianyu.github.io/2020/10/19/hexo-YAMLException-can-not-read-a-block-mapping-entry-a-multiline-key-may-not-be-an-implicit-key/</id>
    <published>2020-10-19T11:19:09.000Z</published>
    <updated>2020-10-19T11:20:52.850Z</updated>
    
    <content type="html"><![CDATA[<p>运行<code>hexo g</code>后，报错如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">YAMLException can not read a block mapping entry; a multiline key may not be an implicit key</span><br></pre></td></tr></table></figure><p>根据报错定位（一般会定位到文件的某行某列），检查一下是不是哪里少了空格，我的情况是文章开头的tags后边少了个空格。</p><hr><p>作者：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">https://www.cnblogs.com/chouxianyu/</a></p><p>欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;运行&lt;code&gt;hexo g&lt;/code&gt;后，报错如下：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;
      
    
    </summary>
    
    
      <category term="hexo" scheme="https://chouxianyu.github.io/tags/hexo/"/>
    
  </entry>
  
</feed>
