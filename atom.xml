<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>臭咸鱼的缺氧瓶</title>
  
  <subtitle>快给我氧气！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://chouxianyu.github.io/"/>
  <updated>2021-04-14T02:40:15.998Z</updated>
  <id>https://chouxianyu.github.io/</id>
  
  <author>
    <name>臭咸鱼</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.5详解基于LSTM的RNN</title>
    <link href="https://chouxianyu.github.io/2021/04/14/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-5%E8%AF%A6%E8%A7%A3%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84RNN/"/>
    <id>https://chouxianyu.github.io/2021/04/14/李宏毅机器学习课程笔记-9-5详解基于LSTM的RNN/</id>
    <published>2021-04-14T02:38:35.000Z</published>
    <updated>2021-04-14T02:40:15.998Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1层LSTM神经元的架构"><a href="#1层LSTM神经元的架构" class="headerlink" title="1层LSTM神经元的架构"></a>1层LSTM神经元的架构</h2><p>根据上述内容，你可能看不出LSTM与RNN有什么关系，接下来具体介绍LSTM在RNN中的应用。</p><p>假设我们现在有一些LSTM（下图中白色部分）作为神经元，每个LSTM的memory cell里都存了一个scalar值（下图中红框中内容），把这些scalar连接起来就组成了1个vector $c^{t-1}$，即关于上个input（时间点为t-1）的memory。</p><p><img src="https://pic4.zhimg.com/80/v2-d96ed6d363d633b87729a22badf815a9_720w.png" alt="img"></p><p><strong>在时间点t，输入为1个vector $x^t$，它会经过4个线性的transform得到$z^f,z^i,z,z^o$，$z^f,z^i,z,z^o$这4个vector的dimension数量和LSTM神经元的数量相等，这4个vector的1个dimension即为1个LSTM神经元的输入（4个vector的第1个dimension为第1个LSTM神经元的输入）。</strong></p><h2 id="1个LSTM神经元的运算方法"><a href="#1个LSTM神经元的运算方法" class="headerlink" title="1个LSTM神经元的运算方法"></a>1个LSTM神经元的运算方法</h2><p>下图是单个LSTM神经元的运算方法，其4个input分别是$z$、$z^i$、$z^f$和$z^o$的其中1维（1维为1个神经元的输入）。每个LSTM神经元的input是各不相同的，但它们可以共同运算。</p><p>1个LSTM神经元的运算方法如下图所示。</p><p><img src="https://pic3.zhimg.com/80/v2-bc5c546a495db0ef197f4527f841562c_720w.png" alt="img"></p><p>$f(z^f)$与上一个时间点的memory $c^{t-1}$对应的cell值相乘，加上$g(z)$与$f(z^i)$的乘积，得到该时刻该cell中的值$c^t$，最终再乘以output gate的信号$f(z^o)$，得到输出$y^t$。</p><h2 id="1个LSTM神经元在相邻时刻时的运算方法"><a href="#1个LSTM神经元在相邻时刻时的运算方法" class="headerlink" title="1个LSTM神经元在相邻时刻时的运算方法"></a>1个LSTM神经元在相邻时刻时的运算方法</h2><p><img src="https://pic1.zhimg.com/80/v2-993d9ee86e0180e96e405e6b3248b41f_720w.png" alt="img"></p><p>上图是同1个LSTM神经元在2个相邻时刻的运算方法，其中与前文描述略有不同的是，这里还需要把当前时刻该神经元的输出$y^t$以及该神经元中cell保存的值$c^t$（peephole）都连接到下一时刻的输入上。因此在$t+1$时刻，神经元不只是考虑当前的输入$x^{t+1}$，还要看前一时刻该神经元的输出$h^t$和cell保存值$c^t$。</p><p>如何考虑结合$t+1$时刻的输入$x^{t+1}$和上一时刻该神经元的信息$h^t,c^t$呢？====&gt;<strong>把$x^{t+1}$、$h^t$和$c^t$这3个vector并在一起</strong>，乘上4个不同的转换矩阵，得到该神经元$t+1$时刻的4个输入$z$、$z^i$、$z^f$、$z^o$。</p><h2 id="多层LSTM在相邻时刻的运算方法"><a href="#多层LSTM在相邻时刻的运算方法" class="headerlink" title="多层LSTM在相邻时刻的运算方法"></a>多层LSTM在相邻时刻的运算方法</h2><p><img src="https://pic2.zhimg.com/80/v2-a6435474533a8871ad59ed5443055159_720w.png" alt="img"></p><p>上图中左边一列的2个LSTM代表2层LSTM，右边一列的2个LSTM则代表它们在下一时刻的状态。即横向是时间轴，纵向是层轴。</p><p>虽然看起来很复杂，感觉不一定work，但LSTM在RNN中已成为了标准做法。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1层LSTM神经元的架构&quot;&gt;&lt;a href=&quot;#1层LSTM神经元的架构&quot; class=&quot;headerlink&quot; title=&quot;1层LSTM神经元的架构&quot;&gt;&lt;/a&gt;1层LSTM神经元的架构&lt;/h2&gt;&lt;p&gt;根据上述内容，你可能看不出LSTM与RNN有什么关系，接下来
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="LSTM" scheme="https://chouxianyu.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.4LSTM入门</title>
    <link href="https://chouxianyu.github.io/2021/04/13/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-4LSTM%E5%85%A5%E9%97%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/13/李宏毅机器学习课程笔记-9-4LSTM入门/</id>
    <published>2021-04-13T00:35:26.000Z</published>
    <updated>2021-04-13T00:47:38.903Z</updated>
    
    <content type="html"><![CDATA[<p>LSTM即Long Short-term Memory。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>前几篇文章提到的RNN都比较简单，可以任意读写memory，没有进一步对memory进行管理。<strong>现在常用的memory管理方式是LSTM</strong>。正如其名，LSTM是比较长的短期记忆，<code>-</code>是在short和term之间。<strong>前几篇提到的RNN在有新的输入时都会更新memory，这样的memory是非常短期的，而LSTM中可以有更久之前的memory</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-2d2d4f677d917a001fcf379e00472bbf_720w.png" alt="img"></p><p>如上图所示，LSTM中有3个gate、4个输入(3个gate控制信号和1个想要写入memory cell的值)和1个输出：</p><ul><li>input gate：当某个neuron的输出想要被写进memory cell，它要先经过input gate。如果input gate是关闭的，则任何内容都无法被写入。input gate的关闭与否、什么时候开闭是由神经网络学习到的。</li><li>output gate：output gate决定了外界是否可以从memory cell中读取数据。当output gate关闭的时候，memory里面的内容无法被读取。output gate的关闭与否、什么时候开闭也是由神经网络学习到的。</li><li>forget gate：forget gate决定什么时候需要把memory cell里存放的内容忘掉，什么时候要保存。这也是由神经网络学习到的。</li></ul><h2 id="LSTM计算式"><a href="#LSTM计算式" class="headerlink" title="LSTM计算式"></a>LSTM计算式</h2><p>下图展示了LSTM的计算式。</p><p><img src="https://pic1.zhimg.com/80/v2-de00669f5a90c477fb32b2fffb71571d_720w.png" alt="img"></p><ul><li>$z$是想要被存到memory cell里的值</li><li>$z_i$是input gate的控制信号</li><li>$z_o$是output gate的控制信号</li><li>$z_f$是forget gate的控制信号</li><li>$a$是综合上述4个输入得到的输出值</li></ul><p>$z$、$z_i$、$z_o$和$z_f$通过激活函数分别得到$g(z)$、$f(z_i)$、$f(z_o)$和$f(z_f)$，其中$z_i$、$z_o$和$z_f$的激活函数$f()$一般会选sigmoid函数，因为其输出在0~1之间，可表示gate的开启程度。</p><p>令$g(z)$与$f(z_i)$相乘得到$g(z)f(z_i)$，然后把原先存放在memory cell中的$c$与$f(z_f)$相乘得到$cf(z_f)$，两者相加得到存在memory cell中的新值$c’=g(z)f(z_i)+cf(z_f)$。</p><ul><li><p>若$f(z_i)=0$，则相当于并不使用输入$z$更新memory；若$f(z_i)=1$，则相当于直接输入$g(z)$。</p></li><li><p>若$f(z_f)=1$，则不忘记memory cell中的原值$c$；若$f(z_f)=0$，则原值$c$将被遗忘清除。</p><p>  可以看出，forget gate的逻辑与直觉是相反的，该控制信号打开表示记得原值，关闭却表示遗忘。这个gate取名为remember gate更好些。</p></li></ul><p>此后，$c’$通过激活函数得到$h(c’)$，与output gate的$f(z_o)$相乘，得到输出$a=h(c’)f(z_o)$。</p><h2 id="Apply-LSTM-to-NN"><a href="#Apply-LSTM-to-NN" class="headerlink" title="Apply LSTM  to NN"></a>Apply LSTM  to NN</h2><p>上述的LSTM应该如何应用于神经网络呢？其实直接把LSTM作为1个神经元就可以了。假设输入层有2个标量输入$x_1,x_2$，隐藏层中有2个神经元，每个神经元输出1个标量，则其结构如下图所示。</p><p><img src="https://pic3.zhimg.com/80/v2-af52398f38860c122b0741e07ebb3dbd_720w.png" alt="img"></p><ul><li><strong>标量输入$x_1,x_2$乘以4个参数得到4个值，这4个值作为LSTM的4个input</strong>。</li><li>在普通的神经元中，1个input对应1个output；而在LSTM中4个input才产生1个output，并且所有的input都是不相同的。</li><li>LSTM所需要的参数量是普通NN的4倍。</li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;LSTM即Long Short-term Memory。&lt;/p&gt;
&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;前几篇文章提到的RNN都比较简单，可以任意读写mem
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="LSTM" scheme="https://chouxianyu.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.3RNN的应用</title>
    <link href="https://chouxianyu.github.io/2021/04/12/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-3RNN%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/12/李宏毅机器学习课程笔记-9-3RNN的应用/</id>
    <published>2021-04-12T00:54:54.000Z</published>
    <updated>2021-04-12T01:02:36.510Z</updated>
    
    <content type="html"><![CDATA[<p>在Slot Filling中，输入是1个word vector，输出是每个word的label，<strong>输入和输出是等长的</strong>。</p><p>然而，RNN还可以实现多对1、多对多…</p><h2 id="Many-to-One"><a href="#Many-to-One" class="headerlink" title="Many to One"></a>Many to One</h2><p>Many to One：输入是1个vector sequence，输出是1个vector</p><ul><li><p>Sentiment Analysis</p><p>  输入1篇文章或1句话等（1个vector sequence），输出其情感倾向（分类或者回归，比如超好、好、普通、差、超差、[-1,1]）。</p></li><li><p>Key Term Extraction</p><p>  输入是1篇文章等，输出是几个关键词。</p></li></ul><h2 id="Many-to-Many"><a href="#Many-to-Many" class="headerlink" title="Many to Many"></a>Many to Many</h2><p><strong>Many to Many：输入和输出都是sequence，但输出更短</strong></p><p>比如Speech Recognition。输入是1段声音信号，每隔1小段时间（通常很短，比如0.01秒）就用1个vector表示，输出是1段文字。因此输入是1个vector sequence，而输出是1个charactor sequence，并且<strong>输入序列要比输出序列短</strong>。</p><p>如果仍然使用Slot Filling的方法，就只能做到输入的每个vector对应输出1个character，输入1句“好棒”的语音后可能输出文字“好好棒棒棒”，但其实应该输出文字“好棒”。我们可以通过<strong>Trimming</strong>去除输出中相同的character，但语音“好棒”和语音“好棒棒”是不同的，应该如何区分呢？可以用<strong>CTC(Connectionist Temporal Classification)</strong>，其基本思路是可以在输出中填充NULL，最终输出时删除NULL即可。</p><p><img src="https://pic2.zhimg.com/80/v2-c135e0952e8aa68cf0831e64c0cb0105_720w.png" alt="img"></p><p>如上图所示，输入中vector的数量多于label中character的数量，那CTC应该怎么训练呢？答案是假设所有的可能性都是对的。</p><h2 id="Many-to-Many-1"><a href="#Many-to-Many-1" class="headerlink" title="Many to Many"></a>Many to Many</h2><p><strong><a href="http://www.oalib.com/paper/4068742" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a>：输入和输出都是sequence，但两者长度不确定。</strong></p><p>以机器翻译为例，RNN要将英文的word sequence翻译成中文的character sequence（并不知道哪个sequence更长或更短）。</p><p><img src="https://pic1.zhimg.com/80/v2-67cb0355203c2db204efaba8eff6b2f2_720w.png" alt="img"></p><p>如上图所示，假设RNN的输入“machine learning”，在2个时间点分别输入”machine”和”learning”，在最后1个时间点时memory中就存储了整个word sequence的信息。接下来让RNN输出，得到“机”，然后把“机”当做input（这1步有很多极技巧，这里省略），并读取memory中的信息，就会输出“器”，以此类推，RNN会一直输出但不知道什么时候停止。那怎么让RNN停止输出呢？可以添加1个symbol<code>===</code>标志停止，当RNN输出这个symbol时就停止输出。</p><h2 id="Seq2Seq-for-Syntatic-Parsing"><a href="#Seq2Seq-for-Syntatic-Parsing" class="headerlink" title="Seq2Seq for Syntatic Parsing"></a>Seq2Seq for Syntatic Parsing</h2><p><a href="http://arxiv.org/abs/1412.7449" target="_blank" rel="noopener">Grammar as a Foreign Langauage</a>： 输入为1个word sequence，输出1个语法树（可以用sequence表示）。</p><h2 id="Seq2Seq-Auto-encoder-for-Text"><a href="#Seq2Seq-Auto-encoder-for-Text" class="headerlink" title="Seq2Seq Auto-encoder for Text"></a>Seq2Seq Auto-encoder for Text</h2><p>如果用bag-of-word来表示1段文本，就容易丢失word之间的联系和语序上的信息。比如“白血球消灭了感染病”和“感染病消灭了白血球”这2段文本语义完全相反但bag-of-word是相同的。</p><p><a href="https://arxiv.org/abs/1506.01057" target="_blank" rel="noopener">A Hierarchical Neural Autoencoder for Paragraphs and Documents</a>：可以使用Seq2Seq Autoencoder，在考虑语序的情况下把文章编码成vector，只需要将RNN作为编码器和解码器即可。</p><p><img src="https://pic2.zhimg.com/80/v2-58c2031f882fd46db6171384e461756d_720w.png" alt="img"></p><p>如上图所示，word sequence输入RNN后被编码成embedded vector，然后再通过另1个RNN解码，如果解码后能得到一模一样的句子，则编码得到的vector就表示了这个word sequence中最重要的信息。</p><p><img src="https://pic4.zhimg.com/80/v2-e67956b01d8f5c5c2bd4b209a9c816ff_720w.png" alt="img"></p><p>如上图所示，这个过程可以是分层的（hierarchical），可以将每1个sentence编码成1个vector然后将它们加起来得到表示整个document的vector，然后再通过它产生多个setence的vector，然后将多个setence的vector解码得到word sequence。这是1个4层的LSTM（word sequence-sentence sequence-document-sentence sequence-word sequence）。</p><p>Seq2Seq Auto-encoder比较容易得到文法的编码，而Skip Thought（输入1个句子，输出其下1句）更容易得到语义的意思。</p><h2 id="Seq2Seq-Auto-encoder-for-Speech"><a href="#Seq2Seq-Auto-encoder-for-Speech" class="headerlink" title="Seq2Seq Auto-encoder for Speech"></a>Seq2Seq Auto-encoder for Speech</h2><p><a href="https://arxiv.org/abs/1603.00982" target="_blank" rel="noopener">Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder</a></p><p>Seq2Seq Auto-encoder还可以用在语音上，它可以把1个audio segment(word-level)编码成1个fixed-length vector。这有什么用处呢？它可以基于语音之间的相似度做语音搜索。</p><p><img src="https://pic1.zhimg.com/80/v2-ac9ee7ee9e1c47683525266543036146_720w.png" alt="img"></p><p>那如何基于语音之间的相似度做语音搜索呢？如上图所示，假如有1个语音的database，可将其划分为audio segments（长度可变），然后使用Seq2Seq Auto-encoder将其编码为1个fixed-length vector。对于1段需要搜索的语音，通过Seq2Seq Auto-encoder将其编码成1个fixed-length vector，计算其与database中audio segments的vector的相似度。</p><p><img src="https://pic1.zhimg.com/80/v2-286be450d67ee1a810c5dac8889deba1_720w.png" alt="img"></p><p>那如何把1个audio segment编码成1个fixed-length vector呢？如上图所示，首先把audio segment转换为acoustic feature sequence，然后输入至RNN。该RNN作为Encoder，在最后1个时间点其memory中的值就代表整个acoustic feature sequence，这就是我们想要的vector。但是只有这个作为Encoder的RNN我们没有办法训练，所以还要训练1个作为Decoder的RNN。该RNN作为Decoder，以Encoder在最后1个时间点时memory中的vector为输入，然后输出1个acoustic feature sequence，训练目标是输出的acoustic feature sequence和输入的acoustic feature sequence越接近越好。由此可知，该例中Encoder和Decoder是要同时训练的。</p><h2 id="Attention-based-Model"><a href="#Attention-based-Model" class="headerlink" title="Attention-based Model"></a>Attention-based Model</h2><blockquote><p>專家發現，小兒失憶現象是由於動物的大腦在神經新生的過程中，處於不斷重組的狀態，為減少太多訊息的干擾，會不斷清除舊記憶，從而增加對新事物的學習能力。年幼小鼠的記憶保留能力所以低下，乃因其高度活躍的神經再生所致，而成年小鼠保留記憶能力的增加，也由於其大腦相對成熟，海馬體的神經再生活力已經下降。腦科學家既然可以抑制年幼小鼠海馬體的高度活躍神經再生活力，又可刺激成年小鼠海馬體增加其神經再生活力。</p><p>——————引自<a href="http://henrylo1605.blogspot.com/2015/05/blog-post_56.html" target="_blank" rel="noopener">http://henrylo1605.blogspot.com/2015/05/blog-post_56.html</a></p></blockquote><p>现在除了RNN之外，Attention-based Model也用到了memory的思想。机器也可以有记忆，神经网络通过操控读/写头去读/写信息，这个就是Neural Turing Machine。</p><h2 id="Reading-Comprehension"><a href="#Reading-Comprehension" class="headerlink" title="Reading Comprehension"></a>Reading Comprehension</h2><p>Attention-based Model常常用在Reading Comprehension上，让机器读1篇document，再把每个setence变成代表语义的vector，接下来让用户向机器提问，神经网络就会去调用读写头，取出memory中与查询语句相关的信息，综合处理之后，可以给出正确的回答。</p><h2 id="Visual-Question-Answering"><a href="#Visual-Question-Answering" class="headerlink" title="Visual Question Answering"></a>Visual Question Answering</h2><h2 id="Speech-Question-Answering"><a href="#Speech-Question-Answering" class="headerlink" title="Speech Question Answering"></a>Speech Question Answering</h2><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在Slot Filling中，输入是1个word vector，输出是每个word的label，&lt;strong&gt;输入和输出是等长的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;然而，RNN还可以实现多对1、多对多…&lt;/p&gt;
&lt;h2 id=&quot;Many-to-One&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.2如何训练RNN</title>
    <link href="https://chouxianyu.github.io/2021/04/11/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-2%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83RNN/"/>
    <id>https://chouxianyu.github.io/2021/04/11/李宏毅机器学习课程笔记-9-2如何训练RNN/</id>
    <published>2021-04-11T02:41:12.000Z</published>
    <updated>2021-04-11T02:43:24.928Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RNN的损失函数"><a href="#RNN的损失函数" class="headerlink" title="RNN的损失函数"></a>RNN的损失函数</h2><p>仍然以Slot Filling为例，如下图所示。</p><p><img src="https://pic2.zhimg.com/80/v2-bbab6148db7b998a52c988e160c3fd16_720w.png" alt="img"></p><p>对于1个word$x^i$，RNN输出1个one-hot编码的vector $y^i$，求$y^i$和对应label的交叉熵损失（Cross Entropy Loss），将多个word的loss求和即为RNN的损失函数。需要注意的是不能打乱word的语序，$x^{i+1}$要紧接着$x^i$输入。</p><p>确定RNN的损失函数后，RNN的训练其实也是用的梯度下降。训练前馈神经网络时我们使用有效的反向传播算法，为了方便地训练RNN，我们使用BPTT。基于BP，<strong>BPTT(Backpropagation Through Time)</strong>考虑了时间维度的信息。</p><h2 id="RNN的Error-Surface"><a href="#RNN的Error-Surface" class="headerlink" title="RNN的Error Surface"></a>RNN的Error Surface</h2><p>RNN的Error Surface如下图所示，其中$z$轴代表loss，$x$轴和$y$轴代表两个参数$w_1$和$w_2$。可以看出，RNN的Error Surface在某些地方非常平坦，在某些地方又非常的陡峭。<strong>这样的Error Surface导致在训练RNN时loss剧烈变化</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-9264ded1a5556fb9bce3cbd5f7d5a3aa_720w.png" alt="img"></p><h2 id="问题出现的原因"><a href="#问题出现的原因" class="headerlink" title="问题出现的原因"></a>问题出现的原因</h2><p>既然RNN的Error Surface中有这么平滑的地方，那会不会是sigmoid激活函数造成的梯度消失呢？原因并不是sigmoid，如果是的话，那换成ReLU就可以，但把sigmoid换成ReLU之后，效果反而更差了。那到底为什么会有非常陡峭和非常平滑的地方呢？</p><p><img src="https://pic4.zhimg.com/80/v2-e601dceb08a568fabb5e12ab811d310b_720w.png" alt="img"></p><p>如上图所示，假设某RNN只含1个神经元，并且该神经元是Linear的，input和output的weight都是1，没有bias，memory传递的weight是$w$，输入序列为[1, 0, 0, 0, …, 0]，所以$y^{1000}=w^{999}$。</p><p>现在我们考虑loss关于参数$w$的梯度，当$w:\ 1\ =&gt;\ 1.01$时，可知$y^{1000}:\ 1\ =&gt;\ 20000$，此时梯度很大；当$w:\ 0.99\ =&gt;\ 0.01$时，可知$y^{1000}$几乎没有变化，此时梯度很小。</p><p>从该例中可知，RNN的Error Surface中的“悬崖”出现的原因是，<strong>关于memory的参数$w$的作用随着时间增加不断增强，导致RNN出现梯度消失或梯度爆炸的问题</strong>。</p><h2 id="处理方法"><a href="#处理方法" class="headerlink" title="处理方法"></a>处理方法</h2><p>如何解决RNN梯度消失或梯度爆炸的问题？可以通过<strong>Clipping</strong>进行处理，Clipping的效果是使梯度不超过某个阈值，即当梯度即将超过某个阈值（比如15）时，就将梯度赋值为该阈值。</p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>有什么更好的方法可以解决RNN的Error Surface中的问题呢？LSTM就是使用最广泛的技巧，它可以“删除”Error Surface中比较平坦的部分，也就解决了梯度消失的问题，但它无法解决梯度爆炸的问题。正因如此，训练LSTM时需要将学习率调得特别小。</p><p>LSTM为什么可以解决RNN中梯度消失的问题呢，因为RNN和LSTM对memory的处理是不同的（LSTM有forget gate）。<strong>在RNN中，每个时间点memory中的旧值都会被新值覆盖，导致参数$w$对memory的影响每次都被清除，进而引发梯度消失。在LSTM中，每个时间点memory里的旧值会乘以$f(g(f))$再与新值相加，只有在forget gate被关闭时参数$w$对memory的影响才会被清除，在forget gate被打开时参数$w$对memory的影响就会通过累加得到保留，因此不会出现梯度消失的问题。</strong></p><p>LSTM在1997年被提出，第1版的LSTM被提出就是为了解决梯度消失的问题，但这1版本是没有forget gate的，forget gate是后来才加上去的。也有1种说法是，在训练LSTM时需要给forget gate特别大的bias，以确保forget gate在多数情况下是开启的。</p><h3 id="GRU（Gated-Recurrent-Unit-Cho-EMNLP’14）"><a href="#GRU（Gated-Recurrent-Unit-Cho-EMNLP’14）" class="headerlink" title="GRU（Gated Recurrent Unit, Cho, EMNLP’14）"></a>GRU（Gated Recurrent Unit, Cho, EMNLP’14）</h3><p>GRU比LSTM更简单，GRU只有2个gate，因此需要更少的参数量、鲁棒性更好、更不容易过拟合。GRU的基本思路是“旧的不去，新的不来”，GRU把input和forget gate联动起来，当forget gate把memory中的值清空时，input gate才会打开然后放入新的值。</p><h3 id="Clockwise-RNN（Jan-Koutnik-JMLR’14）"><a href="#Clockwise-RNN（Jan-Koutnik-JMLR’14）" class="headerlink" title="Clockwise RNN（Jan Koutnik, JMLR’14）"></a>Clockwise RNN（Jan Koutnik, JMLR’14）</h3><h3 id="SCRN（Structrally-Constrained-Recurrent-Network-Tomas-Mikolov-ICLR’15）"><a href="#SCRN（Structrally-Constrained-Recurrent-Network-Tomas-Mikolov-ICLR’15）" class="headerlink" title="SCRN（Structrally Constrained Recurrent Network, Tomas Mikolov, ICLR’15）"></a>SCRN（Structrally Constrained Recurrent Network, Tomas Mikolov, ICLR’15）</h3><h3 id="Vanilla-RNN-Initialized-with-Identity-Matrix-ReLU（Quoc-V-Le-arXiv’15）"><a href="#Vanilla-RNN-Initialized-with-Identity-Matrix-ReLU（Quoc-V-Le-arXiv’15）" class="headerlink" title="Vanilla RNN Initialized with Identity Matrix + ReLU（Quoc V.Le, arXiv’15）"></a>Vanilla RNN Initialized with Identity Matrix + ReLU（Quoc V.Le, arXiv’15）</h3><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;RNN的损失函数&quot;&gt;&lt;a href=&quot;#RNN的损失函数&quot; class=&quot;headerlink&quot; title=&quot;RNN的损失函数&quot;&gt;&lt;/a&gt;RNN的损失函数&lt;/h2&gt;&lt;p&gt;仍然以Slot Filling为例，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-9.1循环神经网络RNN入门</title>
    <link href="https://chouxianyu.github.io/2021/04/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-9-1%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN%E5%85%A5%E9%97%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/10/李宏毅机器学习课程笔记-9-1循环神经网络RNN入门/</id>
    <published>2021-04-10T00:53:08.000Z</published>
    <updated>2021-04-10T01:02:16.365Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Slot-Filling"><a href="#Slot-Filling" class="headerlink" title="Slot Filling"></a>Slot Filling</h2><p>比如在订票系统中，输入“Arrive Taipei on November 2nd”这样一个序列，我们设置几个slot(槽位)，希望算法能够将关键词“Taipei”放入Destination这个slot，将”November”和”2nd”放入到达时间Time of Arrival这个slot，而“Arrive”和“on”不属于任何slot。那这个算法如何实现呢？</p><h2 id="Slot-Filling-with-FNN"><a href="#Slot-Filling-with-FNN" class="headerlink" title="Slot Filling with FNN"></a>Slot Filling with FNN</h2><p>可以用Feedforward Neural Network实现Slot Filling吗？可以，下面介绍这种FNN的输入和输出，但其存在问题。</p><p>输入是一个word（比如“Taipei”）并用vector来表示它；输出是1个probablity distribution，表示输入的word属于各个slot的概率。</p><p>如何用vector表示1个word呢？方法有很多。比如<strong>1-of-N Encoding</strong>(又名one-hot Encoding)，如下图所示。设定1个lexicon(词汇表)，那vector的size就和lexicon的size相同，vector中的每个维度对应lexicon中的word，vector中word对应的维度的值为1、其它维度的值为0。</p><p><img src="https://pic1.zhimg.com/80/v2-44be0e091d503721158e1bbee1cdb048_720w.png" alt="img"></p><p>如下图所示，只有1-of-N Encoding还不够，一些word不在lexicon中，对此我们需要在lexicon中添加1个”<strong>other</strong>“。除了1-of-N Encoding，还可以通过<strong>word hashing</strong>。可以用1个26×26×26的vector表示1个word，该vector中每个元素代表1个3字母序列。比如”apple”包括”app”、”ppl”、”ple”。</p><p><img src="https://pic4.zhimg.com/80/v2-828e73ac8019838e7667eeb033859d40_720w.png" alt="img"></p><p>使用FNN实现Slot Filling时会存在一个问题：假如有2个句子“Arrive Taipei on November 2nd”和“Leave Taipei on November 2nd”，在处理这2个句子时FNN会先处理“arrive”和“leave”这2个词汇然后再处理“Taipei”。<strong>这时FNN没有办法区分出“Taipei”是出发地还是目的地，而我们希望算法在处理序列时是有“记忆力”的（即在处理“Taipei”时，它还记得“Leave”或“Arrive”）</strong>，于是RNN诞生了。</p><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><p>如下图所示，<strong>将每1个隐藏层的输出保存在memory中，网络不仅考虑了input，还要考虑memory中的数据</strong>（merory中的数据是需要有初值的，比如0）。</p><p><img src="https://pic1.zhimg.com/80/v2-f151d27c684e3ae8e60cb9d55d4a78fe_720w.png" alt="img"></p><p>因为RNN会考虑memory中存储的临时值，而不同输入产生的临时值不一定相同，所以<strong>改变输入序列中元素的顺序会导致最终输出结果的改变</strong>（Changing the sequence order will change the output）。</p><h2 id="Slot-Filling-with-RNN"><a href="#Slot-Filling-with-RNN" class="headerlink" title="Slot Filling with RNN"></a>Slot Filling with RNN</h2><p>如下图所示，以“Arrive Taipei on November 2nd” 这个word sequence为例，将“Arrive”的vector$x^1$输入到RNN，隐藏层生成$a^1$，根据$a^1$生成$y^1$，表示“arrive”属于每个slot的概率，其中$a^1$会被存储到memory中；将“Taipei”的vector$x^2$输入到RNN，此时隐藏层同时考虑$x^2$和memory中的$a^1$生成$a^2$，根据$a^2$生成$y^2$，表示“Taipei”属于某个slot的概率，此时再把$a^2$存到memory中；以此类推根据$x_3$和$a_2$生成$a_3$进而得到$y^3$……</p><p><img src="https://pic2.zhimg.com/80/v2-e16243c99c04f52df6f52ae560fdfb03_720w.png" alt="img"></p><h2 id="RNN的变体"><a href="#RNN的变体" class="headerlink" title="RNN的变体"></a>RNN的变体</h2><h3 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h3><p>RNN也有不同的变形。<strong>Elman Network是把隐藏层的输出存到memory中，而Jordan Network是把输出层的输出保存到memory中</strong>。由于隐藏层没有明确的训练目标，而整个NN具有明确的目标，因此Jordan Network的表现会更好一些。</p><p><img src="https://pic4.zhimg.com/80/v2-7df1a297e9879628ea03333d79b06d22_720w.png" alt="img"></p><h3 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h3><p>如下图所示，<strong>RNN可以是双向的</strong>。训练2个方向的RNN，1个从前往后读取序列，1个从后往前读取序列，然后使用2个RNN的隐藏层得到最后的输出层。这样的好处是，<strong>输出层的感受野更大</strong>，因为RNN在得到$y^{t+1}$的时候，它不只看了从句首$x^1$开始到$x^{t+1}$的数据，还看了从句尾$x^{n}$一直到$x^{t+1}$的输入，这就相当于<strong>RNN是在看过整个句子之后才计算每个word属于哪个slot的概率</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-a591f7a3eda6fd1328ae36273451bff2_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Slot-Filling&quot;&gt;&lt;a href=&quot;#Slot-Filling&quot; class=&quot;headerlink&quot; title=&quot;Slot Filling&quot;&gt;&lt;/a&gt;Slot Filling&lt;/h2&gt;&lt;p&gt;比如在订票系统中，输入“Arrive Taipei on N
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-8.2图神经网络(Spatial-based Convolution)</title>
    <link href="https://chouxianyu.github.io/2021/04/05/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-8-2%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Spatial-based-Convolution/"/>
    <id>https://chouxianyu.github.io/2021/04/05/李宏毅机器学习课程笔记-8-2图神经网络-Spatial-based-Convolution/</id>
    <published>2021-04-05T12:06:39.000Z</published>
    <updated>2021-04-05T12:07:56.753Z</updated>
    
    <content type="html"><![CDATA[<h2 id="术语（Terminology）"><a href="#术语（Terminology）" class="headerlink" title="术语（Terminology）"></a>术语（Terminology）</h2><ul><li><p><strong>Aggregation</strong></p><p>  Aggregation是Convolution在GNN中的推广。Aggregation就是在某一个layer中用某node及其neighbor的feature得到下一个layer中该node的feature。</p></li><li><p><strong>Readout</strong></p><p>  Readout有点像是全连接在GNN中的推广。Readout就是汇总整个图的信息，最终得到一个特征来表示这整个图（Graph Representation）。</p></li></ul><h2 id="NN4G-Neural-Network-for-Graph"><a href="#NN4G-Neural-Network-for-Graph" class="headerlink" title="NN4G(Neural Network for Graph)"></a>NN4G(Neural Network for Graph)</h2><p>论文链接：<a href="https://ieeexplore.ieee.org/document/4773279" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/4773279</a></p><ul><li><p>输入层</p><p>  假如是一个化学分子，输入层的图中的结点就是一个原子。不同原子有不同的特征， 其特征可以是任何和原子相关的化学特征，所以需要<strong>embedding</strong>（将高维特征映射到低维特征），做完embedding也就得到了隐藏层$h^0$。</p></li><li><p>隐藏层$h^0$</p><p>  如何做embedding呢？让原特征乘以embedding matrix就得到隐藏层$h^0$。如下图所示，以1个结点为例，输入层中结点$v_3$的特征是$x_3$，该结点embedding时的计算式为$h^0_3=\bar w_0\cdot x_3$。<strong>embedding</strong>后就得到了隐藏层$h^0$，然后再对隐藏层$h^0$进行<strong>Aggregation</strong>就得到了隐藏层$h^1$。</p><p>  <img src="https://pic2.zhimg.com/80/v2-cff58a466296d02f900be31d89fca62a_720w.png" alt="img"></p></li><li><p>隐藏层$h^1$</p><p>  如何做Aggregation呢？如下图所示，以1个结点为例，在隐藏层$h^0$中，结点$h^0_3$和$h^0_0,h^0_2,h^0_4$3个结点相邻，则Aggregation时计算式为$h^1_3=\hat w_{1,0}(h^0_0+h^0_2+h^0_4)+\bar w_1\cdot x_3$。<strong>经过多次Aggregation，最后需要Readout</strong>。</p><p>  <img src="https://pic3.zhimg.com/80/v2-35dabe169d54b8815ad452c1c105cd75_720w.png" alt="img"></p></li><li><p>Readout</p><p>  如何做Readout呢？如下图所示，假设有3个隐藏层，那Readout的计算式为$y=MEAN(h^0)+MEAN(h^1)+MEAN(h^2)$。</p><p>  <img src="https://pic4.zhimg.com/80/v2-99a44d03b0f47295abfe6d62eaa77bde_720w.png" alt="img"></p></li></ul><h2 id="DCNN-Diffusion-Convolution-Neural-Network"><a href="#DCNN-Diffusion-Convolution-Neural-Network" class="headerlink" title="DCNN(Diffusion-Convolution Neural Network)"></a>DCNN(Diffusion-Convolution Neural Network)</h2><p>论文链接：<a href="https://arxiv.org/abs/1511.02136" target="_blank" rel="noopener">https://arxiv.org/abs/1511.02136</a></p><ul><li><p>输入层</p><p>  假如我们有1个和上例中（NN4G）一样的输入图。</p></li><li><p>隐藏层$h^0$</p><p>  如下图所示，从输入层到隐藏层$h^0$的计算式为$h^0_3=w^0_3MEAN(d(3,\cdot)=1)$，其中$d(3,\cdot)=1$表示所有与结点$x_3$距离为1的输入层结点的特征。</p><p>  <img src="https://pic4.zhimg.com/80/v2-97b6883501dbd6fca82612e42f9e02b4_720w.png" alt="img"></p></li><li><p>隐藏层$h^1$</p><p>  如下图所示，从隐藏层$h^0$到隐藏层$h^1$的计算式为$h^1_3=w^1_3MEAN(d(3,\cdot)=2)$，其中$d(3,\cdot)=2$表示所有与结点$x_3$距离为2的输入层结点的特征。</p><p>  <img src="https://pic2.zhimg.com/80/v2-8282b7f79e512aa9bc1a1b3a18a295aa_720w.png" alt="img"></p><p>  以此类推，<strong>叠加k个隐藏层后就可以获取各结点k范围内的信息</strong>。如下图所示，令1个隐藏层中多个结点的特征形成矩阵（1行是1个结点的特征），多个隐藏层的特征就形成多个通道$H^0,H^1,\dots,H^k$。</p><p>  <img src="https://pic2.zhimg.com/80/v2-ec751b3608ee583f1f6a4cd320ff2f0b_720w.png" alt="img"></p></li><li><p>Node features</p><p>  如何表达整个图的特征呢？如下图所示，将每个通道的特征flatten，然后再乘以参数$w$得到$y_1$即可。</p><p>  <img src="https://pic1.zhimg.com/80/v2-c226fe1e317a545f5c9c5b2c114a0a7b_720w.png" alt="img"></p><p>  也有其它做法，ICLR2018中<a href="https://arxiv.org/abs/1707.01926" target="_blank" rel="noopener">DGC(Diffusion Graph Convolution)</a>不是flatten，而是相加，如下图所示。</p><p>  <img src="https://pic4.zhimg.com/80/v2-7110f786304c6ed60fd4071e00e23047_720w.png" alt="img"></p></li></ul><h2 id="MoNET-Mixture-Model-Networks"><a href="#MoNET-Mixture-Model-Networks" class="headerlink" title="MoNET(Mixture Model Networks)"></a>MoNET(Mixture Model Networks)</h2><p>NN4G、DCNN都是将邻居结点的特征直接相加，并没有考虑各个邻居结点特征的重要性，而MoNET考虑了这个问题。</p><p>论文链接：<a href="https://arxiv.org/abs/1611.08402" target="_blank" rel="noopener">https://arxiv.org/abs/1611.08402</a></p><p>MoNET定义了结点距离的概念，基于结点距离表示各个邻居结点特征的重要性然后对各个邻居结点进行<strong>加权</strong>求和，而不是简单地取均值或求和。</p><p>如下图所示，假如我们有和上例一样的输入图，隐藏层$h^0$中结点$v_3$的特征为$h^0_3$，结点$v_3$和结点$v_0$的距离为$u_{3,0}$。</p><p>定义结点$x,y$的距离$u(x,y)=(\frac{1}{\sqrt{deg(x)}},\frac{1}{\sqrt{deg(y)}})^T$，其中$deg(x)$表示结点$x$的度（degree，度是连接到每个节点的边的数量）。</p><p><img src="https://pic1.zhimg.com/80/v2-454c766a5484d4fe7f79178e679fab55_720w.png" alt="img"></p><h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><p><strong>SA</strong>mple and aggre<strong>G</strong>at<strong>E</strong>(GraphSAGE)，在transductive和inductive setting上都能work。</p><p>论文链接：<a href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener">https://arxiv.org/abs/1706.02216</a></p><p>GraphSAGE的Aggregation除了mean，还有max pooling和LSTM。LSTM用来处理序列数据，但图中结点的邻居并没有序列关系，但如果每次在邻居中随机取样出不同顺序，那也许可以忽略顺序学习到顺序无关的信息。</p><h2 id="GAT-Graph-Attention-Networks"><a href="#GAT-Graph-Attention-Networks" class="headerlink" title="GAT(Graph Attention Networks)"></a>GAT(Graph Attention Networks)</h2><p>论文链接：<a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">https://arxiv.org/abs/1710.10903</a></p><p>GAT不只是做加权求和（weighted sum），而其中的weight是通过学习得到的，方法就是对邻居做attention。</p><p>假如我们有1个和上例中（NN4G）一样的输入图。在做aggregation时，我们通过函数$f$计算各个邻居结点$v_0,v_2,v_4$对结点$v_3$的重要性，然后做加权求和。</p><p><img src="https://pic4.zhimg.com/80/v2-d293babed38f57a74e8740be39dd69d8_720w.png" alt="img"></p><h2 id="GIN-Graph-Isomorphism-Network"><a href="#GIN-Graph-Isomorphism-Network" class="headerlink" title="GIN(Graph Isomorphism Network)"></a>GIN(Graph Isomorphism Network)</h2><p>这篇论文偏理论，证明出有些方法是work的，有些是不会work的。</p><p>比如提取特征时不要用mean或max（在一些情况下会fail），要用sum，如下图所示。</p><p><img src="https://pic4.zhimg.com/80/v2-ad2c1fd4a3043d56cd5420c03d3cfef7_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;术语（Terminology）&quot;&gt;&lt;a href=&quot;#术语（Terminology）&quot; class=&quot;headerlink&quot; title=&quot;术语（Terminology）&quot;&gt;&lt;/a&gt;术语（Terminology）&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="图神经网络" scheme="https://chouxianyu.github.io/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-7.4基于CNN和PyTorch的食物图片分类</title>
    <link href="https://chouxianyu.github.io/2021/04/05/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7-4%E5%9F%BA%E4%BA%8ECNN%E5%92%8CPyTorch%E7%9A%84%E9%A3%9F%E7%89%A9%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    <id>https://chouxianyu.github.io/2021/04/05/李宏毅机器学习课程笔记-7-4基于CNN和PyTorch的食物图片分类/</id>
    <published>2021-04-05T11:42:47.000Z</published>
    <updated>2021-04-13T08:52:03.190Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework3的记录。</p><p>全部课程PPT、数据和代码下载链接：</p><p>链接：<a href="https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg" target="_blank" rel="noopener">https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg</a> 提取码：tpmc</p><p>代码仓库：<a href="https://github.com/chouxianyu/LHY_ML2020_Codes" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes</a></p><ul><li><p>任务描述</p><p>  通过CNN实现食物图片分类，数据集已提供</p></li><li><p>数据集描述</p><p>  11个图片类别，训练集中有9866张图片，验证集中有3430张图片，测试集中有3347张图片。</p><p>  训练集和验证集中图片命名格式为<code>类别_编号.jpg</code>，编号不重要。</p></li><li><p>代码</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.activation <span class="keyword">import</span> ReLU</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.batchnorm <span class="keyword">import</span> BatchNorm2d</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.pooling <span class="keyword">import</span> MaxPool1d, MaxPool2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""加载数据"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_files</span><span class="params">(dir_path)</span>:</span> <span class="comment"># 读取文件夹中的所有图片</span></span><br><span class="line">    filenames = sorted(os.listdir(dir_path))</span><br><span class="line">    x = np.zeros((len(filenames), <span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>), dtype=np.uint8) <span class="comment"># (N,H,W,C)</span></span><br><span class="line">    y = np.zeros((len(filenames)), dtype=np.uint8)</span><br><span class="line">    <span class="keyword">for</span> i, filename <span class="keyword">in</span> enumerate(filenames):</span><br><span class="line">        img = cv2.imread(os.path.join(dir_path, filename))</span><br><span class="line">        x[i, : , :] = cv2.resize(img, (<span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">        y[i] = int(filename.split(<span class="string">"_"</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line">train_x, train_y = read_files(<span class="string">"./data/training"</span>)</span><br><span class="line">val_x, val_y = read_files(<span class="string">"./data/validation"</span>)</span><br><span class="line">print(<span class="string">"Data Loaded"</span>)</span><br><span class="line">print(<span class="string">"Size of training data : %d"</span> % len(train_x))</span><br><span class="line">print(<span class="string">"Size of validation data : %d"</span> % len(val_x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""数据变换（训练时进行数据增强）"""</span></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(mode=<span class="keyword">None</span>), <span class="comment"># 将图片格式转换成PIL格式</span></span><br><span class="line">    transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>), <span class="comment"># 随机水平翻转</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">15</span>), <span class="comment"># 随机旋转图片</span></span><br><span class="line">    transforms.ToTensor(), <span class="comment"># 转换成torch中的tensor并将值normalize到[0.0,1.0]</span></span><br><span class="line">])</span><br><span class="line">val_transform = transforms.Compose([</span><br><span class="line">    transforms.ToPILImage(),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""加载数据"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImgDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y=None, transform=None)</span>:</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.y = torch.LongTensor(y)</span><br><span class="line">        self.transform = transform</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.x)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        X = self.x[index]</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            X = self.transform(X)</span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            Y = self.y[index]</span><br><span class="line">            <span class="keyword">return</span> X, Y</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">train_set = ImgDataset(train_x, train_y, train_transform)</span><br><span class="line">val_set = ImgDataset(val_x, val_y, val_transform)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""定义模型"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        <span class="comment"># torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)</span></span><br><span class="line">        <span class="comment"># torch.nn.MaxPool2d(kernel_size, stride, padding)</span></span><br><span class="line">        self.cnn = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [64, 128, 128]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [64, 64, 64]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [128, 64, 64]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [128, 32, 32]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [256, 32, 32]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [256, 16, 16]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [512, 16, 16]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [512, 8, 8]</span></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># [512, 8, 8]</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>), <span class="comment"># [512, 4, 4]</span></span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">11</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.cnn(x)</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>) <span class="comment"># torch.nn只支持mini-batches而不支持单个sample，第1个维度是mini-batch中图片（特征）的索引，即将每张图片都展开</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""训练并测试模型"""</span></span><br><span class="line">model = Model() <span class="comment"># model = Model().cuda()</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    epoch_start_time = time.time()</span><br><span class="line">    train_acc = <span class="number">0.0</span></span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    val_acc = <span class="number">0.0</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># batch_loss.backward()的gradient会累加，所以每个batch都需要置零</span></span><br><span class="line">        pred = model(data[<span class="number">0</span>]) <span class="comment"># pred = model(data[0].cuda())</span></span><br><span class="line">        batch_loss = criterion(pred, data[<span class="number">1</span>]) <span class="comment"># batch_loss = criterion(pred, data[1].cuda())</span></span><br><span class="line">        batch_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        train_acc += np.sum(np.argmax(pred.detach().numpy(), axis=<span class="number">1</span>) == data[<span class="number">1</span>].numpy())</span><br><span class="line">        <span class="comment"># train_acc += np.sum(np.argmax(pred.cpu().detach().numpy(), axis=1) == data[1].numpy())</span></span><br><span class="line">        train_loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(val_loader):</span><br><span class="line">            pred = model[data[<span class="number">0</span>]] <span class="comment"># pred = model(data[0].cuda())</span></span><br><span class="line">            batch_loss = criterion(pred, data[<span class="number">1</span>]) <span class="comment"># batch_loss = criterion(pred, data[1].cuda())</span></span><br><span class="line">            val_acc += np.sum(np.argmax(pred.detach().numpy(), axis=<span class="number">1</span>) == data[<span class="number">1</span>].numpy())</span><br><span class="line">            <span class="comment"># val_acc += np.sum(np.argmax(pred.cpu().detach().numpy(), axis=1) == data[1].numpy())</span></span><br><span class="line">            val_loss += batch_loss.item()</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f'</span> % \</span><br><span class="line">            (epoch+<span class="number">1</span>, epochs, time.time()-epoch_start_time, \</span><br><span class="line">             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))</span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework3的记录。&lt;/p&gt;
&lt;p&gt;全部课程PPT、数据和代码下载链接：&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pan.baidu.com/
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
      <category term="图片分类" scheme="https://chouxianyu.github.io/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
      <category term="卷积神经网络" scheme="https://chouxianyu.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch入门：基于LeNet5和CIFAR10的图片分类</title>
    <link href="https://chouxianyu.github.io/2021/04/04/PyTorch%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8ELeNet5%E5%92%8CCIFAR10%E7%9A%84%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    <id>https://chouxianyu.github.io/2021/04/04/PyTorch入门：基于LeNet5和CIFAR10的图片分类/</id>
    <published>2021-04-04T05:45:03.000Z</published>
    <updated>2021-04-04T05:53:56.482Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://www.cnblogs.com/chouxianyu/p/14613460.html" target="_blank" rel="noopener">PyTorch入门：使用PyTorch搭建神经网络LeNet5</a>一文中，我们已经使用PyTorch实现了一个简单的神经网络LeNet5，本文将基于PyTorch使用LeNet5和CIFAR10实现图片分类模型的定义、训练和测试的全过程，代码(有详细注释)如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 构建神经网络模型：将LeNet5模型的输入改为3个通道</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## normalize：torchvision中数据集是元素值在[0,1]范围的PIL图片(C,H,W)，需将其数值范围转换为[-1,1]</span></span><br><span class="line">normalization = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 加载CIFAR10数据集</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>, transform=normalization, download=<span class="keyword">True</span>)</span><br><span class="line">train_set_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">0</span>) <span class="comment"># Windows系统中建议把num_workers设为0</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>, transform=normalization, download=<span class="keyword">True</span>)</span><br><span class="line">test_set_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">False</span>, num_workers=<span class="number">0</span>) <span class="comment"># Windows系统中建议把num_workers设为0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 显示CIFAR10数据集中的一些图片</span></span><br><span class="line"><span class="comment"># def imshow(img):</span></span><br><span class="line"><span class="comment">#     # print(img.size())</span></span><br><span class="line"><span class="comment">#     img = img / 2 + 0.5 # unnormalize: [-1,1] =&gt; [0,1]</span></span><br><span class="line"><span class="comment">#     img = img.numpy()</span></span><br><span class="line"><span class="comment">#     plt.imshow(np.transpose(img, (1, 2, 0))) # PIL的(C,H,W) =&gt; matplotlib的(H,W,C)</span></span><br><span class="line"><span class="comment">#     plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')</span></span><br><span class="line"><span class="comment"># data_iter = iter(train_set_loader)</span></span><br><span class="line"><span class="comment"># images, labels = data_iter.next() # images, labels都是tensor</span></span><br><span class="line"><span class="comment"># # print(images.size())</span></span><br><span class="line"><span class="comment"># # print(labels.size())</span></span><br><span class="line"><span class="comment"># imshow(torchvision.utils.make_grid(images))</span></span><br><span class="line"><span class="comment"># print(' '.join('%s' % classes[labels[j]] for j in range(len(labels))))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义神经网络、损失函数和优化器</span></span><br><span class="line">net = Net()</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(params=net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>) <span class="comment"># SGD with momentum</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练神经网络</span></span><br><span class="line">print(<span class="string">'Training Started'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>): <span class="comment"># 1个epoch会将所有数据训练一次</span></span><br><span class="line">    running_loss = <span class="number">0.0</span> <span class="comment"># 用来在控制台输出loss，以观察训练情况</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(iterable=train_set_loader, start=<span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 获取数据</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line">        <span class="comment"># 清空梯度缓存</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 输出每2000个mini-batch的平均loss</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>: <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            print(<span class="string">'[%3d, %5d] loss: %.3f'</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 保存模型参数</span></span><br><span class="line">torch.save(net.state_dict(), <span class="string">'./data/LeNet5.pt'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 测试模型</span></span><br><span class="line">print(<span class="string">'Testing Started'</span>)</span><br><span class="line">net_new = Net()</span><br><span class="line">net_new.load_state_dict(torch.load(<span class="string">'./data/LeNet5.pt'</span>))</span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_set_loader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        _, predictions = torch.max(net_new(images), <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predictions==labels).sum().item()</span><br><span class="line">print(<span class="string">'Accuracy: %d/%d = %.2f%%'</span> % (correct, total, correct/total*<span class="number">100</span>) )</span><br><span class="line"></span><br><span class="line"><span class="string">"""Explore:</span></span><br><span class="line"><span class="string">使用GPU后会发现速度并没有增加很多，原因是LeNet这个模型非常小。</span></span><br><span class="line"><span class="string">如果将模型宽度增大（增加2个卷积层的卷积核数量），GPU对模型的加速效果会是怎么样的呢？</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在&lt;a href=&quot;https://www.cnblogs.com/chouxianyu/p/14613460.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PyTorch入门：使用PyTorch搭建神经网络LeNet5&lt;/a&gt;一文中，我们已经使
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
      <category term="图片分类" scheme="https://chouxianyu.github.io/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch入门：使用PyTorch搭建神经网络LeNet5</title>
    <link href="https://chouxianyu.github.io/2021/04/03/PyTorch%E5%85%A5%E9%97%A8%EF%BC%9A%E4%BD%BF%E7%94%A8PyTorch%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CLeNet5/"/>
    <id>https://chouxianyu.github.io/2021/04/03/PyTorch入门：使用PyTorch搭建神经网络LeNet5/</id>
    <published>2021-04-03T01:47:13.000Z</published>
    <updated>2021-04-03T04:17:40.842Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在本文中，我们基于PyTorch构建一个简单的神经网络LeNet5。</p><p>在阅读本文之前，建议您了解一些卷积神经网络的前置知识，比如卷积、Max Pooling和全连接层等等，可以看我写的相关文章：<a href="https://zhuanlan.zhihu.com/p/360796545" target="_blank" rel="noopener">李宏毅机器学习课程笔记-7.1CNN入门详解</a>。</p><p>通过阅读本文，您可以学习到如何使用PyTorch构建神经网络LeNet5。</p><h1 id="模型说明"><a href="#模型说明" class="headerlink" title="模型说明"></a>模型说明</h1><p>在本例中，我们使用如下图所示的神经网络模型：LeNet5。</p><p><img src="https://pic1.zhimg.com/80/v2-e3852bfd8a716bda7cddac603628affa_720w.png" alt="img"></p><p>该模型有1个输入层、2个卷积层、2次Max Pooling、2个全连接层和1个输出层。</p><ul><li><p>输入层INPUT</p><p>  1个channel，图片size是32×32。</p></li><li><p>卷积层C1</p><p>  6个channel，特征图的size是28×28，即每个卷积核的size为(5,5)，stride为1。</p></li><li><p>下采样操作S2</p><p>  6个channel，特征图的size是14×14，即Max Pooling窗口size为(2,2)。</p></li><li><p>卷积层C3</p><p>  16个channel，特征图的size是10×10，即每个卷积核的size为(5,5)，stride为1。</p></li><li><p>下采样操作S4</p><p>  16个channel，特征图的size是5×5，即Max Pooling窗口size为(2,2)。</p></li><li><p>全连接层F5</p><p>  120个神经元。</p></li><li><p>全连接层F6</p><p>  84个神经元。</p></li><li><p>输出层OUTPUT</p><p>  10个神经元。</p></li></ul><p>另外，除了输入层和输出层，剩下的卷积层、最大池化操作和全连接层后面都要加上Relu激活函数，下采样操作S4之后需要进行Flatten以和全连接层F5衔接起来。</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet5</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet5, self).__init__()</span><br><span class="line">        <span class="comment"># 卷积层</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        self.fc5 = nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc6 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.OUTPUT = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.max_pool2d(x, (<span class="number">2</span>, <span class="number">2</span>)) <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>) <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>) <span class="comment"># Flatten</span></span><br><span class="line">        x = F.relu(self.fc5(x))</span><br><span class="line">        x = F.relu(self.fc6(x))</span><br><span class="line">        x = self.OUTPUT(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = LeNet5()</span><br><span class="line">output = net(torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"><span class="comment"># print(output)</span></span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</a></p><p>其实本文内容主要是PyTorch的官方教程。</p><p>PyTorch官方教程中代码实现与图片所示的LeNet5不符（PyTorch官方教程代码中是3×3的卷积核，而图片中LeNet5是5×5的卷积核），本文中我是按照图片所示模型结构实现的。</p><p>其实PyTorch开发者和其他开发者也注意到了这一问题，详见：</p><p><a href="https://github.com/pytorch/tutorials/pull/515" target="_blank" rel="noopener">https://github.com/pytorch/tutorials/pull/515</a></p><p><a href="https://github.com/pytorch/tutorials/commit/630802450c13c78f02f744af1c47d1033b6fe206" target="_blank" rel="noopener">https://github.com/pytorch/tutorials/commit/630802450c13c78f02f744af1c47d1033b6fe206</a></p><p><a href="https://github.com/pytorch/tutorials/pull/1257" target="_blank" rel="noopener">https://github.com/pytorch/tutorials/pull/1257</a></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;在本文中，我们基于PyTorch构建一个简单的神经网络LeNet5。&lt;/p&gt;
&lt;p&gt;在阅读本文之前，建议您了解一些卷积神经网络的前置知识，比
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-8.1图神经网络入门</title>
    <link href="https://chouxianyu.github.io/2021/04/02/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-8-1%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/02/李宏毅机器学习课程笔记-8-1图神经网络入门/</id>
    <published>2021-04-01T17:25:04.000Z</published>
    <updated>2021-04-01T17:41:57.916Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GNN的用处"><a href="#GNN的用处" class="headerlink" title="GNN的用处"></a>GNN的用处</h2><h3 id="分类（Classification）"><a href="#分类（Classification）" class="headerlink" title="分类（Classification）"></a>分类（Classification）</h3><p>比如，有很多不同的化学分子，将其表示成图作为输入，用GNN判断其是否会导致突变，这就是一个有监督分类问题。</p><h3 id="生成（Generation）"><a href="#生成（Generation）" class="headerlink" title="生成（Generation）"></a>生成（Generation）</h3><p>比如，我们需要开发针对新冠病毒的新药，我们可以训练出一个可以生成我们想要的分子（输出形式为图）的GNN，然后用这个GNN生成可以对抗病毒的某种化学分子。</p><p>GraphVAE</p><h2 id="为什么要用GNN"><a href="#为什么要用GNN" class="headerlink" title="为什么要用GNN"></a>为什么要用GNN</h2><p>上面列举的分类、生成的例子中输入或输出都是图，所以要用GNN。图不仅包含了各个结点的信息，更重要的是它包含了<strong>各个结点之间的关系</strong>。</p><h2 id="使用GNN时可能遇到的问题"><a href="#使用GNN时可能遇到的问题" class="headerlink" title="使用GNN时可能遇到的问题"></a>使用GNN时可能遇到的问题</h2><ul><li>如何通过结点间的信息帮助模型</li><li>图中可能包含大量结点，数据集非常大</li><li><strong>大量结点都是没有label的</strong></li></ul><h2 id="GNN的两种思路"><a href="#GNN的两种思路" class="headerlink" title="GNN的两种思路"></a>GNN的两种思路</h2><p>很直观地可以想到，一个结点和它的相邻结点肯定有某些特定的关系，比如说相似（近朱者赤近墨者黑）、相反、合作等等。在多数情况下，数据集unlabeled node都是远多于labeled node的，我们想要得到一个较好的模型，那我们要如何利用仅有的少量labeled node以及它和邻居间的关系呢？结合CNN，我们要如何将卷积这个操作推广到GNN模型中，用卷积将结点嵌入到某个特征空间中呢？有两种方法</p><ul><li><p><strong>Spatial-based Convolution</strong></p><p>  Generalize the concept of convolution(corelation) to graph.</p></li><li><p><strong>Spectral-based Convolution</strong></p><p>  Back to the definition of convolution in signal processing.</p></li></ul><h2 id="GNN导图"><a href="#GNN导图" class="headerlink" title="GNN导图"></a>GNN导图</h2><p><img src="https://pic4.zhimg.com/80/v2-7e0da7ed579a0d8a74bb7d873d3449ae_720w.png" alt="img"></p><h2 id="任务和数据集"><a href="#任务和数据集" class="headerlink" title="任务和数据集"></a>任务和数据集</h2><h3 id="常见任务"><a href="#常见任务" class="headerlink" title="常见任务"></a>常见任务</h3><ul><li>Classification</li><li>Regression</li><li>Graph Classification</li><li>Graph Representation Learning</li><li>Link Prediction</li></ul><h3 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h3><ul><li>CORA: citation network, 2.7k nodes and 5.4k links</li><li>TU-MUTAG: 188 molecules with 18 nodes on average</li></ul><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p><a href="https://www.dgl.ai/" target="_blank" rel="noopener">https://www.dgl.ai/</a></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;GNN的用处&quot;&gt;&lt;a href=&quot;#GNN的用处&quot; class=&quot;headerlink&quot; title=&quot;GNN的用处&quot;&gt;&lt;/a&gt;GNN的用处&lt;/h2&gt;&lt;h3 id=&quot;分类（Classification）&quot;&gt;&lt;a href=&quot;#分类（Classification）&quot;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="图神经网络" scheme="https://chouxianyu.github.io/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-7.3CNN应用案例</title>
    <link href="https://chouxianyu.github.io/2021/04/01/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7-3CNN%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
    <id>https://chouxianyu.github.io/2021/04/01/李宏毅机器学习课程笔记-7-3CNN应用案例/</id>
    <published>2021-04-01T04:15:26.000Z</published>
    <updated>2021-04-01T04:18:04.287Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deep-Dream"><a href="#Deep-Dream" class="headerlink" title="Deep Dream"></a>Deep Dream</h2><p><a href="https://deepdreamgenerator.com/" target="_blank" rel="noopener">Deep Dream</a>是这样的：如果给机器一张图片$x$，Deep Dream会把机器看到的内容加到图片$x$中得到$x’$。那如何实现呢？</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221135654357-945005692.png"></p><p>如上图所示，将图片$x$输入到CNN中，然后取出CNN中某一层$L$（可以是卷积、池化阶段的隐藏层，也可以是FNN中的隐藏层）的输出$O$，然后将$L$中的正值调大、负值调小得到一个新的输出$O’$，然后通过梯度下降找到一张新的图片$x’$使层$L$的输出为$O’$，这个$x’$就是我们要的结果。直观理解的话，也就是<strong>让CNN夸大它所看到的内容</strong>。</p><p>然后就得到了如下结果……（看到的时候我惊了，真是十分哇塞）</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221135818664-1882676933.png"></p><h2 id="Deep-Style"><a href="#Deep-Style" class="headerlink" title="Deep Style"></a>Deep Style</h2><p><a href="https://dreamscopeapp.com/" target="_blank" rel="noopener">Deep Style</a>是这样的：如果给机器一张图片$x$和$y$，Deep Style可以把图片$y$的风格加到图片$x$上，也就是<strong>风格迁移</strong>。</p><p>那如何实现呢？论文：<a href="https://arxiv.org/abs/1508.06576" target="_blank" rel="noopener">A Neural Algorithm of Artistic Style</a>。</p><ol><li>把图片$x$传入CNN并得到输出，然后其输出作为图片$x$的内容$c_x$（<strong>content</strong>）；</li><li>把图片$y$传入CNN并得到输出，但不是考虑输出的值是什么，而是考虑输出层中各个filter输出之间的相关性（corelation）作为图片$y$的风格$s_y$（<strong>style</strong>）；</li><li>最后基于同一个CNN找到图片$z$，图片$z$传入CNN后得到的内容$c_z$像$c_x$、风格$s_z$像$s_y$。</li></ol><p>如下图所示</p><p><img src="https://pic1.zhimg.com/80/v2-fb5dd2b4083a6b896f57bb7e7a231c50_720w.png" alt="img"></p><h2 id="围棋"><a href="#围棋" class="headerlink" title="围棋"></a>围棋</h2><p>CNN不单单可以用在图像上，还可以用在其它方面，比如下围棋。</p><p>在下围棋这件事上，其实FNN就可以（输入和输出都是19×19=361的vector），但CNN的效果更好。当然还可以用强化学习。</p><p>为什么CNN可以用来下围棋呢？因为围棋具有图像的3个性质，不过AlphaGo并没有用Max Pooling因为它不需要。</p><h2 id="语音"><a href="#语音" class="headerlink" title="语音"></a>语音</h2><p>如下图所示，用<strong>语谱图（Spectrogram）</strong>表示语音。</p><p>语谱图的x轴是时间，y轴是频率，z轴是幅度。幅度用颜色表示（比如亮色表示高、暗色表示低）。</p><p>在语谱图中，CNN的卷积核往往只在y轴方向上移动，这样可以消除男生女生声音频率的差异；卷积核往往不在x轴上移动，因为时间域一般是在后面用LSTM等等进行处理，如下图所示。</p><p><img src="https://pic1.zhimg.com/80/v2-65e8d3ffdc43a45f4f9ca39ea16a9805_720w.png" alt="img"></p><h2 id="文本"><a href="#文本" class="headerlink" title="文本"></a>文本</h2><p>CNN也可以用在文字处理上，比如文本情感分析。具体不再讲，可以看李宏毅老师的<a href="https://www.bilibili.com/video/BV1JE411g7XF?p=17" target="_blank" rel="noopener">视频</a>。</p><p><img src="https://pic4.zhimg.com/80/v2-fec6cc1a30d3f436c71587f91f8f82ee_720w.png" alt="img"></p><h2 id="图片生成"><a href="#图片生成" class="headerlink" title="图片生成"></a>图片生成</h2><p>Deep Dream的方法还是不能画出图片，不过也有其它较为成功的方法，如下图所示。</p><p><img src="https://pic2.zhimg.com/80/v2-dc95952a08fcb965d30784de27789aef_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Deep-Dream&quot;&gt;&lt;a href=&quot;#Deep-Dream&quot; class=&quot;headerlink&quot; title=&quot;Deep Dream&quot;&gt;&lt;/a&gt;Deep Dream&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://deepdreamgenerator.co
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://chouxianyu.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-7.2CNN学到了什么</title>
    <link href="https://chouxianyu.github.io/2021/03/31/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7-2CNN%E5%AD%A6%E5%88%B0%E4%BA%86%E4%BB%80%E4%B9%88/"/>
    <id>https://chouxianyu.github.io/2021/03/31/李宏毅机器学习课程笔记-7-2CNN学到了什么/</id>
    <published>2021-03-31T05:36:40.000Z</published>
    <updated>2021-03-31T05:37:13.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="卷积核学到了什么"><a href="#卷积核学到了什么" class="headerlink" title="卷积核学到了什么"></a>卷积核学到了什么</h2><p>如果想知道下图中第1个卷积层中的每个卷积核的功能，因为它参数比较少而且其输入是原图片，所以我们直接结合原图片观察卷积核的参数就可以知道该卷积核的功能。</p><p><img src="https://pic2.zhimg.com/80/v2-cb97966c67656a905b06956b7925f645_720w.png" alt="img" style="zoom: 80%;"></p><p>如上图所示，CNN中第2个卷积层的输入不是直观的图片而且其卷积核的感受野比第1个卷积层中卷积核的感受野更大，因此我们无法通过观察卷积核参数了解卷积核学习到了什么。</p><p>上图中CNN第2个卷积层中有50个卷积核，每个卷积核的输出都是一个大小为11×11的矩阵。</p><p>现在定义函数$a^k=\sum_{i=1}^{11}\sum_{j=1}^{11}a^k_{ij}$来衡量某卷积核被“激活”的程度，即将卷积核所输出矩阵的元素之和作为其被“激活”程度，被“激活”程度指CNN输入与该卷积核有多匹配。</p><p>接下来，通过梯度下降求得$x^<em>=arg\ max_x\ a^k$，即找到最能“激活”卷积核的输入图片$x^</em>$，然后将其可视化希求反映该卷积核学习到的内容。</p><p>上图左下角可视化了最能“激活”第2个卷积层中某12个卷积核的12张图片，可以看出各卷积核适用于检测<strong>小的纹理</strong>（这个案例是数字识别）。</p><h2 id="全连接层学到了什么"><a href="#全连接层学到了什么" class="headerlink" title="全连接层学到了什么"></a>全连接层学到了什么</h2><p>如下图所示，我们同样可以用梯度下降找到最能“激活”全连接层中某个神经元的输入，将该输入其可视化，以此观察全连接层学习到了什么。和卷积核不同，卷积核学习到的是<strong>较小的pattern</strong>，全连接层中的神经元学习到的是<strong>尺寸较大的pattern</strong>。</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221130655109-1379139074.png" style="zoom: 50%;"></p><h2 id="输出层学到了什么"><a href="#输出层学到了什么" class="headerlink" title="输出层学到了什么"></a>输出层学到了什么</h2><p>如下图所示，我们同样可以用梯度下降找到最能“激活”输出层中某个神经元的输入，将该输入其可视化，以此观察输出层学习到了什么。在想象中这些输入应该是对应的数字，然而并不是（有人说眯着眼马马虎虎可以从中看到数字）。</p><p>在其它的案例中也有这样的情况，即<strong>机器学习到的内容和人类所理解的内容是不同的</strong>。<a href="https://www.youtube.com/watch?v=M2IebCN9Ht4" target="_blank" rel="noopener">这个视频</a>里讲了相关例子。</p><p>但这个方法(通过“激活”卷积核反映学习到的内容)确实是有效的，那如何改进呢？我们可以对输入做一些限制（constraint）。</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221131204130-1053266641.png" style="zoom: 50%;"></p><p>在该例中数字图片中底色是黑色，前景色（数字）是白色。</p><p>如下图所示，简单考虑的话，我们可以添加图片中大部分像素是黑色的限制（值接近0，也就是L1正则化），然后将求得的输入可视化。</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221133929054-200979562.png" style="zoom:50%;"></p><p>当然，我们可以用更好的方法（比如说更好的限制）找到人类更容易理解的输入……这也是Deep Dream的精神。</p><h2 id="神经网络可视化"><a href="#神经网络可视化" class="headerlink" title="神经网络可视化"></a>神经网络可视化</h2><p><img src="https://pic4.zhimg.com/80/v2-59efdfca9bf326af0199db3ece69dd28_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;卷积核学到了什么&quot;&gt;&lt;a href=&quot;#卷积核学到了什么&quot; class=&quot;headerlink&quot; title=&quot;卷积核学到了什么&quot;&gt;&lt;/a&gt;卷积核学到了什么&lt;/h2&gt;&lt;p&gt;如果想知道下图中第1个卷积层中的每个卷积核的功能，因为它参数比较少而且其输入是原图片，所以我
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://chouxianyu.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-7.1CNN入门详解</title>
    <link href="https://chouxianyu.github.io/2021/03/29/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7-1CNN%E5%85%A5%E9%97%A8%E8%AF%A6%E8%A7%A3/"/>
    <id>https://chouxianyu.github.io/2021/03/29/李宏毅机器学习课程笔记-7-1CNN入门详解/</id>
    <published>2021-03-29T09:09:58.000Z</published>
    <updated>2021-03-29T10:54:37.445Z</updated>
    
    <content type="html"><![CDATA[<p>卷积神经网络（CNN）常常被用来做图像处理，当然也可以用一般的神经网络，那它们各自有什么优缺点呢？</p><h2 id="FNN用于图片处理的缺点"><a href="#FNN用于图片处理的缺点" class="headerlink" title="FNN用于图片处理的缺点"></a>FNN用于图片处理的缺点</h2><p>使用一般的全连接前馈神经网络（FNN）处理图片时的缺点：</p><ul><li><p>需要很多的参数</p><p>  假设有一张尺寸100×100的图片（尺寸已经算很小了），那输入层就有100×100×3=30K个像素，假设第一个隐藏层有1K个神经元（一个神经元包含30K个参数），这就已经需要30M个参数了……</p></li><li><p>该架构中每个神经元就是一个分类器，这是没必要的</p><p>  第一个隐藏层作为最基础的pattern分类器（比如判断有无绿色、边缘等），第二个隐藏层基于第一个隐藏层继续做pattern分类（比如木头、肉类），以此类推……</p></li></ul><p>按照人类的直观理解，我们不是像全连接神经网络一样去处理图片的。具体来看，有哪些方面呢？</p><h2 id="图片的一些性质"><a href="#图片的一些性质" class="headerlink" title="图片的一些性质"></a>图片的一些性质</h2><p>结合全连接前馈神经网络的缺点和人类对图片的直观理解，可以得到下述图片的3个性质。</p><h3 id="性质1：Some-patterns-are-much-smaller-than-the-whole-image"><a href="#性质1：Some-patterns-are-much-smaller-than-the-whole-image" class="headerlink" title="性质1：Some patterns are much smaller than the whole image."></a>性质1：Some patterns are much smaller than the whole image.</h3><p>在识别某个模式（pattern）时，一个神经元并不需要图片的所有像素点。对于一张人类全身照的图片，我们只需要看到头部而非整张图片就可以判断它是一个人脸。所以我们应该是可以用少量参数去识别这些pattern的。</p><p><img src="https://pic4.zhimg.com/80/v2-753ba3ca45cd6cc08d8a8d837d2ec425_720w.jpeg" alt="img" style="zoom:80%;"></p><h3 id="性质2：The-same-patterns-appear-in-different-regions"><a href="#性质2：The-same-patterns-appear-in-different-regions" class="headerlink" title="性质2：The same patterns appear in different regions."></a>性质2：The same patterns appear in different regions.</h3><p>比如说人脸可以在图片的中间区域，也可以在图片的某个角落区域。所以识别不同区域中的相同pattern的多个分类器（或detector）应该用同一组参数或者共享参数。</p><p><img src="https://pic1.zhimg.com/80/v2-cb9dfd6b582245692a8fe50979f5f77d_720w.png" alt="img" style="zoom:80%;"></p><h3 id="性质3：Subsampling-the-pixels-will-not-change-the-object"><a href="#性质3：Subsampling-the-pixels-will-not-change-the-object" class="headerlink" title="性质3：Subsampling the pixels will not change the object"></a>性质3：Subsampling the pixels will not change the object</h3><p>将图片缩小/下采样，并不会影响我们理解图片。所以我们可以通过将图片变小，进而用更少的参数处理图片。</p><p><img src="https://pic2.zhimg.com/80/v2-ee9888d41dfe87f1d8715949676ca025_720w.png" alt="img" style="zoom:80%;"></p><h2 id="CNN架构说明"><a href="#CNN架构说明" class="headerlink" title="CNN架构说明"></a>CNN架构说明</h2><p>2014年在ECCV上提出，针对上述的图片的3个性质，确定了CNN的架构如下。</p><p><img src="https://pic2.zhimg.com/80/v2-ea984acebb9fe399847d16b36d6fc559_720w.png" alt="img" style="zoom:80%;"></p><p>如上图所示，图片经过卷积层然后进行最大池化（max pooling），这个步骤可以进行多次；然后将数据展开（Flatten），然后将数据传进全连接前馈网络得到最后的图片分类结果。</p><h2 id="CNN架构作用探析"><a href="#CNN架构作用探析" class="headerlink" title="CNN架构作用探析"></a>CNN架构作用探析</h2><p><img src="https://pic1.zhimg.com/80/v2-544d4e0360461bfebe9624b4b20fedb5_720w.png" alt="img" style="zoom:80%;"></p><p>如上图所示，卷积是针对了图片的性质1和性质2，最大池化是针对了图片的性质3。</p><h2 id="卷积-Convolution-★"><a href="#卷积-Convolution-★" class="headerlink" title="卷积(Convolution) ★"></a>卷积(Convolution) ★</h2><p>假设有一张6×6的二值图，即一个6×6的矩阵。</p><h3 id="卷积核（Filter）"><a href="#卷积核（Filter）" class="headerlink" title="卷积核（Filter）"></a>卷积核（Filter）</h3><p>神经元就是一个计算/函数，卷积核其实就是神经元。</p><p>如下图所示，1个卷积层可以有多个卷积核，矩阵里元素的值就是需要通过学习得到的参数。</p><p>因为这里的输入是一个矩阵，所以卷积核也是1个矩阵（卷积核的通道数等于输入的通道数）。</p><p>假设卷积核大小是3×3，这对应了图片的性质1，即用小的卷积核识别一个小的pattern。</p><p><img src="https://pic1.zhimg.com/80/v2-dd755bcb25fdadf2bc819d96e03879f8_720w.png" alt="img" style="zoom:80%;"></p><h3 id="怎么做卷积"><a href="#怎么做卷积" class="headerlink" title="怎么做卷积"></a>怎么做卷积</h3><p>如下图所示</p><p><img src="https://pic3.zhimg.com/80/v2-be01890275fa85d7b665e5cc70131f30_720w.png" alt="img" style="zoom:80%;"></p><ul><li><p>卷积区域</p><p>  根据该卷积核的大小（以3×3为例），选择图片中相同大小的区域进行卷积。</p></li><li><p>卷积的计算方法</p><p>  从图片中扫描得到的3×3矩阵和卷积核的3×3矩阵，这2个矩阵相同位置的元素相乘可以得到9个值并求和（也就是内积）得到1个值，这就是1次卷积操作。</p></li><li><p>卷积顺序和方向</p><p>  卷积核按照从左到右、从上到下的顺序，从图片左上角开始移动，移动步长（stride）可以设置（以1为例）。在扫描到的每个区域中，都进行1次卷积。1个卷积核移动结束后，则得到1个新的矩阵（大小为4×4），即<strong>1个卷积核的输出是1个矩阵</strong>。</p><p>  卷积层有多个卷积核，每个卷积核都按照该方式进行卷积得到多个矩阵，这些矩阵合起来就形成了1个卷积层的<strong>特征图（Feature Map）</strong>，这个特征图也就是卷积层的输出。</p><p>  <strong>卷积层特征图的通道数等于该卷积层中卷积核的数量，即某卷积层有多少个卷积核，那该卷积层的特征图就有多少个通道</strong>。</p></li></ul><h3 id="卷积的作用"><a href="#卷积的作用" class="headerlink" title="卷积的作用"></a>卷积的作用</h3><p>在上图中，卷积核1的“对角线”的值都是1，就可以识别出图片中哪个区域具有“对角线”。在上图得到的4×4矩阵中，我们可以看到左上角和左下角3个元素的值为3，即有“对角线”。这对应了图片的性质2，我们用1个filter/1组参数识别了不同位置的相同pattern。</p><h3 id="卷积核是矩阵还是张量"><a href="#卷积核是矩阵还是张量" class="headerlink" title="卷积核是矩阵还是张量"></a>卷积核是矩阵还是张量</h3><p>卷积核可以是矩阵，也可以是张量，要根据其输入决定。</p><p><strong>卷积核的通道数等于其所在卷积层输入的通道数（即其所在卷积层前一层的特征图的通道数）</strong>，即单个卷积核会考虑输入的所有通道。</p><p>假如输入是一张RGB图片（大小为3×N×N，即有3个通道、每个通道中矩阵大小为N×N），那卷积核就是1个张量（大小为3×M×M，即有3个通道、每个通道中矩阵大小为M×M），卷积计算方法和上面一样是求内积（两个3×M×M的张量求内积）。</p><h3 id="卷积中如何做梯度下降"><a href="#卷积中如何做梯度下降" class="headerlink" title="卷积中如何做梯度下降"></a>卷积中如何做梯度下降</h3><p>因为一个卷积核要对图片的不同区域进行多次卷积，每次卷积都会有一个梯度，最终把这些梯度取平均值就好了。</p><p>大概这么个意思，实操中这种底层的东西不需要我们实现。</p><h2 id="Convolution-VS-Fully-Connected"><a href="#Convolution-VS-Fully-Connected" class="headerlink" title="Convolution VS Fully Connected"></a>Convolution VS Fully Connected</h2><p>其实，卷积核就是一个神经元，它相当于全连接层中的神经元并没有全连接（没有使用图片的所有像素，当然这样的层也就不能称为全连接层了），这样有两个好处</p><h3 id="模型的参数更少"><a href="#模型的参数更少" class="headerlink" title="模型的参数更少"></a>模型的参数更少</h3><p>如下图所示，因为没有使用所有像素、不是全连接，因此需要的参数更少。</p><p><img src="https://pic2.zhimg.com/80/v2-d386d44d45dc643867b81dc8a4a6a719_720w.png" alt="img" style="zoom: 80%;"></p><h3 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h3><p>如下图所示，因为用同1个卷积核在图片的不同区域做卷积，即用1组参数对不同区域的像素进行计算，得到了多个值、达成了多个神经元的效果，实现了参数共享。比如下图中神经元“3”处理图片第1个像素和神经元“-1”处理图片第2个像素就使用了同一个参数“1”（卷积核的第1个参数）。</p><p>因为实现了参数共享，卷积就比全连接进一步减少了参数量。</p><p><img src="https://pic2.zhimg.com/80/v2-ce7e944ef9280d1bb8505b4fc5f831cf_720w.png" alt="img" style="zoom: 80%;"></p><h2 id="最大池化（Max-Pooling）"><a href="#最大池化（Max-Pooling）" class="headerlink" title="最大池化（Max Pooling）"></a>最大池化（Max Pooling）</h2><p>最大池化是一种下采样（Subsample）,下采样不一定要取最大值，也可以取平均值。</p><h3 id="怎么做最大池化"><a href="#怎么做最大池化" class="headerlink" title="怎么做最大池化"></a>怎么做最大池化</h3><p><img src="https://pic1.zhimg.com/80/v2-b42884461b41f67fa084cc8e2a9d8ac3_720w.png" alt="img" style="zoom: 80%;"></p><p>如上图所示，假如某Max Pooling层的上1层是1个有2个卷积核的卷积层，该卷积层的输出是一个张量（大小为2×4×4，即2个通道、每个通道中矩阵的大小为4×4）。</p><p>对于输入中的每个通道，Max Pooling将4×4的矩阵划分成4个2×2的子矩阵（子矩阵大小可以人为设定），只取出每个子矩阵中的最大值就得到一个2×2的矩阵。又因为该Max Pooling层的输入有2个通道，所以其输出就是一个大小为2×2×2的张量。</p><h3 id="取最大值的话，该怎么微分？"><a href="#取最大值的话，该怎么微分？" class="headerlink" title="取最大值的话，该怎么微分？"></a>取最大值的话，该怎么微分？</h3><p>见笔记“Tips for Training DNN”Maxout部分，Max Pooling和Maxout其实是一样的。</p><h2 id="Flatten-amp-FNN"><a href="#Flatten-amp-FNN" class="headerlink" title="Flatten &amp; FNN"></a>Flatten &amp; FNN</h2><p>经过一系列的卷积和最大池化，将得到的特征图展开排列，作为FNN的输入，最后输出结果。</p><p><img src="https://pic4.zhimg.com/80/v2-591c85b68d94cb41194b8a70303a063c_720w.png" alt="img" style="zoom: 80%;"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;卷积神经网络（CNN）常常被用来做图像处理，当然也可以用一般的神经网络，那它们各自有什么优缺点呢？&lt;/p&gt;
&lt;h2 id=&quot;FNN用于图片处理的缺点&quot;&gt;&lt;a href=&quot;#FNN用于图片处理的缺点&quot; class=&quot;headerlink&quot; title=&quot;FNN用于图片处理的缺
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://chouxianyu.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-4.5逻辑回归Python实战</title>
    <link href="https://chouxianyu.github.io/2021/03/28/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4-5%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92Python%E5%AE%9E%E6%88%98/"/>
    <id>https://chouxianyu.github.io/2021/03/28/李宏毅机器学习课程笔记-4-5逻辑回归Python实战/</id>
    <published>2021-03-28T02:40:20.000Z</published>
    <updated>2021-04-13T08:57:16.104Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework2的记录。</p><p>关注我的公众号：<code>臭咸鱼</code>，回复<code>LHY</code>可获取课程PPT、数据和代码下载链接。</p><p>代码仓库：<a href="https://github.com/chouxianyu/LHY_ML2020_Codes" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes</a></p><ul><li><p>任务描述（Task Description）</p><p>  二分类（Binary Classification）</p><p>  根据个人资料，判断每个人的年收入是否超过50000美元。</p></li><li><p>数据集描述（Dataset Description）</p><ul><li>train.csv</li><li>test_no_label.csv</li><li>x_train.csv</li><li>Y_train.csv</li><li>X_test.csv</li></ul></li><li><p>参考链接</p><p>  <a href="https://colab.research.google.com/drive/1JaMKJU7hvnDoUfZjvUKzm9u-JLeX6B2C" target="_blank" rel="noopener">https://colab.research.google.com/drive/1JaMKJU7hvnDoUfZjvUKzm9u-JLeX6B2C</a></p></li><li><p>代码</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">np.random.seed(<span class="number">0</span>) <span class="comment"># 使每次随机生成的数字相同</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 函数定义</span></span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalize</span><span class="params">(X, train=True, specified_column=None, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalizes specific columns of X.</span></span><br><span class="line">    <span class="comment"># The mean and standard variance of training data will be reused when processing testing data.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#     X: data to be processed</span></span><br><span class="line">    <span class="comment">#     train: 'True' when processing training data, 'False' for testing data</span></span><br><span class="line">    <span class="comment">#     specific_column: indexes of the columns that will be normalized. If 'None', all columns</span></span><br><span class="line">    <span class="comment">#         will be normalized.</span></span><br><span class="line">    <span class="comment">#     X_mean: mean value of training data, used when train = 'False'</span></span><br><span class="line">    <span class="comment">#     X_std: standard deviation of training data, used when train = 'False'</span></span><br><span class="line">    <span class="comment"># Outputs:</span></span><br><span class="line">    <span class="comment">#     X: normalized data</span></span><br><span class="line">    <span class="comment">#     X_mean: computed mean value of training data</span></span><br><span class="line">    <span class="comment">#     X_std: computed standard deviation of training data</span></span><br><span class="line">    <span class="keyword">if</span> specified_column <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        specified_column = np.arange(X.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X[:, specified_column], axis=<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        X_std = np.std(X[:, specified_column], axis=<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    X[:, specified_column] = (X[:, specified_column] - X_mean) / (X_std + <span class="number">1e-8</span>)</span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集划分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_valid_split</span><span class="params">(X, Y, valid_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and validation set.</span></span><br><span class="line">    train_size = int(len(X) * (<span class="number">1</span> - valid_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据打乱</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two equal-length list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> (X[randomize], Y[randomize])</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmoid function can be used to calculate probability.</span></span><br><span class="line">    <span class="comment"># To avoid overflow, minimum/maximum output value is set.</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - ( <span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This is the logistic regression function, parameterized by w and b</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguements:</span></span><br><span class="line">    <span class="comment">#     X: input data, shape = [batch_size, data_dimension]</span></span><br><span class="line">    <span class="comment">#     w: weight vector, shape = [data_dimension, ]</span></span><br><span class="line">    <span class="comment">#     b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#     predicted probability of each row of X being positively labeled, shape = [batch_size, ]</span></span><br><span class="line">    <span class="keyword">return</span> _sigmoid(np.matmul(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X </span></span><br><span class="line">    <span class="comment"># by rounding the result of logistic regression function.</span></span><br><span class="line">    <span class="keyword">return</span> np.round(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算精度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes the cross entropy.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguements:</span></span><br><span class="line">    <span class="comment">#     y_pred: probabilistic predictions, float vector</span></span><br><span class="line">    <span class="comment">#     Y_label: ground truth labels, bool vector</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#     cross entropy, scalar</span></span><br><span class="line">    <span class="keyword">return</span> -np.dot(Y_label, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label), np.log(<span class="number">1</span> - y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradient</span><span class="params">(X, Y_label, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes the gradient of cross entropy loss with respect to weight w and bias b.</span></span><br><span class="line">    Y_pred = _f(X, w, b)</span><br><span class="line">    pred_error = Y_label - Y_pred</span><br><span class="line">    w_grad = -np.sum(pred_error * X.T, axis=<span class="number">1</span>)</span><br><span class="line">    b_grad = -np.sum(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, b_grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 文件路径</span></span><br><span class="line">X_train_fpath = <span class="string">'../data/X_train.csv'</span></span><br><span class="line">Y_train_fpath = <span class="string">'../data/Y_train.csv'</span></span><br><span class="line">X_test_fpath = <span class="string">'../data/X_test.csv'</span></span><br><span class="line">output_fpath = <span class="string">'output.csv'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 读取数据</span></span><br><span class="line"><span class="keyword">with</span> open(X_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    X_train = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float) <span class="comment"># 不要第一列的ID</span></span><br><span class="line">    <span class="comment"># print(X_train)</span></span><br><span class="line"><span class="keyword">with</span> open(Y_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    Y_train = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float)<span class="comment"># 不要第一列的ID，只取第二列</span></span><br><span class="line">    <span class="comment"># print(Y_train)</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    X_test = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float)</span><br><span class="line">    <span class="comment"># print(X_test)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 数据集处理</span></span><br><span class="line"><span class="comment"># 训练集和测试集normalization</span></span><br><span class="line">X_train, X_mean, X_std = _normalize(X_train, train=<span class="keyword">True</span>)</span><br><span class="line">X_test, _, _ = _normalize(X_test, train=<span class="keyword">False</span>, specified_column=<span class="keyword">None</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"><span class="comment"># 训练集验证集划分</span></span><br><span class="line">X_train, Y_train, X_valid, Y_valid = _train_valid_split(X_train,Y_train, valid_ratio=<span class="number">0.1</span>)</span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">valid_size = X_valid.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line">print(<span class="string">'Size of training set: &#123;&#125;'</span>.format(train_size))</span><br><span class="line">print(<span class="string">'Size of validation set: &#123;&#125;'</span>.format(valid_size))</span><br><span class="line">print(<span class="string">'Size of testing set: &#123;&#125;'</span>.format(test_size))</span><br><span class="line">print(<span class="string">'Dimension of data: &#123;&#125;'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练（使用小批次梯度下降法，Mini-batch training）</span></span><br><span class="line"><span class="comment"># 参数初始化</span></span><br><span class="line">w = np.zeros((data_dim, ))</span><br><span class="line">b = np.zeros((<span class="number">1</span>, ))</span><br><span class="line"><span class="comment"># 训练参数</span></span><br><span class="line">max_iter = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"><span class="comment"># 保存每个epoch的loss以作图</span></span><br><span class="line">train_loss = []</span><br><span class="line">valid_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">valid_acc = []</span><br><span class="line">step = <span class="number">1</span></span><br><span class="line"><span class="comment"># 迭代</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(max_iter):</span><br><span class="line">    <span class="comment"># 打乱训练集</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(int(np.floor(X_train.shape[<span class="number">0</span>] / batch_size))):</span><br><span class="line">        <span class="comment"># 取batch</span></span><br><span class="line">        X = X_train[idx * batch_size : idx * batch_size + batch_size]</span><br><span class="line">        Y = Y_train[idx * batch_size : idx * batch_size + batch_size]</span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line">        <span class="comment"># 梯度下降（learning rate decay with time）</span></span><br><span class="line">        w = w - learning_rate / np.sqrt(step) * w_grad</span><br><span class="line">        b = b - learning_rate / np.sqrt(step) * b_grad</span><br><span class="line">        step = step + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 计算训练集和验证集的loss和精度</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.round(y_train_pred)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size)</span><br><span class="line">    y_valid_pred = _f(X_valid, w, b)</span><br><span class="line">    Y_valid_pred = np.round(y_valid_pred)</span><br><span class="line">    valid_acc.append(_accuracy(Y_valid_pred, Y_valid))</span><br><span class="line">    valid_loss.append(_cross_entropy_loss(y_valid_pred, Y_valid) / valid_size)</span><br><span class="line">print(<span class="string">'Training loss: &#123;&#125;'</span>.format(train_loss[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">'Validation loss: &#123;&#125;'</span>.format(valid_loss[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">'Training accuracy: &#123;&#125;'</span>.format(train_acc[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">'Validation accuracy: &#123;&#125;'</span>.format(valid_acc[<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练过程可视化</span></span><br><span class="line"><span class="comment"># loss可视化</span></span><br><span class="line">plt.plot(train_loss)</span><br><span class="line">plt.plot(valid_loss)</span><br><span class="line">plt.title(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'valid'</span>])</span><br><span class="line">plt.savefig(<span class="string">'Loss.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># accuracy可视化</span></span><br><span class="line">plt.plot(train_acc)</span><br><span class="line">plt.plot(valid_acc)</span><br><span class="line">plt.title(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'valid'</span>])</span><br><span class="line">plt.savefig(<span class="string">'Accuracy.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 预测测试集</span></span><br><span class="line">predictions = _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id,label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;,&#123;&#125;\n'</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 寻找最重要的10个维度的特征</span></span><br><span class="line">index = np.argsort(np.abs(w))[::<span class="number">-1</span>] <span class="comment"># 将w按绝对值从大到小排序</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    features = np.array(f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index[:<span class="number">10</span>]:</span><br><span class="line">        print(features[i], w[i])</span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework2的记录。&lt;/p&gt;
&lt;p&gt;关注我的公众号：&lt;code&gt;臭咸鱼&lt;/code&gt;，回复&lt;code&gt;LHY&lt;/code&gt;可获取课程PPT、数据和代码下载链接。&lt;/p&gt;
&lt;p&gt;代码仓库：&lt;a href=&quot;https://g
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="https://chouxianyu.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="线性回归" scheme="https://chouxianyu.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="分类" scheme="https://chouxianyu.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-4.4概率生成模型Python实战</title>
    <link href="https://chouxianyu.github.io/2021/03/28/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4-4%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8BPython%E5%AE%9E%E6%88%98/"/>
    <id>https://chouxianyu.github.io/2021/03/28/李宏毅机器学习课程笔记-4-4概率生成模型Python实战/</id>
    <published>2021-03-28T02:12:34.000Z</published>
    <updated>2021-04-13T08:57:27.173Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework2的记录。</p><p>关注我的公众号：<code>臭咸鱼</code>，回复<code>LHY</code>可获取课程PPT、数据和代码下载链接。</p><p>代码仓库：<a href="https://github.com/chouxianyu/LHY_ML2020_Codes" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes</a></p><ul><li><p>任务描述（Task Description）</p><p>  二分类（Binary Classification）</p><p>  根据个人资料，判断每个人的年收入是否超过50000美元。</p></li><li><p>数据集描述（Dataset Description）</p><ul><li>train.csv</li><li>test_no_label.csv</li><li>x_train.csv</li><li>Y_train.csv</li><li>X_test.csv</li></ul></li><li><p>参考链接</p><p>  <a href="https://colab.research.google.com/drive/1JaMKJU7hvnDoUfZjvUKzm9u-JLeX6B2C" target="_blank" rel="noopener">https://colab.research.google.com/drive/1JaMKJU7hvnDoUfZjvUKzm9u-JLeX6B2C</a></p></li><li><p>代码</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 文件路径</span></span><br><span class="line">X_train_fpath = <span class="string">'../data/X_train.csv'</span></span><br><span class="line">Y_train_fpath = <span class="string">'../data/Y_train.csv'</span></span><br><span class="line">X_test_fpath = <span class="string">'../data/X_test.csv'</span></span><br><span class="line">output_fpath = <span class="string">'output.csv'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 函数定义</span></span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalize</span><span class="params">(X, train=True, specified_column=None, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalizes specific columns of X.</span></span><br><span class="line">    <span class="comment"># The mean and standard variance of training data will be reused when processing testing data.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#     X: data to be processed</span></span><br><span class="line">    <span class="comment">#     train: 'True' when processing training data, 'False' for testing data</span></span><br><span class="line">    <span class="comment">#     specific_column: indexes of the columns that will be normalized. If 'None', all columns</span></span><br><span class="line">    <span class="comment">#         will be normalized.</span></span><br><span class="line">    <span class="comment">#     X_mean: mean value of training data, used when train = 'False'</span></span><br><span class="line">    <span class="comment">#     X_std: standard deviation of training data, used when train = 'False'</span></span><br><span class="line">    <span class="comment"># Outputs:</span></span><br><span class="line">    <span class="comment">#     X: normalized data</span></span><br><span class="line">    <span class="comment">#     X_mean: computed mean value of training data</span></span><br><span class="line">    <span class="comment">#     X_std: computed standard deviation of training data</span></span><br><span class="line">    <span class="keyword">if</span> specified_column <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        specified_column = np.arange(X.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X[:, specified_column], axis=<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        X_std = np.std(X[:, specified_column], axis=<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    X[:, specified_column] = (X[:, specified_column] - X_mean) / (X_std + <span class="number">1e-8</span>)</span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集划分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_valid_split</span><span class="params">(X, Y, valid_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and validation set.</span></span><br><span class="line">    train_size = int(len(X) * (<span class="number">1</span> - valid_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmoid function can be used to calculate probability.</span></span><br><span class="line">    <span class="comment"># To avoid overflow, minimum/maximum output value is set.</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - ( <span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This is the logistic regression function, parameterized by w and b</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguements:</span></span><br><span class="line">    <span class="comment">#     X: input data, shape = [batch_size, data_dimension]</span></span><br><span class="line">    <span class="comment">#     w: weight vector, shape = [data_dimension, ]</span></span><br><span class="line">    <span class="comment">#     b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#     predicted probability of each row of X being positively labeled, shape = [batch_size, ]</span></span><br><span class="line">    <span class="keyword">return</span> _sigmoid(np.matmul(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X </span></span><br><span class="line">    <span class="comment"># by rounding the result of logistic regression function.</span></span><br><span class="line">    <span class="keyword">return</span> np.round(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算精度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 读取数据</span></span><br><span class="line"><span class="keyword">with</span> open(X_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    X_train = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float) <span class="comment"># 不要第一列的ID</span></span><br><span class="line">    <span class="comment"># print(X_train)</span></span><br><span class="line"><span class="keyword">with</span> open(Y_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    Y_train = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float)<span class="comment"># 不要第一列的ID，只取第二列</span></span><br><span class="line">    <span class="comment"># print(Y_train)</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    X_test = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float)</span><br><span class="line">    <span class="comment"># print(X_test)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 数据集处理</span></span><br><span class="line"><span class="comment"># 训练集和测试集normalization</span></span><br><span class="line">X_train, X_mean, X_std = _normalize(X_train, train=<span class="keyword">True</span>)</span><br><span class="line">X_test, _, _ = _normalize(X_test, train=<span class="keyword">False</span>, specified_column=<span class="keyword">None</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 计算每个类别的样本的平均值和协方差</span></span><br><span class="line"><span class="comment"># 区分类别</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y==<span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y==<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 计算每个类别的样本的平均值</span></span><br><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>) <span class="comment"># 计算每个维度特征的平均值</span></span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 计算每个类别的样本的协方差矩阵（可以研究下协方差矩阵是如何计算的以及为什么）</span></span><br><span class="line">cov_0 = np.zeros((data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros((data_dim, data_dim))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>] <span class="comment"># transpose没有参数的话，就是转置，计算协方差矩阵时需要转置</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 计算共享协方差矩阵（Shared covariance is taken as a weighted average of individual in-class covariance）</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train_0.shape[<span class="number">0</span>] + X_train_1.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 计算权重和偏置</span></span><br><span class="line"><span class="comment"># 计算协方差矩阵的逆矩阵</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and accurately.</span></span><br><span class="line">u, s, v = np.linalg.svd(cov, full_matrices=<span class="keyword">False</span>)</span><br><span class="line">inv = np.matmul(v.T * <span class="number">1</span> / s, u.T)</span><br><span class="line"><span class="comment"># 计算weight和bias</span></span><br><span class="line">w = np.dot(inv, mean_0 - mean_1)</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.dot(inv, mean_0)) + <span class="number">0.5</span> * np.dot(mean_1, np.dot(inv, mean_1)) + np.log(float(X_train_0.shape[<span class="number">0</span>])) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 计算在训练集上的准确率</span></span><br><span class="line">Y_train_pred = <span class="number">1</span> - _predict(X_train, w, b)</span><br><span class="line">print(<span class="string">'Training accuracy: &#123;&#125;'</span>.format(_accuracy(Y_train_pred, Y_train)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 预测测试集结果</span></span><br><span class="line">predictions = <span class="number">1</span> - _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id,label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;,&#123;&#125;\n'</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 寻找最重要的10个维度的特征</span></span><br><span class="line">index = np.argsort(np.abs(w))[::<span class="number">-1</span>] <span class="comment"># 将w按绝对值从大到小排序</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    features = np.array(f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index[:<span class="number">10</span>]:</span><br><span class="line">        print(features[i], w[i])</span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework2的记录。&lt;/p&gt;
&lt;p&gt;关注我的公众号：&lt;code&gt;臭咸鱼&lt;/code&gt;，回复&lt;code&gt;LHY&lt;/code&gt;可获取课程PPT、数据和代码下载链接。&lt;/p&gt;
&lt;p&gt;代码仓库：&lt;a href=&quot;https://g
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://chouxianyu.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="概率生成模型" scheme="https://chouxianyu.github.io/tags/%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>《基于深度学习的显著性目标检测综述》阅读笔记</title>
    <link href="https://chouxianyu.github.io/2021/03/20/%E3%80%8A%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://chouxianyu.github.io/2021/03/20/《基于深度学习的显著性目标检测综述》阅读笔记/</id>
    <published>2021-03-20T09:57:10.000Z</published>
    <updated>2021-03-20T09:58:22.201Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  基于深度学习的显著性目标检测综述</p></li><li><p>作者</p><p>  史彩娟等</p></li><li><p>发表时间</p><p>  2020年</p></li><li><p>来源</p><p>  知网</p></li></ul><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>本文基本研究对象为：基于深度学习的SOD算法。</p><p>下文中将显著性目标检测简写为SOD（Salient Object Detection），将基于深度学习的显著性目标检测算法简写为DSOD（Deep Learning Based SOD）。</p><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><ul><li><p>知识</p><ul><li><p>低层特征中包含更多的边界信息</p><ul><li>通过编码低层特征距离来检测边界信息可能导致部分边界细节丢失。</li><li>通过引入相关操作来增强低层特征具有的边界信息，可以获得更清晰的边界，但容易造成显著性目标检测主体不准确的问题。</li></ul></li><li><p>高层特征中包含更多的语义信息。</p><ul><li>只对高层特征所包含的语义进行增强，有时会造成显著性目标边界模糊或者多个显著性目标重合。</li></ul></li><li><p>只进行边界增强容易造成显著性目标不准确，而只进行语义增强则会引起显著性目标的边界不准确，所以可以同时增强两者。</p><p>  因为通过语义增强可以减小无效目标的干扰，更好地定位显著性目标的位置；通过边界增强可以获得清晰的显著性目标边界。</p><ul><li>金字塔结构可以处理高低层的特征</li></ul></li><li><p>全局信息（颜色，纹理，背景/前景等）包含显著性目标的位置信息，而局部信息可以增强显著性目标边界。</p></li></ul></li><li><p>一些未知的东西</p><ul><li>BASNet是怎么实现的？其中的混合Loss是什么</li><li>Boundary-Enhanced Loss是什么？</li><li>注意力机制（Attention）是什么？</li><li>金字塔结构是什么？</li><li>层次递归卷积神经网络(Hierarchical Recurrent Convolutional Neural Network，HRCNN)是什么？</li><li>字幕网络(Image Captioning Network，ICN)是什么？</li><li>SqueezeNet是什么？3种设计原则？</li><li>MobileNet是什么？深度可分离卷积？</li><li>可变形卷积是什么？</li><li>评估指标<ul><li>F-度量($F-measure,F_\beta$)</li><li>加权F-度量($Weighted\  F-measure,F_\beta^\omega$)</li><li>P-R曲线</li><li>平均绝对误差(Mean Absolute Error, MAE)</li></ul></li></ul></li><li><p>思考</p><ul><li>研究至少有2个思路：横向（分类）和纵向（深入）。</li></ul></li></ul><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><ol><li><p>根据原理不同，分3类介绍并定性分析比较DSOD。</p><p> 3个类别为：边界/语义增强、全局/局部结合、辅助网络</p></li><li><p>简单介绍DSOD的常用数据集和评估准则</p></li><li><p>现有DSOD方法在多个数据集上进行多方面的性能比较</p><p> 包括定量比较、P-R曲线和视觉比较</p></li><li><p><strong>现有DSOD算法在复杂背景、小目标、实时性检测等方面的不足</strong></p></li><li><p><strong>DSOD的未来发展方向，如复杂背景、实时、小目标、弱监督</strong></p></li></ol><h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><ul><li>传统SOD方法主要利用<strong>人类直观感觉或启发式先验</strong>，如利用色度比较，背景比较和边界点先验等，通过人工提取特征来检测目标，但人工提取特征非常耗时。</li><li>DSOD可自动学习到<strong>多尺度</strong>特征，精度速度大幅提升，但也存在<strong>不足</strong>：复杂背景下的性能有待提升、实时性需提高、模型复杂度需降低。</li></ul><h1 id="2-基于深度学习的显著性目标检测方法"><a href="#2-基于深度学习的显著性目标检测方法" class="headerlink" title="2 基于深度学习的显著性目标检测方法"></a>2 基于深度学习的显著性目标检测方法</h1><ul><li>传统方法中人工提取特征耗时或者迁移性较差</li><li>DSOD可分为3个类别：边界/语义增强、全局/局部结合、辅助网络</li><li>==图1==给出了今年DSOD的发展历程和主要算法</li></ul><h2 id="2-1-基于边界-语义增强的SOD"><a href="#2-1-基于边界-语义增强的SOD" class="headerlink" title="2.1 基于边界/语义增强的SOD"></a>2.1 基于边界/语义增强的SOD</h2><h3 id="2-1-1-基于边界增强的SOD方法"><a href="#2-1-1-基于边界增强的SOD方法" class="headerlink" title="2.1.1 基于边界增强的SOD方法"></a>2.1.1 基于边界增强的SOD方法</h3><ul><li><p>边界增强是指<strong>通过增强深度特征中的低层特征来获得更多的边界信息，从而更好的定位显著目标边界</strong>。</p><ul><li><p>ELD算法和KSR算法<strong>通过编码低层特征距离来检测边界信息</strong>，定位显著性目标轮廓，但是轮廓边界有时会模糊，导致部分边界细节丢失。</p></li><li><p>DCL算法和DSS算法<strong>通过引入相关操作来增强低层特征具有的边界信息</strong>。</p><p>  相较于直接编码低层特征距离的算法，这类方法获得的显著性目标<strong>边界更加清晰</strong>。但是，这些操作的引入容易引起显著性目标检测<strong>主体不准确</strong>，比如只有1个目标却检测出了2个。</p><p>原文中==图2==展示并对比了上述4个算法的检测效果。</p></li></ul></li><li><p>还可以<strong>直接对显著性目标的边界进行检测</strong>，比如GearNet、AFNet（<strong>采用BEL，Boundary-Enhanced Loss</strong>）、BASNet。</p><p>  这类方法能够提取清晰的显著性目标边界，边界细节相对较好，显著性目标的检测准确度较高(无关的显著性目标较少)，效果见原文==图3==。</p></li></ul><h3 id="2-1-2-基于语义增强的SOD方法"><a href="#2-1-2-基于语义增强的SOD方法" class="headerlink" title="2.1.2 基于语义增强的SOD方法"></a>2.1.2 基于语义增强的SOD方法</h3><ul><li><p>语义增强是指从<strong>高层特征</strong>中获得丰富的<strong>语义</strong>信息，从而更好的定位显著性目标，使显著性目标更加突出。</p><p>  比如R_FCN算法、CPD算法和PoolNet算法等，算法效果见原文==图4==。</p><p>  这类方法可以<strong>准确定位</strong>显著性目标，但是由于<strong>仅针对</strong>高层特征所包含的语义进行增强，有时会造成显著性目标<strong>边界模糊</strong>或者多个显著性目标重合。</p></li><li><p>还可以通过引入<strong>注意力机制</strong>进一步增强语义信息，如PiCANet和RAS算法。主体准确性和边界准确性都不错，算法效果见原文==图5==。</p></li></ul><h3 id="2-1-3-基于边界-语义增强的SOD方法"><a href="#2-1-3-基于边界-语义增强的SOD方法" class="headerlink" title="2.1.3 基于边界/语义增强的SOD方法"></a>2.1.3 基于边界/语义增强的SOD方法</h3><ul><li><p>只进行边界增强容易造成显著性目标模糊，而只进行语义增强则会引起显著性目标的边界模糊，所以可以<strong>同时对两者进行增强</strong>。</p><p>  因为通过语义增强可以减小无效目标的干扰，更好地定位显著性目标的位置；通过边界增强可以获得清晰的显著性目标边界</p><p>  这类算法有Amulet、BDMPM等，算法效果见原文==图6==。</p></li><li><p>还可以采用<strong>金字塔结构</strong>同时对高低层特征进行处理，以同时增强显著性目标边界和语义。</p><p>  这类算法有SRM算法、PAGE算法、FPA算法，算法效果见原文==图7==。</p></li></ul><h2 id="2-2-基于全局-局部结合的SOD"><a href="#2-2-基于全局-局部结合的SOD" class="headerlink" title="2.2 基于全局/局部结合的SOD"></a>2.2 基于全局/局部结合的SOD</h2><ul><li><p><strong>全局信息</strong>（颜色，纹理，背景/前景等）包含显著性目标的位置信息，而<strong>局部信息</strong>可以增强显著性目标边界。</p></li><li><p>一些检测方法采用<strong>递归</strong>操作、<strong>多分辨率</strong>操作和<strong>注意力机制</strong>等将全局/局部相结合以 获得更好的显著性目标检测性能</p></li><li><p>这类算法有DHSNet、GRL、NLDF、PAGR算法等，算法效果见原文==图8==。</p></li></ul><h2 id="2-3-基于辅助网络的SOD"><a href="#2-3-基于辅助网络的SOD" class="headerlink" title="2.3 基于辅助网络的SOD"></a>2.3 基于辅助网络的SOD</h2><ul><li>基于辅助网络的显著性目标检测是指采用其它领域已有模型作为辅助网络来提升显著性目标检测性能</li><li>这类算法有MDF、C2S-Net、CapSal、MLMSNet算法等，算法效果见原文==图9==。</li></ul><h2 id="2-4-不同类型SOD方法分析比较"><a href="#2-4-不同类型SOD方法分析比较" class="headerlink" title="2.4 不同类型SOD方法分析比较"></a>2.4 不同类型SOD方法分析比较</h2><ul><li>现有DSOD方法边界模糊的原因和解决办法<ol><li>深度模型包含许多下采样操作，上采样后的特征难以恢复原有的空间信息，融合后引起边界模糊。因此，为了减小<strong>下采样操作引起的多尺度融合损失</strong>，引入一些特定操作，如 PoolNet 算法中采用<strong>功能聚合模块</strong>等。</li><li>针对不同因素对边界检测的影响，通过<strong>编码低层特征距离</strong>来检测边界信息，定位显著性目标轮廓，如ELD算法和KSR算法；或者是<strong>设计新的损失函数</strong>， 通过反向传播调整模型参数，如AFNet算法和BASNet算法。</li><li>基础<strong>模型简易导致检测的边界模糊</strong>，可以通过<strong>多尺度操作</strong>增强原有的特征效果，如DSS算法、SRM算法和PAGE算法等，或添加<strong>注意力机制</strong>来提取更有效的低层特征，如PFA算法等。</li></ol></li><li>基于深度学习的显著性目标检测方法中常常引入注意力机制，大致可分为3类<ol><li><strong>时空域注意力</strong>，比较适合同时具有时序及空域特征的场景，通过递归神经网络 （Recurrent Neural Network，RNN）设计注意力机制，如PAGR算法。</li><li><strong>软注意力</strong>，是一种确定性的注意力，可以直接通过网络生成，它也是可微的， 可以通过神经网络算出梯度并且前向传播和后向 反馈来学习得到注意力的权重，如PFA算法和RAS算法；</li><li><strong>硬注意力</strong>，从输入信息中选择重要的特征，如PiCANet算法每个像素生成注意力图，这种方式更高效和直接。</li></ol></li></ul><h1 id="3-常用数据集及评估标准"><a href="#3-常用数据集及评估标准" class="headerlink" title="3 常用数据集及评估标准"></a>3 常用数据集及评估标准</h1><h2 id="3-1-常用数据集"><a href="#3-1-常用数据集" class="headerlink" title="3.1 常用数据集"></a>3.1 常用数据集</h2><ul><li><p>SOD数据集</p><p>  MSRA、SOD、MSRA10K、HKU-IS、DUTS、SED、ECSSD、DUTO-OMRON、PASCAL-S</p></li><li><p>常用DSOD数据集</p><ul><li>MSRA10K：边界框级别的显著性真值标定</li><li>HKU-IS：4447个图像，多个断开连接的显著性目标，多目标的边界重合和色彩对比度较低</li><li>DUTS：10553个训练图像和5019个测试图像，训练和测试集都包含非常重要的场景</li><li>SOD：300张图像，像素级注释，大部分图像包含多个显著性目标，并且目标与背景的颜色对比度较低。</li><li>ECSSD：1000张图像，图像具有复杂的结构和背景</li><li>DUTO-OMRON：5168个高质量图像，图像具有多个显著性目标，背景相对复杂</li><li>PASCAL-S：8 个类别，850张图像，用于评估具有复杂背景、多个目标场景的模型性能</li></ul></li></ul><h2 id="3-2-常用评估准则"><a href="#3-2-常用评估准则" class="headerlink" title="3.2 常用评估准则"></a>3.2 常用评估准则</h2><p>DSOD常用评估准则</p><ul><li><p>F-度量($F-measure,F_\beta$)</p><p>  对精度和召回率进行总体评估，最终值越大表明性能越好，其中$\beta$是一个参数，一般取$\beta^2=0.3$。公式暂略。</p></li><li><p>加权F-度量($Weighted\  F-measure,F_\beta^\omega$)</p><p>  加权F-度量是F-度量的推广，通过交替计算精度和召回率得到。</p><p>  加权F-度量为了解决邻域信息的不同， 为不同位置的不同误差分配了不同的权重。公式暂略。</p></li><li><p>P-R曲线</p><p>  以Precision和Recall作为纵-横轴坐标的二维曲线，即查准率-查全率曲线，选取不同阈值时对应的精度和召回率绘制。P-R曲线围起来 的面积是AP(Average Precision)值，AP值越高，模型性能越好。公式暂略。</p></li><li><p>平均绝对误差(Mean Absolute Error, MAE)</p><p>  MAE值越小表示模型越好。公式暂略。</p></li></ul><h1 id="4-基于深度学习的显著性目标检测方法性能比较"><a href="#4-基于深度学习的显著性目标检测方法性能比较" class="headerlink" title="4 基于深度学习的显著性目标检测方法性能比较"></a>4 基于深度学习的显著性目标检测方法性能比较</h1><ul><li>在数据集ECSSD、DUT-OMRON、HKU-IS和DUTS-TE上进行实验，采用F-度量和平均绝对误差MAE作为评估准则</li></ul><h1 id="5-基于深度学习的显著性目标检测算法的不足与未来展望"><a href="#5-基于深度学习的显著性目标检测算法的不足与未来展望" class="headerlink" title="5 基于深度学习的显著性目标检测算法的不足与未来展望"></a>5 基于深度学习的显著性目标检测算法的不足与未来展望</h1><ul><li><p>不足与未来展望</p><ul><li><p>复杂背景</p><ul><li>设计适应复杂背景（对背景敏感或者前景背景对比度低等）的显著性目标检测模型</li><li>建立包含复杂背景的图像数据集</li></ul></li><li><p>实时性</p><ul><li>设计轻量化网络，比如遵守SqueezeNet特有的三种设计原则</li><li>采用深度可分离卷积，比如MobileNet</li><li>对网络直接进行压缩与编码</li></ul></li><li><p>小目标</p><ul><li>使用分辨率更高的卷积特征图以及残差模块来增强对小目标的检测能力</li></ul></li><li><p>矩形框定位</p><ul><li>可变形卷积</li></ul></li><li><p>完全监督学习</p><p>  无监督或弱监督</p></li></ul></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文名称&lt;/p&gt;
&lt;p&gt;  基于深度学习的显著性目标检测综述&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;作者&lt;/p
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>PoolNet论文详解</title>
    <link href="https://chouxianyu.github.io/2021/03/19/PoolNet%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/"/>
    <id>https://chouxianyu.github.io/2021/03/19/PoolNet论文详解/</id>
    <published>2021-03-19T12:20:04.000Z</published>
    <updated>2021-03-19T12:25:47.568Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  A Simple Pooling-Based Design for Real-Time Salient Object Detection</p></li><li><p>作者</p><p>  Jiang-Jiang Liu, Qibin Hou, <strong>Ming-Ming Cheng</strong>等</p></li><li><p>发表时间</p><p>  2020年</p></li><li><p>来源</p><p>  CVPR2019</p></li></ul><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><ul><li>知识<ul><li>low-level和high-level可以指人类意识中的低级和高级，也可以指CNN物理结构中的浅层（低层）和深层（高层）。</li><li><strong>因为</strong>CNN类似于金字塔的结构特点；其浅层阶段具有较大尺寸并保留丰富的低级信息；其深层阶段包含更多高级语义信息也<strong>更容易从中得到显著目标的位置</strong>，但很coarse（粗糙）。</li></ul></li><li>一些未知的东西<ul><li>FPN</li><li>上采样</li><li>ResNet</li><li>PPM</li><li>Richer convolutional features for edge detection</li><li>SRM（A stagewise refinement model for detecting salient objects in images）</li><li>weight decay</li><li>balanced binary cross entropy loss</li><li>standard binary cross entropy loss，BCE？</li></ul></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ol><li><p>思路</p><p> 通过扩展<strong>池化</strong>在CNN中的作用，实现SOD。</p></li><li><p>实现</p><p> 基于<strong>U-shape</strong> architecture，构造2个模块</p><ol><li><p>GGM（global guidance module）</p><p> 自底向上，用来<strong>为不同层的特征图提供</strong>潜在显著目标的位置信息。</p></li><li><p>FAM（feature aggregation module）</p><p> 从顶向下，使coarse-level的语义信息和fine-level的特征<strong>较好地融合</strong>。</p><p> 在自顶向下的融合操作之后添加FAM，可以将GGM中coarse-level的特征与不同尺度的特征无缝融合。</p><p>这2个模块使高级语义信息逐渐完善，生成信息丰富的显著性图。</p></li></ol></li><li><p>通过锐化细节，和SOTA相比可以更精确地定位目标。</p></li><li><p>300×400的图片，FPS超过30，代码<a href="http://mmcheng.net/poolnet/。" target="_blank" rel="noopener">http://mmcheng.net/poolnet/。</a></p></li></ol><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li><p>U-shape结构</p><ul><li>多数传统方法通过手工设计的特征，单独或同时捕捉局部细节和全局上下文信息，但其性能因缺乏高层语义信息而受到限制，而CNN可以在不同尺度空间中提取高级语义信息和低级细节特征。</li><li><strong>因为</strong>CNN类似于金字塔的结构特点，其浅层阶段具有较大尺寸并保留丰富的低级信息，其深层阶段包含更多高级语义信息也<strong>更容易从中得到显著目标的位置</strong>。</li><li><strong>基于上述知识</strong>，学者设计了许多结构，其中U-shape的结构最受关注，其通过在分类网络上构建自上而下的路径来构建丰富的特征图。</li></ul></li><li><p>U-shape结构还存在提升空间</p><ul><li><p>存在问题</p><ol><li>因为U-shape结构中高级语义信息（位于深层网络）是逐步传到浅层的，与此同时深层捕捉到的位置信息可能会被稀释。</li><li>一个CNN的感受野大小和其深度是不成比例的。</li></ol></li><li><p>解决方法</p><ul><li><p>现有方法</p><p>  现有方法（见原文参考文献）通过引入注意力机制、以循环方式细化特征图、结合多尺度特征信息、向显著图添加额外约束（例如边界loss）等方法解决上述U-shape结构的问题。</p></li><li><p>本文方法</p><p>  本文提出的方法是基于U-shape并扩展池化技术，其中GGM和FAM都是基于FPN（feature pyramid networks）的。</p></li></ul></li></ul></li><li><p><strong>PoolNet</strong></p><p>  因为GGM和FAM都基于池化，所以将本文方法取名为PoolNet。</p><ul><li><p>GGM</p><p>  GGM包括一个修改过的PPM（pyramid pooling module）和一系列GGF（global guiding flows）。</p><ul><li><p>PPM</p><p>  和SRM（A stagewise refinement model for detecting salient objects in images）直接将PPM插入U-shape结构不同，本文提出的GGM是一个独立的模块。具体来讲，即把PPM放在backbone顶部用来捕捉全局指导信息（显著目标的位置）</p></li><li><p>GGF</p><p>  通过GGF，PPM收集到的高级语义信息可以传送到所有金字塔层，弥补了U-shape网络自上而下信号逐渐被稀释的问题。</p></li></ul></li><li><p>FAM</p><p>  考虑到来自GGF的coarse-level特征图与金字塔不同尺度特征图的融合问题，本文提出了一种FAM，其输入为融合后的特征图。</p><p>  该模块首先将融合（==应该是FPN操作？==）得到的特征图转换到多个特征空间，以捕获不同尺度的局部上下文信息，然后将这些信息进行组合以更好地权衡融合的输入特征图的组成。（==最后半句没懂，“权衡”一词具体指什么？==）</p></li></ul></li><li><p>edge detection branch</p><p>  还使用了边缘检测分支（edge detection branch），通过和边缘检测协同训练以锐化显著物体的细节。</p></li><li><p>性能</p><ul><li><p>精度</p><p>  <strong>Without bells and whistles，大幅超过之前的SOTA。</strong></p></li><li><p>速度</p><ul><li>一个NVIDIA Titan Xp GPU，图片尺寸300 × 400，<strong>速度超过30FPS</strong>。</li><li>不使用边缘检测分支时，5000张图片，训练时长不超过6小时，比多数方法快很多，因为池化操作比较快速。</li></ul></li></ul></li></ul><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><p>暂略</p><h1 id="3-PoolNet"><a href="#3-PoolNet" class="headerlink" title="3. PoolNet"></a>3. PoolNet</h1><ul><li>high-level semantic features are helpful for discovering the specific locations of salient objects.</li><li>low- and midlevel features are also essential for improving the features extracted from deep layers from coarse level to fine level.</li></ul><h2 id="3-1-Overall-Pipeline"><a href="#3-1-Overall-Pipeline" class="headerlink" title="3.1. Overall Pipeline"></a>3.1. Overall Pipeline</h2><ul><li>基于FPN（一种U型结构，从底向上和自顶向下，优点是可以组合多层特征）<ul><li>在从底向上之后引入GGM，提取高级语义信息后将其与各个金字塔层融合。</li><li>GGM之后，引入FAM保证不同尺度的特征图可以无缝融合。</li></ul></li></ul><h2 id="3-2-Global-Guidance-Module"><a href="#3-2-Global-Guidance-Module" class="headerlink" title="3.2. Global Guidance Module"></a>3.2. Global Guidance Module</h2><ul><li><p>现有不足</p><ul><li><p>FPN的问题</p><p>  问题之一：自顶向下是在自底向上之后的，就是高级特征被传给低层时会逐渐稀释。</p></li><li><p>CNN的问题</p><p>  根据实验，CNN的感受野比理论上要小得多，<strong>特别是对于较深的层</strong>，所以整个网络的感受野并不足够大以捕捉输入的全局信息。</p></li><li><p>直接影响</p><p>  可以看原文图2，只能检测到显著目标的局部。</p></li></ul></li><li><p>解决方法：GGM</p><p>  使得每个尺度的特征图都可以感知显著目标的位置。</p><ul><li><p>PPM</p><p>  包括4个子分支，作用是<strong>捕获</strong>输入图像的上下文信息。</p><p>  第一和最后一个子分支分别是一个identity mapping layer和一个global average pooling layer。</p><p>  中间的两个分支，我们采用adaptive average pooling layer，以确保它们输出的特征图分别具有3×3和5×5的空间大小。</p></li><li><p>GGF</p><p>  作用是将PPM捕捉到的信息与接下来自顶向下中不同金字塔层的特征图合理地<strong>融合</strong>在一起。</p><p>  与SRM（A stagewise refinement model for detecting salient objects in images）不同，它是将PPM视为U形结构的一部分，而本文中的GGM独立于U形结构。</p><p>  如原文图1中的绿色箭头，通过引入一系列GGF（identity mappings），可以将高级语义信息传递到各个级别的特征。</p><p>  这样，我们在自上而下路径的每个部分中显式增加了全局导航信息的权重，以<strong>确保在构建FPN时不会稀释位置信息</strong>。</p><p>  可以看原文图2，观察GGF的具体作用。</p></li></ul></li></ul><h2 id="3-3-Feature-Aggregation-Module"><a href="#3-3-Feature-Aggregation-Module" class="headerlink" title="3.3. Feature Aggregation Module"></a>3.3. Feature Aggregation Module</h2><ul><li><p>解决的问题</p><p>  使GGM的粗略特征图与金字塔不同尺度的特征地图无缝融合。</p><p>  具体来讲，在原始的FPN（VGGNet版本）中，高层特征图上采样比率为2，所以在上采样后边加一个3×3的卷积可以减少其带来的aliasing effect。</p><p>  但是，<strong>GGF还需要更大的上采样比率</strong>，比如8。所以使用FAM充分、高效地处理GGF和不同金字塔层特征图之间巨大的尺寸差异。</p></li><li><p>FAM</p><ul><li><p>结构</p><p>  每个FAM包含4个子分支，如原文图3所示。在forward过程中，输入的特征图先以不同比率进行下采样（平均池化），然后再以不同比率进行上采样，然后将4个分支融合（sum），然后送入一个3×3的卷积层。</p></li><li><p>优点</p><ul><li>减少上采样带来的aliasing effect，特别是当上采样比率较大（比如8）的时候。</li><li>使每个spatial location（空间位置）可以看到不同尺度空间的局部上下文信息，而且增大了整个网络的感受野。</li></ul></li></ul></li><li><p>实验</p><ul><li><p>原文图4</p><p>  将FAM替换成2个卷积层进行对比，把FAM模块附近的特征图可视化，证明FAM可以更好地捕捉显著目标的位置和细节信息。</p></li><li><p>原文图2</p><p>  f列和g列（尤其是第2行）进行对比，证明引入FAM可以sharpen显著目标的细节信息。</p><p>在下文的实验部分，会给出更多的数值结果。</p></li></ul></li></ul><h1 id="4-Joint-Training-with-Edge-Detection"><a href="#4-Joint-Training-with-Edge-Detection" class="headerlink" title="4. Joint Training with Edge Detection"></a>4. Joint Training with Edge Detection</h1><ul><li><p>问题</p><p>  第3节描述的网络结构已经在多个常用评估准则上超过了之前所有SOTA的单个模型的结果。</p><p>  但是原文作者发现<strong>许多不准确（incomplete or over-predicted）的预测是由于不清晰的目标边界造成的</strong>。</p></li><li><p>Edge Detection Branch</p><p>  在第3节描述的结构中添加1个预测分支，<strong>用来estimate显著目标的边界</strong>，具体结构见原文图1。</p><p>  在3个不同尺度的FAM之后添加3个residual block，<strong>用来information transformation</strong>，这3个residual block和<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>的设计相似并且具有<code>{128,256,512}</code>个通道（从fine level到coarse level）；和<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Richer_Convolutional_Features_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Richer convolutional features for edge detection</a>一文相同，每个residual block后面都有1个16通道的3×3卷积层（<strong>用来feature compression</strong>），还有1个单通道的1×1卷积层（<strong>用来边缘检测</strong>）。</p><p>  将上述3个16通道的3×3特征图进行拼接（concatenate）然后将其送入3个连续的48通道3×3卷积层，以将捕捉到的边缘信息传递给显著性目标检测分支，<strong>用来增强细节</strong>。</p></li><li><p>Train Edge Detection Branch taking the boundaries of salient objects as GT</p><p>  和<a href="http://arxiv.org/abs/1704.03604" target="_blank" rel="noopener">Instancelevel salient object segmentation</a>一文相似，本文在训练阶段将显著目标的边界作为GT<strong>用来联合训练</strong>，然而这并没有带来任何性能提升并且仍然缺少目标边界的细节信息。如图5的c列，当场景的前后景对比度较低时，得到的显著性图和边界图仍然很模糊。<strong>导致这个问题的原因可能是来自显著目标的GT边界图仍然缺少显著目标的大部分细节信息</strong>。GT边界只告诉我们显著目标的外边界的位置，特别是当显著目标之间有重叠的时候。</p></li><li><p>Train Edge Detection Branch taking the boundaries of salient objects as GT</p><p>  根据上述内容，本文尝试了和边缘检测任务实现协同训练，使用和和<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Richer_Convolutional_Features_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Richer convolutional features for edge detection</a>一文中相同的数据集。在训练时，来自显著目标检测数据集和边缘检测数据集的图像被交替输入。如图5所示，和边缘检测任务实现协同训练大幅提升了检测到的显著目标的细节。在下文实验部分，会给出更多的定量分析。</p></li></ul><h1 id="5-Experimental-Results"><a href="#5-Experimental-Results" class="headerlink" title="5. Experimental Results"></a>5. Experimental Results</h1><h2 id="5-1-Experiment-Steup"><a href="#5-1-Experiment-Steup" class="headerlink" title="5.1. Experiment Steup"></a>5.1. Experiment Steup</h2><ul><li><p>实现细节</p><p>  使用PyTorch框架，所有实验中的学习率优化器为Adam（5e-4的weight decay，初始学习率为5e-5然后15个epoch之后除以10）。</p><p>  本文的网络共训练24个epoch。</p><p>  网络backbone（如VGG-16、ResNet-50）的参数通过在ImageNet数据集上预训练的对应模型进行初始化，剩余参数随机初始化。</p><p>  如果没有特别声明，本文中消融实验（ablation experiments）默认使用VGG-16作为backbone，并使用和<a href="http://arxiv.org/abs/1704.03604" target="_blank" rel="noopener">Instancelevel salient object segmentation</a>一文相同的联合数据集（MSRA-B和HKU-IS）。</p><p>  本文在数据增强方面只使用了水平翻转。</p><p>  在训练和测试中，和<a href="http://arxiv.org/pdf/1611.04849v4.pdf" target="_blank" rel="noopener">Deeply supervised salient object detection with short connections</a>一文中一样，输入图片的尺寸保持不变。</p></li><li><p>数据集和损失函数</p><p>  在6个常用数据集（ECSSD、PASCALS、DUT-OMRON、HKU-IS、SOD和DUTS）上开展实验以评估性能。</p><p>  显著性目标检测中使用使用standard binary cross entropy loss，边缘检测使用balanced binary cross entropy loss。</p></li><li><p>评估标准</p><p>  使用3个广泛应用的指标（PR曲线、F-measure score和MAE）评估本文提出的方法。</p></li></ul><h2 id="5-2-Ablation-Studies"><a href="#5-2-Ablation-Studies" class="headerlink" title="5.2. Ablation Studies"></a>5.2. Ablation Studies</h2><p>ablation的译文是消融。</p><p>该section首先研究GGM和FAM的有效性，然后开展实验研究如何配置GGM，最后研究协同训练对性能的影响。</p><ul><li><p>Effectiveness of GGM and FAMs</p><p>  基于FPN的baseline，以VGG-16为backbone，研究GGM和FAMs的有效性。除了GGM和FAMs的不同组合，其它所有配置都相同。原文表1展示了其在数据集DUT-O和SOD上的性能，对应的视觉比较可以在原文图2中看到。</p><ul><li><p>GGM Only</p><p>  原文表1第4行数据说明GGM提升了F-measure和MAE。<strong>GGM生成的全局指导信息使网络更多地关注显著目标的完整性，大幅提升了所得显著性图的质量。因此，显著目标的细节（这些细节容易被感受野有限的模型错误预测为背景，比如原文图2的最后1行）可以被增强</strong>。</p></li><li><p>FAMs Only</p><p>  原文表1第5行的数据说明简单地将FAMs嵌入到原文图1所示的FPN baseline中提升了F-measure和MAE。这可能是因为<strong>FAM中的池化操作扩大了整个网络的感受野</strong>，并且FPN baseline仍然需要融合不同尺度的特征图，这说明<strong>FAM了缓解上采样aliasing effect的有效性</strong>。</p></li><li><p>GGM &amp; FAMs</p><p>  原文表1最后1行的数据说明同时引入GGM和FAMs可以得到更优的F-measure和MAE，<strong>这说明GGM和FAM是互补的。通过它们可以精确地定位显著目标并改善其细节（如图2所示）。原文图6中包含更多的定性结果。</strong></p></li></ul></li><li><p>Configuration of GGM</p><p>  为更好地了解GGM，独立使用PPM和GGF开展实验，数据分别在原文表1的第2行和第3行。这2个实验的结果都比使用GGM时的结果（原文表1第4行的数据）。<strong>这说明PPM和GGF在GGM中都起着重要作用。</strong></p></li><li><p>The Impact of Joint Training</p><p>  如原文表2所示，在3个数据集上，将显著目标边界（SalEdge）作为GT进行训练并没有提升baseline的性能，而使用标准的边缘（StdEdge）作为GT可以大幅提升baseline的性能，特别是MAE。这说明<strong>引入详细的边缘信息有助于显著性目标检测</strong>。</p></li></ul><h2 id="5-3-Comparisons-to-the-State-of-the-Arts"><a href="#5-3-Comparisons-to-the-State-of-the-Arts" class="headerlink" title="5.3. Comparisons to the State-of-the-Arts"></a>5.3. Comparisons to the State-of-the-Arts</h2><p>该section比较了本文方法和13个SOTA方法（具体是哪13个方法见原文，此处省略）。为公平比较，这13个方法的结果是原结果或者使用初始公开代码得到的结果。所有结果都不经过任何后处理，所有预测得到的显著性图都使用同一份代码进行评估。</p><ul><li><p>Quantitative Comparisons</p><p>  如原文表3所示，分别使用VGG-16和ResNet-50作为backbone，并在多份训练集上开展实验以<strong>排除潜在的性能波动</strong>。可以看到，在相同的训练集上，使用相同的backbone，PoolNet超过了之前所有的SOTA方法。平均速度（FPS）对比如原文表4所示。</p></li><li><p>PR Curves</p><p>  原文图7为在3个数据集上的PR曲线，可以看到PoolNet的PR曲线优于其它算法。随着Recall值趋于1，PoolNet的Precision比其它算法高很多。这说明PooNet得到的显著性图的错误正样本（false positives）较少。</p></li><li><p>Visual Comparisons</p><p>  原文图6给出了PoolNet和其它算法的定性对比。从上到下，分别是<strong>透明目标、小目标、大目标、复杂形状和前背景低对比度</strong>。可以看出，在几乎所有环境下，PoolNet不仅可以正确找出显著目标，还可以增强它们的边缘。</p></li></ul><h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h1><p> 本文设计GGM和FAM两个模块，提出PoolNet，并和边缘检测任务实现协同训练，在6个常用数据集上的效果优于之前所有SOTA方法。</p><p>GGM和FAM是独立于网络结构的，<strong>可以灵活地迁移到任何基于金字塔的模型</strong>。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文名称&lt;/p&gt;
&lt;p&gt;  A Simple Pooling-Based Design for Real-
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="PoolNet" scheme="https://chouxianyu.github.io/tags/PoolNet/"/>
    
  </entry>
  
  <entry>
    <title>BASNet论文详解</title>
    <link href="https://chouxianyu.github.io/2021/03/08/BASNet%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/"/>
    <id>https://chouxianyu.github.io/2021/03/08/BASNet论文详解/</id>
    <published>2021-03-08T03:04:00.000Z</published>
    <updated>2021-03-21T12:34:49.946Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  BASNet: Boundary-Aware Salient Object Detection</p></li><li><p>作者</p><p>  Xuebin Qin等</p></li><li><p>发表时间</p><p>  2019年</p></li><li><p>来源</p><p>  CVPR2019</p></li></ul><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><ul><li><p>知识</p><ul><li><p>本文混合损失函数中的3种loss</p></li><li><p>本文核心是混合损失函数以及refine module，其中refine module借用了ResNet中的思路：$S_{refined}=S_{coarse}+S_{residual}$。</p></li><li><p>如图3所示，coarse saliency map存在2方面的问题：①the blurry and noisy boundaries(边界不准确、不清晰), ②the unevenly predicted regional probabilities(同类区域中像素概率不均匀)</p></li><li><p>RRM_LC和RRM_MS等模块比较shallow，所以很难捕捉到可用于refinement的high level information。</p></li><li><p>评估指标</p><ul><li><p>PR Curve</p><p>  PR Curve是1种评估预测所得显著性图的标准方式。1张显著性图的precision和recall通过比较二值化的显著性图及其ground truth计算。<strong>1个二值化threshold对应的1对precision和recall是数据集中所有显著性图的平均precision和recall</strong>。threshold从0到1变化，可以得到<strong>1个precision-recall pair序列</strong>，画出来就是PR Curve。</p></li><li><p>F-measure</p><p>  F-measure可以全面地衡量precision和recall，其<strong>基于1对precision和recall进行计算</strong>。在进行算法比较时，一般采用最大的F-measure进行比较。</p></li><li><p>MAE</p><p>  MAE指显著性图与其ground truth的average absolute per-pixel difference。<strong>模型对于1个数据集的MAE为所有显著性图的MAE的平均值</strong>。</p></li></ul></li></ul></li><li><p>一些未知的东西</p><ul><li>U-Net</li><li>SegNet</li><li>dilation convolution</li><li>感受野的计算方法</li></ul></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul><li>背景：多数前人工作只关注region accuracy而非boundary quality。</li><li>本文提出：a predict-refine architecture: BASNet and a new hybrid loss for Boundary-Aware Salient object detection<ul><li>BASNet<ul><li>a densely supervised Encoder-Decoder network, in charge of saliency prediction</li><li>a residual refinement module(RRM), in charge of saliency map refinement</li></ul></li><li>The hybrid loss<ul><li>3级层次结构：pixel-level, patch-level, map-level</li><li>方式：混合3种loss，Binary Cross Entropy (BCE), Structural SIMilarity (SSIM) and Intersection-over-Union (IoU) losses</li></ul></li></ul></li><li><strong>效果</strong>：effectively segment the salient object regions and accurately predict <strong>the fine structures with clear boundaries</strong><ul><li>精度：在6个数据集上，ofregional and boundary evaluation measures超过SOTA。</li><li>速度：over 25 fps on a single GPU</li></ul></li><li>Code：<a href="https://github.com/NathanUA/BASNet" target="_blank" rel="noopener">https://github.com/NathanUA/BASNet</a></li></ul><h1 id="1-Inroducion"><a href="#1-Inroducion" class="headerlink" title="1. Inroducion"></a>1. Inroducion</h1><ul><li><p>背景</p><p>  近期FCN被用于显著性目标检测，性能优于传统算法，但their predicted saliency maps are still <strong>defective in fine structures and/or boundaries</strong>，如图1(c)和(d)所示。</p></li><li><p><strong>two main challenges</strong> in accurate salient object detection</p><ol><li>network：networks that <strong>aggregate multi-level deep features</strong> are needed</li><li>loss：models trained with CE loss usually have <strong>low confidence</strong> in differentiating boundary pixels, leading to <strong>blurry</strong> boundaries. Other losses such as Intersection over Union (IoU) loss, F-measure loss and Dice-score loss are not specifically designed for capturing <strong>fine structures</strong>。</li></ol></li><li><p>BASNet: the prediction module and RRM</p><p>  To capture both <strong>global (coarse)</strong> and <strong>local (fine)</strong> contexts，a new <strong>predict-refine</strong> network is proposed。</p><ul><li><p>the prediction module</p><p>  a <strong>U-Net-like</strong> densely supervised Encoder-Decoder network，transfers the input image to a probability map</p></li><li><p>RRM</p><p>  a novel residual refinement module，refines the predicted map by learning the residuals between the coarse saliency map and ground truth</p></li></ul></li><li><p>the hybrid loss</p><p>  To obtain <strong>high confidence</strong> saliency map and <strong>clear</strong> boundary, we propose a hybrid loss that combines Binary Cross Entropy (<strong>BCE</strong>), Structural SIMilarity (<strong>SSIM</strong>) and <strong>IoU</strong> losses, which are expected to learn from ground truth information in <strong>pixel-, patch- and map- level</strong>, respectively.</p><p>  <strong>Rather than using explicit boundary losses</strong> (NLDF+ , C2S ), we implicitly inject the goal of accurate boundary prediction in the hybrid loss, contemplating that it may help reduce spurious error from cross propagating the information learned on the boundary and the other regions on the image。</p><p>  毕竟任务不是边缘检测，所以不能只检测边缘。</p></li></ul><h1 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2. Related Works"></a>2. Related Works</h1><ul><li><p>Traditional Methods</p><p>  早期方法根据1个<strong>预定义</strong>的基于<strong>手工特征</strong>计算的saliency measure<strong>搜索像素</strong>以检测显著目标。Borji等人提供了1个全面的综述。</p></li><li><p>Patch-wise Deep Methods</p><p>  受深度卷积神经网络推动图片分类发展的启发，早期的deep方法基于从单/多尺度提取的<strong>local image patches</strong>将image pixels和super pixels分类为显著/非显著。这些方法通常生成<strong>coarse outputs</strong>因为在全连接层中<strong>spatial information会丢失</strong>。</p></li><li><p>FCN-based Methods</p><p>  相比于Traditional Methods和Patch-wise Deep Methods，基于FCN的方法取得了重要进展，因为FCN is able to <strong>capture richer spatial and multiscale information</strong>。</p><p>  UCF、Amulet、NLDF+、DSS+、HED、RAS、LFR、BMPM。</p></li><li><p>Deep Recurrent and attention Methods</p><p>  PAGRN、RADF+、RFCNPiCANetR</p></li><li><p>Coarse to Fine Deep Methods</p><p>  为捕捉finer structures和more accurate boundaries，学者提出了大量refinement策略。</p><p>  SRM、R3Net+、DGRL</p></li></ul><h1 id="3-BASNet"><a href="#3-BASNet" class="headerlink" title="3. BASNet"></a>3. BASNet</h1><h2 id="3-1-Overview-of-Network-Architecture"><a href="#3-1-Overview-of-Network-Architecture" class="headerlink" title="3.1. Overview of Network Architecture"></a>3.1. Overview of Network Architecture</h2><p>BASNet包括2个Module：Predict Module和Refine Module，如图2所示。</p><ul><li><p>Predict Module</p><p>  a U-Net-like densely supervised Encoder-Decoder network，作用是predict saliency map from input images</p></li><li><p>Refine Module</p><p>  Residual Refinement Module，<strong>refines the resulting saliency map</strong> of the prediction module by learning the residuals between the saliency map and the ground truth。</p></li></ul><h2 id="3-2-Predict-Module"><a href="#3-2-Predict-Module" class="headerlink" title="3.2. Predict Module"></a>3.2. Predict Module</h2><ul><li><p>整体</p><ul><li>受U-Net和SegNet启发，predict module是1个<strong>Encoder-Decoder</strong>网络，这种结构可以同时捕捉<strong>high level global contexts and low level details</strong>。</li><li>受HED启发，为减少过拟合，通过ground truth对decoder每个stage的最后1层进行监督，如图2所示。</li></ul></li><li><p>encoder</p><p>  encoder部分包含<strong>1个输入卷积层</strong>和<strong>6个由basic res-blocks组成的stage</strong>，其中输入卷积层和前4个stage是修改过的<strong>ResNet34</strong>。</p><p>  改动为本文中的输入卷积层有64个步长为1的3×3卷积核而非步长为2的7×7卷积核，并且在输入卷积层之后没有pooling，这使得在前几层获得更大尺寸的特征图但也减小了整体的感受野。</p><p>  为获得和ResNet34相同的感受野，本文<strong>在ResNet34又加了2个stage</strong>，每个stage均由size为2的不重叠maxpooling层及3个512filter的basic res-block组成。</p></li><li><p>bridge</p><p>  为进一步捕捉global infomation，本文<strong>在encoder和decoder之间添加了1个bridge</strong>。</p><p>  该bridge包括3个512核的dilation为2的3×3卷积层，其中每个卷积层后都有1个BN层和ReLU。</p></li><li><p>decoder</p><p>  decoder几乎和encoder完全对称。decoder的每个stage包含3个卷积层，每个卷积层后有1个BN层和ReLU。</p><p>  <strong>每个stage的输入是</strong>其前1个stage的输出和其对应的encoder中的stage的输出的<strong>concatenate</strong>结果。</p><p>  为得到side-output saliency maps，bridge和decoder中每个stage的输出被进行处理，处理过程为：<strong>1个3×3卷积、上采样、sigmoid</strong>。因此输入1张图片，本文的predict module在训练过程中输出<strong>7个saliency maps</strong>，其中<strong>最后1个saliency map</strong>的accuracy最高，所以其作为predict module的最终输出传入refine module。</p></li></ul><h2 id="3-3-Refine-Module"><a href="#3-3-Refine-Module" class="headerlink" title="3.3. Refine Module"></a>3.3. Refine Module</h2><ul><li>Refinement Module通常被定义为1个<strong>residual block</strong>，其通过学习saliency map和ground truth之间的residual$S_{residual}$来refine预测得到的coarse saliency map$S_{coarse}$，公式为$S_{refined}=S_{coarse}+S_{residual}$。</li><li>如图3所示，<strong>coarse包含2方面的含义</strong>：①the blurry and noisy boundaries, ②the unevenly predicted regional probabilities，模型预测所得的coarse saliency map中这2方面的问题都会有。</li><li><strong>RRM_LC</strong>(residual refinement module based on local context)起初被提出是用于boundary refinement，因为其感受野较小，Islam和Deng等人iteratively或recurrently在不同尺度上使用它refine saliency maps。Wang等人使用了PPM(pyramid pooling module)，其中3个尺度的pyramid pooling features被concatenate。为避免池化操作导致细节损失，<strong>RRM_MS</strong>使用kernel size和dilation不同的卷积层捕捉multi-scale contexts。然而这些模块是shallow的，所以<strong>很难捕捉到可用于refinement的high level information</strong>。</li><li>本文的<strong>RRM和predict module结构相似</strong>但简单很多，其包含1个输入层、1个encoder、1个bridge、1个decoder和1个输出层。encoder和decoder都包含<strong>4个stage</strong>，<strong>每个stage</strong>只包含1个64核的3×3卷积层，<strong>每个卷积层</strong>后面都有1个BN层和1个ReLU。bridge和1个stage结构相同，也包含1个64核的3×3卷积层(后面跟着1个BN层和1个ReLU)。encoder中下采样时使用maxpooling，decoder中上采样时使用bilinear interpolation。RRM的输出即本文整个模型最终的输出。</li></ul><h2 id="3-4-Hybrid-Loss"><a href="#3-4-Hybrid-Loss" class="headerlink" title="3.4. Hybrid Loss"></a>3.4. Hybrid Loss</h2><ul><li><p>训练中的Loss是各个side output的loss的加权和，每个side output的loss公式都是1个hybrid loss(3种loss之和)。本文模型对8个side output进行深度监督，其中7个side output来自Predict Module、1个side output来自Refine Module。</p><ul><li>BCE Loss：在二分类和分割任务中，二值交叉熵损失函数（Binary Cross Entropy Loss，BCE Loss）是最常用的损失函数。公式略</li><li>SSIM Loss：结构相似损失（Structural Similarity Loss，SSIM Loss）在提出时被用于图像质量评价。它可以捕捉一张图片中的结构信息，因此本文将其集成于混合损失函数中，以获取显著目标标注中的结构信息。公式略</li><li>IoU Loss：交并比损失（Intersection over Union Loss，IoU Loss）在提出时被用来衡量2个集合的相似性，后来被作为目标检测和分割的标准评估指标。最近，它也被用在了显著性目标检测的训练中。公式略</li></ul></li><li><p>本文阐述了3种loss的作用，图5中的热力图展示了每个像素的loss随训练过程的变化，3列分别代表训练过程中的不同阶段，3行分别是不同的loss。</p><ul><li><p>BCE Loss是<strong>pixel-level</strong>的measure，它并不考虑周围像素的label并且foreground像素和background像素的权重相同，<strong>有助于所有像素的收敛</strong>。</p></li><li><p>SSIM Loss是<strong>patch-level</strong>的measure，它考虑每个像素的local neighborhood，<strong>对边界具有更高的权重</strong>(即使预测前景的概率相同，但边界附近的loss比前景中心的loss更高)。</p><p>  在训练过程的初始阶段，边界周围像素的loss是最大的(见图5第2行)，<strong>这帮助集中于边界附近像素的收敛</strong>。随着训练过程，foreground的SSIM Loss减小而<strong>background的loss成为主导项</strong>。但只有当background的预测非常接近ground truth(0)时 background的loss才会起作用，<strong>这时loss会从1急速下降到0</strong>，因为通常只有在训练晚期BCE loss平滑(flat)时background的预测才会到0。SSIM Loss保证有足够的梯度使得网络继续学习。因为预测被push到0，background的预测看起来会更clean。</p></li><li><p>IoU Loss是<strong>map-level</strong>的measure，但是本文为了阐述所以根据式6画出了每个像素的IoU Loss。</p><p>  随着foreground的预测越来越接近1，foreground的loss最终变成0。</p><p>把3个loss混合起来，<strong>利用BCE使每个像素都有smooth gradient，利用IoU给予foreground更多注意力，通过SSIM基于图像结构使得边界的loss更大</strong>。</p></li></ul></li></ul><h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4. Experimental Results"></a>4. Experimental Results</h1><h2 id="4-1-Datasets"><a href="#4-1-Datasets" class="headerlink" title="4.1. Datasets"></a>4.1. Datasets</h2><p>在6个常用数据集上对模型进行了评估。具体是哪些数据集、这些数据集有哪些特点请见原文。</p><h2 id="4-2-Implementation-and-Experimental-Setup"><a href="#4-2-Implementation-and-Experimental-Setup" class="headerlink" title="4.2. Implementation and Experimental Setup"></a>4.2. Implementation and Experimental Setup</h2><ul><li><p>训练</p><ul><li>使用DUTS-TR训练模型，训练前进行离线数据增强(将图片水平翻转)。</li><li>将图片resize到256×256并随机裁剪成224×224。</li><li>encoder的部分参数使用ResNet34的预训练模型进行初始化，其它卷积层通过Xavier初始化。</li><li>使用Adam进行训练，超参数为默认值（初始学习率1e-3，betas=(0.9, 0.999)，eps=1e-8，weight decay=0）</li><li>一直训练到loss收敛，不使用validation set。最终经历了400k iterations，batch size为8，耗时125小时c</li></ul></li><li><p>测试/推理</p><p>  将原图resize到256×256，再将最终得到的显著图resize back到原图大小</p></li><li><p>软/硬件环境</p><p>  训练和测试的软硬件环境一致。</p><p>  PyTorch0.4.0，An eight-core PC with an AMD Ryzen 1800x 3.5 GHz CPU (with 32GB RAM) and a GTX 1080ti GPU (with 11GB memory)</p><p>  256×256图片的推理耗时为0.04秒。</p></li></ul><h2 id="4-3-Evaluation-Metrics"><a href="#4-3-Evaluation-Metrics" class="headerlink" title="4.3. Evaluation Metrics"></a>4.3. Evaluation Metrics</h2><p>PR Curve、F-measure、MAE、relaxed F-measure of boundary ($relaxF^b_{\beta}$)。对这几个评估指标的具体介绍请见原文。</p><ul><li><p>PR Curve</p><p>  PR Curve是1种评估预测所得显著性图的标准方式。1张显著性图的precision和recall通过比较二值化的显著性图及其ground truth计算。<strong>1个二值化threshold对应的1对precision和recall是数据集中所有显著性图的平均precision和recall</strong>。threshold从0到1变化，可以得到<strong>1个precision-recall pair序列</strong>，画出来就是PR Curve。</p></li><li><p>F-measure</p><p>  F-measure可以全面地衡量precision和recall，其<strong>基于1对precision和recall进行计算</strong>。在进行算法比较时，一般采用最大的F-measure进行比较。</p></li><li><p>MAE</p><p>  MAE指显著性图与其ground truth的average absolute per-pixel difference。<strong>模型对于1个数据集的MAE为所有显著性图的MAE的平均值</strong>。</p></li><li><p>relaxed F-measure of boundary</p><p>  此处省略，请见原文。</p></li></ul><h2 id="4-4-Ablation-Study"><a href="#4-4-Ablation-Study" class="headerlink" title="4.4. Ablation Study"></a>4.4. Ablation Study</h2><p>这个section验证了本文模型中的每个关键component。消融实验包括architecture ablation和loss ablation2个部分。消融实验是在ECSSD数据集上进行的。</p><ul><li><p>architecture ablation</p><p>  为证明BASNet的有效性，本文提供了BASNet与其它相关结构的量化对比结果。</p><p>  Loss都使用BCE，首先以U-Net作为baseline，然后是Encoder-Decoder Network、Deep Supervision、RRM_LC、RRM_MS、RRM_Ours，结果如表1所示，可见本文提出的<strong>BASNet在这5个实验中性能最优</strong>。</p></li><li><p>loss ablation</p><p>  为阐述本文提出的fusion loss的有效性，本文基于BASNet使用不同loss进行了1系列实验。表1中的结果证明<strong>本文提出的hybrid loss极大地提升了性能</strong>，特别是边界的质量。</p><p>  为进一步阐述损失函数对于BASNet预测质量的影响，使用不同Loss对BASNet进行训练的结果如图7所示，很明显可以看出<strong>本文提出的混合损失函数达到了最优的质量</strong>。</p></li></ul><h2 id="4-5-Comparison-with-State-of-the-arts"><a href="#4-5-Comparison-with-State-of-the-arts" class="headerlink" title="4.5. Comparison with State-of-the-arts"></a>4.5. Comparison with State-of-the-arts</h2><p>和15个SOTA算法进行比较。公平起见，使用原文作者提供的显著性图或者使用原文作者公开的模型。</p><ul><li><p>Quantitative evaluation</p><p>  <strong>图6</strong>展示了在5个数据集上的PR曲线和F-measure曲线，表2展示了在6个数据集上的maximum region-based F-measure、MAE、the relaxed boundary Fmeasure。数据提了很多个percent（略）</p></li><li><p>Qualitative evaluation</p><p>  <strong>图8</strong>展示对比了8种算法对<strong>不同类型图片</strong>的识别效果图，图片类型有images with low contrast、fine structures、large object touching image boundaries、complex object boundaries。</p></li></ul><h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h1><p>略</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文名称&lt;/p&gt;
&lt;p&gt;  BASNet: Boundary-Aware Sal
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="BASNet" scheme="https://chouxianyu.github.io/tags/BASNet/"/>
    
  </entry>
  
  <entry>
    <title>PSPNet论文阅读笔记</title>
    <link href="https://chouxianyu.github.io/2021/03/05/PSPNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://chouxianyu.github.io/2021/03/05/PSPNet论文阅读笔记/</id>
    <published>2021-03-05T07:53:52.000Z</published>
    <updated>2021-03-05T08:14:43.765Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  Pyramid Scene Parsing Network</p></li><li><p>作者</p><p>  Hengshuang Zhao等</p></li><li><p>发表时间</p><p>  2016年</p></li><li><p>来源</p><p>  CVPR2017</p></li></ul><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><h2 id="知识"><a href="#知识" class="headerlink" title="知识"></a>知识</h2><ul><li><p>scene parsing的目标是给图片中每个像素赋予1个类别标签，其可以提供对场景的完整理解，预测每个element的标签、位置和形状。</p></li><li><p>本文主要贡献</p><ul><li>提出PSPNet，将difficult scenery context features嵌入1个FCN based pixel prediction framework。</li><li>基于deeply supervised loss，为ResNet提供1个高效的优化策略。</li><li>为SOTA的scene parsing and semantic segmentation构建了1个practical system，并公开了所有关键细节。</li></ul></li><li><p>本文方法原理</p><p>  为获得合适的global features，本文提出PSPNet。除了<strong>使用传统的dilated FCN实现pixel prediction</strong>，我们<strong>将pixel-level feature扩展到了专门设计的global pyramid pooling的特征中</strong>，局部和全局的clues共同使得最终的预测更可靠。我们还提出了1种使用deeply supervised loss的优化策略。我们给出了所有实验细节（对本文模型的性能很重要），并公开了代码和训练好的模型。</p></li><li><p><strong>本文核心思路</strong></p><p>  scene parsing还面临着diverse scenes和unrestricted vocabulary的困难，其导致一些外表相似的物体被错误预测，但当有了context prior之后，理应可以得到正确的预测。目前基于FCN算法的主要问题也是缺乏利用global scene category clue的合适策略。所以PSPNet基于FCN将<strong>PPM（pyramid pooling module）</strong>作为高效的global context prior。</p></li><li><p><strong>PPM核心思路</strong></p><p>  感受野的大小可以大概表示我们能用多少context information。而CNN的实验感受野比其理论感受野小得多，特别是网络深层。这使得许多网络没有充分融合重要的global scenery prior。</p><p>  Global average pooling直接将所有像素混合得到1个vector可能会失去spatial relation并导致ambiguity。考虑到这一点，Global context information和sub-region context有助于区分各种category，1个更有力的representation可以是来自不同sub-regions的具有这些感受野的信息的融合。</p></li><li><p>其它</p><ul><li>FCN用卷积层代替全连接层进行分类</li><li>dilated convolution可以增大神经网络的感受野。</li><li>本文baseline network是FCN和dilated network(《Semantic image segmentation with deep convolutional nets and fully connected crfs》)。</li><li>一些工作主要探索了2个方向：multi-scale feature ensembling和structure prediction，这2个方向都都改善了scene parsing的localization ability。</li><li>开创性的《Semantic image segmentation with deep convolutional nets and fully connected crfs》使用条件随机场(CRF)作为post processing以精化分割结果。</li><li>《Parsenet: Looking wider to see better》证明了global average pooling with FCN可以提升语义分割结果。</li></ul></li></ul><h2 id="一些未知的东西"><a href="#一些未知的东西" class="headerlink" title="一些未知的东西"></a>一些未知的东西</h2><ul><li>scene和vocabulary具体指什么？</li><li>context prior具体指什么？</li><li>knowledge graph（知识图谱）是什么？</li><li>dilated convolution是什么？</li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul><li><p>背景</p><p>  对于unrestricted open vocabulary和diverse scenes，scene parsing(场景解析)具有挑战性。</p></li><li><p>方法</p><p>  本文使用PPM(pyramid pooling module)和提出的PSPNet(pyramid scene parsing network)，<strong>实现了通过融合different-region-based context获取全局context信息的能力</strong>。本文中的global prior presentation(全局先验表征)可在scene pareing任务中有效产生高质量结果，而PSPNet提供了1个出色的pixel-level prediction framework。</p></li><li><p>结果</p><p>  本文提出的方法在多个数据集上实现了SOTA，取得ImageNet scene parsing challenge 2016、PASCAL VOC 2012 benchmark和Cityscapes benchmark的第1名。单独1个PSPNet在PASCAL VOC 2012取得mIoU accuracy 85.4%的新纪录，在Cityscapes取得mIoU accuracy 80.2%的新纪录。</p></li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li><p>背景和意义</p><p>  基于semantic segmentation的scene parsing是计算机视觉中的基本主题，其目标是<strong>给图片中每个像素赋予1个类别标签</strong>。<strong>scene parsing可提供对场景的完整理解，它可以预测每个element的标签、位置和形状</strong>。对于自动驾驶、机器人感应等潜在应用，该主题引起了广泛兴趣。</p></li><li><p>困难</p><p>  <strong>scene parsing的困难与scene和label的多样性紧密相关</strong>。首创的scene parsing任务是对LMO数据集中2688张图片的33个场景进行分类。更多最近的PASCAL VOC语义分割数据集和PASCAL context数据集中包括更多context相似的标签，比如chair和sofa、horse和cow等等。新的ADE20K是最具挑战性的1个数据集数据集，其具有大量无限制开放的vocabulary和更多的scene类别，图1展示了其中一些具有代表性的图片。为这些数据集找到1个有效的算法需要克服许多困难。</p></li><li><p>目前进展</p><p>  scene parsing的SOTA算法多数是基于全卷积神经网络（FCN）。基于CNN的算法推动了dynamic object understanding，但考虑到<strong>diverse scenes和unrestricted vocabulary</strong>则还面临着挑战。图2第1行中1个boat被错误识别为car，出现这些错误是由于<strong>similar appearance of objects</strong>。但<strong>当图片的context prior为1条河附近的boathouse时，理应得到正确的预测</strong>。</p><p>  为了精准的场景感知，知识图谱依赖于scene context的先验信息(prior information)。我们发现<strong>目前基于FCN算法的主要问题是缺乏利用global scene category clue的合适策略</strong>。对于complex scene understanding，为了获取1个global image-level feature，<strong>spatial pyramid poolin</strong>g被广泛应用，因为spatial statistics为overall scene interpretation提供了较好的descriptor。<strong>Spatial pyramid pooling network</strong>进一步增强了这个能力。</p></li><li><p>本文方法原理</p><p>  与上述方法不同，为获得合适的global features，本文提出PSPNet。除了<strong>使用传统的dilated FCN实现pixel prediction</strong>，我们<strong>将pixel-level feature扩展到了专门设计的global pyramid pooling的特征中</strong>，局部和全局的clues共同使得最终的预测更可靠。我们还提出了1种使用deeply supervised loss的优化策略。我们给出了所有实验细节（对本文模型的性能很重要），并公开了代码和训练好的模型。</p></li><li><p>本文方法性能</p><p>  在所有公开数据集上达到SOTA，是ImageNet scene parsing challenge 2016的冠军、PASCALVOC 2012 semantic segmentation benchmark的第1名、urban scene Cityscapes data的第1名。这证明PSPNet为pixellevel prediction tasks指明了1个方向，它甚至可以帮助基于CNN的stereo matching、optical flow、depth estimation等。</p></li><li><p>本文主要贡献</p><ul><li>提出PSPNet，将difficult scenery context features嵌入1个FCN based pixel prediction framework。</li><li>基于deeply supervised loss，为ResNet提供1个高效的优化策略。</li><li>为SOTA的scene parsing and semantic segmentation构建了1个practical system，并公开了所有关键细节。</li></ul></li></ul><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><ul><li><p>受<strong>FCN用卷积层代替全连接层进行分类</strong>的启发，scene parsing和semantic segmentation等pixel-level prediction任务取得了巨大进步。为增大神经网络的感受野，《Semantic image segmentation with deep convolutional nets and fully connected crfs》和《Multi-scale context aggregation by dilated convolutions》使用了<strong>dilated convolution</strong>。Noh等人提出了1个coarse-to-fine structure with deconvolution network以学习segmentation mask。<strong>本文baseline network是FCN和dilated network(《Semantic image segmentation with deep convolutional nets and fully connected crfs》)</strong>。</p></li><li><p>其它工作主要探索了2个方向。</p><ul><li>第1个方向是multi-scale feature ensembling。因为在深度网络中，higher-layer feature包含更多semantic meaning，很少location information。<strong>融合多尺度的特征</strong>可以提高性能。</li><li><p>第2个方向是基于structure prediction。开创性的《Semantic image segmentation with deep convolutional nets and fully connected crfs》<strong>使用条件随机场(CRF)作为post processing以精化分割结果</strong>。还有一些方法通过end-to-end modeling精化了网络。</p><p>这2个方向都都改善了scene parsing的<strong>localization ability</strong>（predicted semantic boundary fits objects），然而在复杂场景中还有很大空间可以更加有效地利用必要信息。</p></li></ul></li><li><p>为充分利用global image-level priors(for diverse scene understanding)，一些方法使用传统方法而非神经网络提取了global context information。在object detection frameworks下也有了相似的提升。Liu等人证明了<strong>global average pooling with FCN可以提升语义分割结果</strong>。然而本文实验证明，对于ADE20K数据集，<strong>这些global descriptors还不足够representative</strong>。因此本文使用PSPNet实现了通过融合<strong>different-region-based context</strong>获取全局context信息的能力。</p></li></ul><h1 id="3-Pyramid-Scene-Parsing-Network"><a href="#3-Pyramid-Scene-Parsing-Network" class="headerlink" title="3. Pyramid Scene Parsing Network"></a>3. Pyramid Scene Parsing Network</h1><p>在把应用FCN到scene parsing时，我们观察到一些具有代表性的失败案例并对其进行了分析。这激发了我们<strong>将pyramid pooling module 作为高效global context prior</strong>的思路。图3展示了PSPNet的结构。</p><h2 id="3-1-Important-Observations"><a href="#3-1-Important-Observations" class="headerlink" title="3.1. Important Observations"></a>3.1. Important Observations</h2><p>ADE20K数据集包含150个stuff/object category labels（比如wall、sky、tree）和1038张imagelevel scene descriptors（比如airport terminal、bedroom、street），所以形成了大量的label和分布广阔的scene。检查了《Semantic understanding of scenes through the ADE20K dataset》提供的FCN baseline的预测结果，我们总结出了complex-scene parsing的几个普遍问题：Mismatched Relationship、Confusion Categories和Inconspicuous Classes（具体内容见原文）。总结这些问题，很多错误与<strong>不同感受野的contextual relationship和global information</strong>部分或完全相关。因此1个具有合适<strong>global-scene-level prior</strong>的神经网络可以大量提高scene parsing的性能。</p><h2 id="3-2-Pyramid-Pooling-Module"><a href="#3-2-Pyramid-Pooling-Module" class="headerlink" title="3.2. Pyramid Pooling Module"></a>3.2. Pyramid Pooling Module</h2><p>实验证明，pyramid pooling module是高效的global contextual prior。</p><p>在1个神经网络中，<strong>感受野的大小可以大概表示我们能用多少context information</strong>。虽然理论上ResNet的感受野已经超过输入图片的大小了，但Zhou等人发现<strong>CNN的实验感受野比其理论感受野小得多，特别是网络深层</strong>。这使得<strong>许多网络没有充分融合重要的global scenery prior</strong>。本文提出1个高效的global prior representation来解决这个问题。</p><p>Global average pooling可以很好地作为global contextual prior的baseline model，常常被用在图片分类任务中，也被用于semantic segmentation。考虑到ADE20K数据集中图片scene的复杂性，该策略并不足以涵盖必要信息：<strong>图片的每个像素被标注为许多stuff/objects，直接将它们混合得到1个vector可能会失去spatial relation并导致ambiguity</strong>。考虑到这一点，<strong>Global context information和sub-region context有助于区分各种category</strong>，1个更有力的representation可以是来自不同sub-regions的具有这些感受野的信息的融合。一些场景/图片分类方面的工作[引]也得到了相似结论。</p><p>在《Hypercolumns for object segmentation and fine-grained localization》中，pyramid pooling生成的不同层次的feature maps最终被flatten和concatenate，然后送入1个全连接层进行分类。该global prior的目标是移除CNN用于图片分类时fixed-size的constraint。为进一步减少不同sub-region之间的context information loss，本文提出1个hierarchical global prior，<strong>包含不同尺度和不同sub-region的信息</strong>，称其为pyramid pooling module，用来在神经网络最后1个特征图上构建global scene prior，如图3(c)所示。</p><p><strong>PPM的具体结构：</strong></p><p>PPM融合4个不同金字塔尺度的特征。最coarse的level是<strong>global pooling</strong>，生成1个single bin output。接下来的pyramid level将特征图<strong>划分成不同的sub-region并为不同location形成pooled representation</strong>。PPM中不同level的输出包含不同尺寸的特征图。为了保持global feature的占比，我们在每1个pyramid level后使用<strong>1×1卷积</strong>以将context representation的dimension减少到1。然后将这些1维特征图<strong>上采样</strong>以使其与初始特征图大小相同。最终将不同level的特征<strong>concatenate</strong>后作为最终的pyramid pooling global feature。</p><p>注意pyramid level的数量和每个level的size是<strong>可以修改</strong>的，它们与输入至pyramid pooling layer的特征图的size相关。该结构采用size不同、stride不同的pooling kernel以提取不同sub-region的特征。因此这些kernel在representation上应该保持合理的gap。本文中的PPM包含<strong>4个level</strong>，bin的size分别是<strong>1×1、2×2、3×3和6×6</strong>。关于选择最大池化还是平均池化，本文5.2节中做了大量实验。</p><h2 id="3-3-Network-Architecture"><a href="#3-3-Network-Architecture" class="headerlink" title="3.3. Network Architecture"></a>3.3. Network Architecture</h2><p>如图3所示，输入1张图片，使用预训练的<strong>ResNet</strong>模型并使用<strong>dilated network的策略</strong>提取得到feature map。最后feature map的size是输入图片size的1/8。然后使用PPM获取context information。通过本文4个level的PPM，可以涵盖图片的whole、half和small portions。然后将PPM4个分支的输出和PPM的输入concatenate。最后再用1个卷积层获得最后的prediction。</p><p>PSPNet为pixel-level scene parsing提供了高效的global contextual prior。PPM提取到的特征比global pooling更具代表性。考虑到计算成本，PSPNet相比于其 baseline(the original dilated FCN network)并没有增加多少计算成本。通过end-to-end learning，全局的PPM特征和局部的FCN特征可以最优化。</p><h1 id="4-Deep-Supervision-for-ResNet-Based-FCN"><a href="#4-Deep-Supervision-for-ResNet-Based-FCN" class="headerlink" title="4. Deep Supervision for ResNet-Based FCN"></a>4. Deep Supervision for ResNet-Based FCN</h1><p>深度网络可以得到好的性能，但增加网络深度可能导致optimization difficulty，ResNet通过在每个block中使用skip connection解决了这个问题。ResNet的Latter layers主要基于previous ones学习residues。</p><p>相反地，本文通过1个additional loss生成initial results，然后通过the final loss学习residue。因此，深度网络的优化被分解成易解的2个。</p><p>略……</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文名称&lt;/p&gt;
&lt;p&gt;  Pyramid Scene Parsing Network&lt;/p&gt;
&lt;/li&gt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="scene parsing" scheme="https://chouxianyu.github.io/tags/scene-parsing/"/>
    
      <category term="PSPNet" scheme="https://chouxianyu.github.io/tags/PSPNet/"/>
    
  </entry>
  
  <entry>
    <title>FPN网络图解及论文笔记</title>
    <link href="https://chouxianyu.github.io/2021/03/02/FPN%E7%BD%91%E7%BB%9C%E5%9B%BE%E8%A7%A3%E5%8F%8A%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>https://chouxianyu.github.io/2021/03/02/FPN网络图解及论文笔记/</id>
    <published>2021-03-02T14:38:21.000Z</published>
    <updated>2021-03-02T15:00:11.761Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FPN网络图解"><a href="#FPN网络图解" class="headerlink" title="FPN网络图解"></a>FPN网络图解</h1><p><img src="https://pic4.zhimg.com/v2-4bda90f7c75b31ff8d669a5f71860943_b.jpg" alt="FPN"></p><p><strong>原图片以及PPT源文件下载链接（欢迎关注我的知乎！）：</strong></p><p>链接：<a href="https://pan.baidu.com/s/10y78HagInyCuCA-aMeNJpg" target="_blank" rel="noopener">https://pan.baidu.com/s/10y78HagInyCuCA-aMeNJpg</a></p><p>提取码：iccm </p><p>复制这段内容后打开百度网盘手机App，操作更方便哦</p><h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  Feature Pyramid Networks for Object Detection</p></li><li><p>作者</p><p>  Tsung-Yi Lin等</p><p>  Facebook AI Research</p></li><li><p>发表时间</p><p>  2017年</p></li><li><p>来源</p><p>  CVPR2017</p></li></ul><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><ul><li><p>知识</p><ul><li><p>高级语义信息是有助于识别目标但有害于定位目标，低级空间信息有害于识别目标但有助于定位目标（由于其下采样次数较少，所以其可以更准确地定位）。</p></li><li><p>SSD没有利用higher-resolution maps，而本文证明了higher-resolution maps对于检测小物体很重要。</p></li><li><p>Introduction整体思路</p><p>  Feature Pyramids可以用于检测不同尺度的目标。</p><p>  Featurized Image Pyramids将图片缩放到不同大小并分别提取其特征并进行检测，其结构如图1(a)所示，其被大量应用于手工特征，其优点是每个层次都具有很强的语义信息（即使是high-resolution levels），其缺点是分别提取每个层次的特征并进行检测导致推理时间相应加倍。</p><p>  在recognition任务中，卷积网络逐渐取代了手工特征。卷积网络（Single feature map）逐渐不仅可以表示高级语义，在尺度变化方面也更具稳健性，因此可以只在一个尺度的特征图上进行检测，其结构如图1(b)所示，但其缺点是未利用到卷积网络固有的Pyramidal feature hierarchy。</p><p>  卷积网络（Pyramidal feature hierarchy）天然具有多尺度的、金字塔型的特征层次，其结构如图1(c)所示，其缺点是造成不同层次特征图之间的语义差异，high-resolution maps中包含的低级特征降低了该特征图在目标检测任务中的表征能力。SSD并没有利用higher-resolution maps，但本文证明了higher-resolution maps对于检测小物体很重要。</p><p>  如图2(top)所示，一些采用自上到下路径和skip connections的方法仅在自上到下路径顶部的单个特征图上进行预测，实际上这些方法还需要image pyramids以识别多尺度的目标。</p><p>  如图2(bottom)和图1(d)所示，FPN自然地利用卷积网络固有的金字塔层次同时创建每层都有较强语义信息的特征金字塔，通过1个从上到下的路径和侧边连接将low-resolution、semantically strong的特征和high-resolution、semantically weak的特征结合起来，在自上到下路径所有特征图上都进行预测，最终仅从单尺度的输入图片得到各层都有较强语义信息的特征金字塔，且不需额外计算成本。</p></li><li><p>FPN以<strong>任意大小</strong>的单张图片为输入。</p></li><li><p>对于ResNet，FPN并不将其第1个stage的输出包含到FPN中因为其内存占用量比较大。</p></li><li><p>FPN中bottom-up路径中相邻层间下采样比例为2</p></li><li><p>FPN的building block</p><p>  <strong>图3</strong>展示了创建top-down路径中特征图的building block。将top-down路径中coarser-resolution的特征图<strong>上采样（比例为2）</strong>，将bottom-up路径中的特征图<strong>通过1×1卷积减少通道数</strong>，然后将两者<strong>相加（element-wise）</strong>。这个过程一直迭代到生成最大的特征图。在开始top-down路径之前会在bottom-up路径顶层<strong>使用1×1卷积生成尺度最小的特征图</strong>。在每个相加操作之后<strong>使用3×3卷积减少上采样带来的混淆效应</strong>（aliasing effect）。</p></li><li><p>FPN和传统的featurized image pyramid一样，各个金字塔层都使用<strong>共享的classifier/regressor</strong>。</p></li></ul></li><li><p>一些未知的东西</p><ul><li><p>特征金字塔</p><p>  原文：E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image processing. RCA engineer, 1984.</p></li><li><p>尺度不变性（scale-invariant）</p></li></ul></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul><li>Feature Pyramids（特征金字塔）是识别系统中用来检测不同尺度的目标的一个基本组件</li><li><p>近期深度学习目标检测方法却避免了pyramid representations，如原文图1(b)，部分原因是它们是计算和内存密集型的</p></li><li><p>FPN(Feature Pyramid Network)</p><ul><li>使用少量成本，利用卷积网络固有的多尺度金字塔层次结构来构建特征金字塔</li><li>是一种带有侧向连接的自顶向下的结构，可以<strong>在所有尺度上构建高级语义特征</strong></li></ul></li><li>性能<ul><li>将FPN应用于基本的Faster-RCNN，在COCO detection benchmark上超过SOTA的single model</li><li>在GPU上达6FPS</li></ul></li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li><p>识别不同尺寸的目标是计算机视觉领域的一个基础挑战</p></li><li><p><strong>Featurized Image Pyramids</strong>（特征化的图像金字塔）</p><ul><li><p>定义</p><p>  Featurized Image Pyramids即Feature pyramids built upon image pyramids（建立在图像金字塔上的特征金字塔），见原文<strong>图1(a)</strong>。也就是<strong>将图片缩放到不同大小并分别提取其特征并进行检测</strong>。</p><p>  其构成了标准解决方案的基础，其具有<strong>尺度不变性（scale-invariant）</strong>，因为可以通过金字塔中的不同层来处理不同尺寸的目标，可以基于层维度和位置维度对目标进行扫描检测。</p></li><li><p>应用</p><p>  Featurized Image Pyramids在<strong>手工设计的特征</strong>中被大量使用，如DPM算法。</p></li><li><p>优点</p><p>  所有最近的ImageNet和COCO比赛的前几名都在Featurized Image Pyramids上进行多尺度测试。</p><p>  Featurized Image Pyramids的主要优点是<strong>每个层次都具有很强的语义信息，即使是high-resolution levels</strong>。</p></li><li><p>缺点</p><ul><li>分别提取每个层次的特征并进行检测，推理时间相应地加倍。</li><li><p>考虑到内存等因素，不可能端到端地在图像金字塔上训练卷积网络，最多只能在测试的时候使用图像金字塔（同时这也造成训练和测试的不一致性）</p><p>因为这些原因，Fast和Faster R-CNN默认也就没有使用FIP。</p></li></ul></li></ul></li><li><p>卷积网络（Single feature map）</p><p>  在Recognition任务中，手工特征逐渐被卷积网络取代。</p><p>  卷积网络不仅可以表示高级语义，在尺度变化方面也更具稳健性，因此可以<strong>只在一个尺度的特征图上进行检测</strong>，如原文<strong>图1(b)</strong>。</p><p>  但即使具备这样的稳健性，卷积网络中仍然需要金字塔（可以通过Pyramidal feature hierarchy改进）。</p></li><li><p>卷积网络（Pyramidal feature hierarchy）</p><ul><li><p>固有金字塔特征层次</p><p>  然而image pyramids并不是实现多尺度特征表示的唯一方法，<strong>卷积神经网络天然具有多尺度的、金字塔型的特征层次</strong>（feature hierarchy），如原文<strong>图1(c)</strong>。</p></li><li><p>缺点</p><p>  卷积网络的特征层次产生了不同尺寸的特征图，但是<strong>造成不同层次特征图之间的语义差异</strong>，尺寸大的特征图（high-resolution maps）包含着低级特征，这些<strong>低级特征降低了该特征图在目标检测任务中的表征能力</strong>。</p><p>  SSD是第一批尝试利用卷积网络中Pyramidal feature hierarchy的方法之一，但其也存在不足。理想情况中，SSD系列会利用forward pass中生成的多尺度特征，因此没有额外计算成本。但是为了避免使用低级特征，SSD没有使用已有的层，而是从网络高层开始构建金字塔并添加几个新层。因此<strong>SSD没有利用higher-resolution maps，而本文证明了higher-resolution maps对于检测小物体很重要</strong>。</p></li></ul></li><li><p>Feature Pyramid Network</p><ul><li><p>定义</p><p>  自然地利用卷积网络固有的金字塔层次同时创建每层都有较强语义信息的特征金字塔。如<strong>图1(d)和图2(bottom)</strong>所示，通过1个从上到下的路径和侧边连接<strong>将low-resolution、semantically strong的特征和high-resolution、semantically weak的特征结合</strong>起来，<strong>仅从单尺度的输入图片得到各层都有较强语义信息的特征金字塔</strong>，且不需额外计算成本。</p></li><li><p>相关工作</p><p>  如<strong>图2(top)</strong>所示，其它采用自上到下路径和skip connections的方法<strong>仅在自上到下路径顶部的单个特征图上进行预测</strong>，实际上<strong>这些方法还需要image pyramids以识别多尺度的目标</strong>，而FPN是在自上到下路径所有特征图上都进行预测。</p></li><li><p>效果</p><p>  在detection和segmentation系统上进行评估，仅将FPN应用于基础的Faster R-CNN在COCO detection benchmark上实现SOTA。</p><p>  在ablation experiments中，对于bounding box proposals，FPN将AR(Average Recall)提升了8 points；对于目标检测，将COCO-style Average Precision (AP)提升了2.3 points，将PASCAL-style AP，提高了3.8 points。</p><p>  FPN易于应用于到mask proposals并能提升实例分割的AR和速度。</p><p>  FPN可以进行端到端的多尺度训练，并且在训练和测试时都可以用，还不增加计算成本。</p></li></ul></li></ul><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><ul><li><p>Hand-engineered features and early neural networks</p><p>  SIFT特征用于特征点匹配，HOG和后来的SIFT在整个图像金字塔上进行密集计算，它们在图像分类、目标检测、人体姿态估计等任务中被大量使用。</p><p>  很快地也有了许多关于featurized image pyramids的研究。Dollar等人首先按尺度稀疏采样得到1个金字塔，然后对缺失的level进行插值，实现了快速的金字塔计算。在HOG和SIFT之前，使用卷积网络进行人脸检测的早期工作计算图像金字塔上的浅层网络以检测跨多尺度的人脸。</p></li><li><p>Deep ConvNet object detectors</p><p>  OverFeat采用1种与早期神经网络人脸检测器相似的策略，其将1个卷积网络作为1个sliding window detector应用于图像金字塔。</p><p>  R-CNN采用1种基于region proposal的策略，其中在使用卷积网络分类之前each proposal was scale-normalized。</p><p>  SPPNet表明这些基于region的detector可以更有效地应用于从单尺度图片提取出的特征图上。</p><p>  最近更加准确的Fast R-CNN和Faster R-CNN等算法提倡使用从单尺度图片计算得到的特征，因为这可以实现accuracy和speed的trade-off。</p><p>  <strong>Multi-scale detection仍然表现更佳，特别是对于小目标</strong>。</p></li><li><p>Methods using multiple layers</p><p>  最近大量方法利用卷积网络中的不同层提高了检测和分割的性能。FCN在多个尺度上计算每个category的的partial score以得到语义分割，Hypercolumns将类似方法应用于object instance segmentation。HyperNet、ParseNet和ION等方法在计算结果前将多层特征拼接，这等价于对转换后的特征求和。SSD和MS-CNN在多个特征层分别预测结果，并没有将多层特征结合。</p><p>  最近有很多方法在探索可以将低级特征和高级特征融合的lateral/skip connections，包括用于segmentation的U-Net和SharpMask、用于人脸检测的Recombinator networks、用于关键点预测的Stacked Hourglass networks。Ghiasi等人实现了在FCN上实现了1个Laplacian pyramid presentation，可以逐渐优化分割结果。即使这些方法采用了金字塔形状的结构，<strong>它们并不像featurized image pyramids一样在所有层独立预测结果</strong>，如图2所示，实际上<strong>这些方法还需要image pyramids以识别多尺度的目标</strong>。</p></li></ul><h1 id="3-Feature-Pyramid-Networks"><a href="#3-Feature-Pyramid-Networks" class="headerlink" title="3. Feature Pyramid Networks"></a>3. Feature Pyramid Networks</h1><p>FPN的目标是构造1个各层都具有较强高级语义信息的特征金字塔。FPN is general-purpose（多用途的），本文将其应用于RPN和Fast R-CNN，在Sec. 6中将其generalize到instance segmentation proposals。</p><p>FPN通过全卷积的方式，以任意大小的单张图片为输入，在多个尺度输出对应比例大小的特征图。该过程独立于骨干网络的具体架构，本文展示了基于ResNet的结果。FPN的构建包括1个bottom-up路径、1个top-down路径和lateral connections。</p><ul><li><p>Bottom-up pathway</p><p>  Bottom-up pathway就是骨干网络的feedforward，<strong>每层间下采样比例为2</strong>。网络中经常有连续几个层输出的特征图尺寸相同，我们称这些层位于同1个stage。在FPN中，为每个stage定义1个金字塔层。取每个stage中最后1层的输出代表该stage，因为每个stage中最深的层应该具有最强的特征。</p><p>  对于ResNet，使用后4个stage（相对于输入的步长分别为4、8、16、32）的输出，并不将第1个stage的输出包含到FPN中因为<strong>其内存占用量比较大</strong>。</p></li><li><p>Top-down pathway and lateral connections</p><p>  top-down pathway将金字塔中空间信息粗糙、语义信息更强的高层特征图上采样生成尺寸较大的特征图，然后通过lateral connections用bottom-up路径中的特征对这些特征进行增强。每个lateral connection将bottom-up路径和top-down路径中相同尺寸的特征图融合。bottom-up路径中的特征图具有较低级别的语义信息，但是<strong>由于其下采样次数较少</strong>所以其可以更准确地定位。</p><p>  <strong>图3</strong>展示了创建top-down路径中特征图的building block。将top-down路径中coarser-resolution的特征图<strong>上采样（比例为2）</strong>，将bottom-up路径中的特征图<strong>通过1×1卷积减少通道数</strong>，然后将两者<strong>相加（element-wise）</strong>。这个过程一直迭代到生成最大的特征图。在开始top-down路径之前会在bottom-up路径顶层<strong>使用1×1卷积生成尺度最小的特征图</strong>。在每个相加操作之后<strong>使用3×3卷积减少上采样带来的混淆效应</strong>（aliasing effect）。</p><p>  因为各个金字塔层都和传统的featurized image pyramid一样使用共享的classifier/regressor，所以<strong>将所有额外卷积层的输出通道数设置为256</strong>。在这些额外的层中，并不存在non-linearities，但我们凭经验发现其影响很小。</p><p>  simplicity对FPN非常重要，我们发现FPN对很多设计选择具有鲁棒性。我们使用更sophisticated的block（比如使用multilayer residual blocks作为连接）进行实验并观察到了略胜一筹的结果。设计更优的connection并非本文的重点，所以我们采用了上述的简单设计。</p></li></ul><h1 id="4-Applications"><a href="#4-Applications" class="headerlink" title="4. Applications"></a>4. Applications</h1><p>略……</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FPN网络图解&quot;&gt;&lt;a href=&quot;#FPN网络图解&quot; class=&quot;headerlink&quot; title=&quot;FPN网络图解&quot;&gt;&lt;/a&gt;FPN网络图解&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4bda90f7c75b31
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="FPN" scheme="https://chouxianyu.github.io/tags/FPN/"/>
    
  </entry>
  
</feed>
