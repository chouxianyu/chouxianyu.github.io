<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>臭咸鱼的缺氧瓶</title>
  
  <subtitle>快给我氧气！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://chouxianyu.github.io/"/>
  <updated>2021-05-31T07:06:29.557Z</updated>
  <id>https://chouxianyu.github.io/</id>
  
  <author>
    <name>臭咸鱼</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Boundary Aware PoolNet(3)：Boundary Aware PoolNet模型与代码介绍</title>
    <link href="https://chouxianyu.github.io/2021/05/31/Boundary-Aware-PoolNet-3-%EF%BC%9ABoundary-Aware-PoolNet%E6%A8%A1%E5%9E%8B%E4%B8%8E%E4%BB%A3%E7%A0%81%E4%BB%8B%E7%BB%8D/"/>
    <id>https://chouxianyu.github.io/2021/05/31/Boundary-Aware-PoolNet-3-：Boundary-Aware-PoolNet模型与代码介绍/</id>
    <published>2021-05-31T06:15:05.000Z</published>
    <updated>2021-05-31T07:06:29.557Z</updated>
    
    <content type="html"><![CDATA[<p>Boundary Aware PoolNet = PoolNet + BASNet，即使用BASNet中的Deep Supervision策略和Hybrid Loss改进PoolNet。</p><p>前面两篇文章分别介绍了PoolNet的模型及其代码、BASNet中的深度监督策略和混合损失函数，现在让我们看向PoolNet，使用BASNet一文提出的Deep Supervision和Hybrid Loss，形成BAPoolNet(Boundary Aware PoolNet)。</p><p>本文将详细介绍如何使用BASNet中的的Deep Supervision和Hybrid Loss改进PoolNet得到BAPoolNet，并介绍我在研究BAPoolNet时观察到的一些现象和结论。</p><p><strong>相关文章汇总：</strong></p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/359698101" target="_blank" rel="noopener">Boundary Aware PoolNet：基于PoolNet和BASNet的显著性目标检测</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698301" target="_blank" rel="noopener">Boundary Aware PoolNet(1)：PoolNet模型与代码介绍</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698320" target="_blank" rel="noopener">Boundary Aware PoolNet(2)：BASNet模型与代码介绍</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698332" target="_blank" rel="noopener">Boundary Aware PoolNet(3)：Boundary Aware PoolNet模型与代码介绍</a></p></li></ul><h1 id="BAPoolNet结构"><a href="#BAPoolNet结构" class="headerlink" title="BAPoolNet结构"></a>BAPoolNet结构</h1><p>在PoolNet中Backbone是ResNet50时，模型自顶向下路径中有5个FUSE操作，我借鉴BASNet中的Deep Supervision和Hybrid Loss使用这5层输出的混合Loss之和进行梯度下降，我将这整个模型称为<strong>BAPoolNet(Boundary Aware PoolNet)</strong>，其结构如下图所示。</p><p><img src="https://pic4.zhimg.com/80/v2-7854dd376ab3ea8529d00b7056343067_720w.jpeg" alt="img"></p><p>与PoolNet相比，BAPoolNet的不同之处为：</p><ol><li>添加5个边路输出以进行Deep Supervision</li><li>在计算Loss时使用BCE损失、SSIM损失、IOU损失之和</li></ol><p>如何实现Boundary Aware PoolNet，具体请看代码。</p><h1 id="BAPoolNet训练"><a href="#BAPoolNet训练" class="headerlink" title="BAPoolNet训练"></a>BAPoolNet训练</h1><h2 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h2><p>本文使用DUTS数据集进行模型训练和测试，其训练集包含10553张图片、测试集包含5019张图片。</p><p>除了对PoolNet的改进之外，BAPoolNet的其它实现细节和实验细节和PoolNet保持一致。</p><h2 id="训练过程中五个边路输出损失的变化情况"><a href="#训练过程中五个边路输出损失的变化情况" class="headerlink" title="训练过程中五个边路输出损失的变化情况"></a>训练过程中五个边路输出损失的变化情况</h2><p><img src="https://pic2.zhimg.com/80/v2-69b6a85891ab226ee5df51eb0d9c57de_720w.jpeg" alt="img"></p><h2 id="训练过程中三种损失的变化情况"><a href="#训练过程中三种损失的变化情况" class="headerlink" title="训练过程中三种损失的变化情况"></a>训练过程中三种损失的变化情况</h2><p><img src="https://pic1.zhimg.com/80/v2-71c71848b4c66a5b74d5af83306de182_720w.jpeg" alt="img"></p><h2 id="5个边路输出可视化结果"><a href="#5个边路输出可视化结果" class="headerlink" title="5个边路输出可视化结果"></a>5个边路输出可视化结果</h2><p><img src="https://pic2.zhimg.com/80/v2-1ce1a1a0a3ad8f86bfd91c6755a6018a_720w.jpeg" alt="img"></p><h2 id="训练过程中MAE的变化情况"><a href="#训练过程中MAE的变化情况" class="headerlink" title="训练过程中MAE的变化情况"></a>训练过程中MAE的变化情况</h2><p>下图为在DUTS-TE数据集上的测试结果。</p><p><img src="https://pic4.zhimg.com/80/v2-30646925efae3fa92521ee036bff90e8_720w.jpeg" alt="img"></p><h2 id="训练过程中F-measure的变化情况"><a href="#训练过程中F-measure的变化情况" class="headerlink" title="训练过程中F-measure的变化情况"></a>训练过程中F-measure的变化情况</h2><p>下图为在DUTS-TE数据集上的测试结果（二值化阈值为0.5）。</p><p><img src="https://pic1.zhimg.com/80/v2-ec40f811a9ab71a4c549e8c90c643495_720w.jpeg" alt="img"></p><h1 id="BAPoolNet性能"><a href="#BAPoolNet性能" class="headerlink" title="BAPoolNet性能"></a>BAPoolNet性能</h1><h2 id="视觉对比"><a href="#视觉对比" class="headerlink" title="视觉对比"></a>视觉对比</h2><p><img src="https://pic4.zhimg.com/80/v2-b507a874646b5988e1e4a6b18c24ecf7_720w.jpeg" alt="img"></p><h2 id="量化对比"><a href="#量化对比" class="headerlink" title="量化对比"></a>量化对比</h2><p>下表中MAE和maxF为各方法在DUTS-TE数据集上的测试结果。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Conference</th><th style="text-align:center">Backbone</th><th style="text-align:center">Size(MB)</th><th style="text-align:center">MAE↓</th><th style="text-align:center">maxF↑</th></tr></thead><tbody><tr><td style="text-align:center">CapSal</td><td style="text-align:center">CVPR19</td><td style="text-align:center">ResNet-101</td><td style="text-align:center">-</td><td style="text-align:center">0.063</td><td style="text-align:center">0.826</td></tr><tr><td style="text-align:center">PiCANet</td><td style="text-align:center">CVPR18</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">197.2</td><td style="text-align:center">0.050</td><td style="text-align:center">0.860</td></tr><tr><td style="text-align:center">DGRL</td><td style="text-align:center">CVPR18</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">646.1</td><td style="text-align:center">0.049</td><td style="text-align:center">0.828</td></tr><tr><td style="text-align:center">BASNet</td><td style="text-align:center">CVPR19</td><td style="text-align:center">ResNet-34</td><td style="text-align:center">348.5</td><td style="text-align:center">0.047</td><td style="text-align:center">0.860</td></tr><tr><td style="text-align:center">U2Net</td><td style="text-align:center">CVPR20</td><td style="text-align:center">RSU</td><td style="text-align:center">176.3</td><td style="text-align:center">0.044</td><td style="text-align:center">0.873</td></tr><tr><td style="text-align:center">CPD</td><td style="text-align:center">CVPR19</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">183.0</td><td style="text-align:center">0.043</td><td style="text-align:center">0.865</td></tr><tr><td style="text-align:center">PoolNet</td><td style="text-align:center">CVPR19</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">260.0</td><td style="text-align:center">0.040</td><td style="text-align:center">0.880</td></tr><tr><td style="text-align:center"><strong>BAPoolNet</strong></td><td style="text-align:center">-</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">260.7</td><td style="text-align:center"><strong>0.035</strong></td><td style="text-align:center"><strong>0.892</strong></td></tr></tbody></table></div><h2 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h2><p>下图为各方法在DUTS-TE数据集上的测试结果。</p><p><img src="https://pic4.zhimg.com/80/v2-e6ad03d7c136cf33416ad169bf0f89fc_720w.png" alt="img"></p><h2 id="F-measure曲线"><a href="#F-measure曲线" class="headerlink" title="F-measure曲线"></a>F-measure曲线</h2><p>下图为各方法在DUTS-TE数据集上的测试结果。</p><p><img src="https://pic2.zhimg.com/80/v2-94a0c6ae4d9e04127fdc38ece5310ae7_720w.png" alt="img"></p><h1 id="BAPoolNet代码"><a href="#BAPoolNet代码" class="headerlink" title="BAPoolNet代码"></a>BAPoolNet代码</h1><p>传送门(Boundary Aware PoolNet代码)：<a href="https://github.com/chouxianyu/Boundary-Aware-PoolNet" target="_blank" rel="noopener">https://github.com/chouxianyu/Boundary-Aware-PoolNet</a></p><p>相比于PoolNet，BAPoolNet代码的改动之处有：</p><ol><li><p>BCE Loss计算方法</p><p> 设置为<code>reduction=mean</code>而非<code>reduction=sum</code>，并且用<code>sigmoid+BCE</code>代替<code>F.binary_cross_entropy_with_logits</code>。</p></li><li><p>PoolNet<code>forward()</code>返回结果</p><p> PoolNet类返回了5个边路输出而非最终输出</p></li><li><p>整体Loss计算方法</p><p> 使用Hybrid Loss和Deep Supervision计算整体Loss</p></li></ol><p>模型性能评估代码（MAE、F-measure等），我参考了：<a href="https://github.com/Hanqer/Evaluate-SOD" target="_blank" rel="noopener">https://github.com/Hanqer/Evaluate-SOD</a></p><p>除了对PoolNet的改进之外，BAPoolNet的其它实现细节和实验细节和PoolNet保持一致。</p><h1 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h1><h2 id="“黑图片”问题"><a href="#“黑图片”问题" class="headerlink" title="“黑图片”问题"></a>“黑图片”问题</h2><p>在训练模型时，如果模型使用<code>train()</code>模式，结果反而比<code>eval()</code>差，具体现象为使用<code>train()</code>模式时，预测结果中有大量“黑图片”，即图片中的像素几乎都是黑色的。</p><h2 id="浅-深层的作用"><a href="#浅-深层的作用" class="headerlink" title="浅/深层的作用"></a>浅/深层的作用</h2><p>将5个边路输出可视化，可以看出0层的主要作用是定位显著目标，而后几层的主要作用是细化显著目标的特征，即在自顶向下路径中显著目标的细节逐渐丰富，同时这也代表着当浅层无法准确定位显著目标时最终输出也就无法准确定位显著目标。</p><p>我尝试过在计算5层Loss之和时调整不同层Loss的权重，比如第1层Loss权重为5、其它4层Loss权重为1或者前4层Loss权重为1、最后1层Loss权重为10等等，评估指标(MAE、F-meausure)显示调整不同层Loss权重的做法对性能提升并没有什么作用。</p><p>既然自顶向下路径中浅层富含高级语义信息/局部细节信息较少、深层高级语义信息相对较少/局部细节信息丰富，我想也许可以利用这一点在不同层使用不同的损失函数。（学识尚浅，也许这样并无效果）</p><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>基于Deep Supervision+Hybrid Loss，我还做了一些更具体的尝试，结果如下：</p><ol><li>将<code>F.binary_cross_entropy_with_logits</code>换为<code>sigmoid+BCE</code>对结果并没有什么影响，虽然官方也说了两者仅在计算上有细微差别。</li><li>若只使用BCE Loss，则<code>reduction=mean</code>和<code>reduction=sum</code>在训练效果上差别不大，只是两者所得Loss数值的数量级有所差异，前者Loss的数量级为0到10，后者Loss的数量级在1000到几万。</li><li>在计算边路输出时使用3×3卷积或者1×1卷积，评估所得模型，MAE、F-measure等差别不大。</li><li>调整不同层Loss的权重对模型性能不大。</li><li>调整学习率对模型性能影响不大。</li></ol><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Boundary Aware PoolNet = PoolNet + BASNet，即使用BASNet中的Deep Supervision策略和Hybrid Loss改进PoolNet。&lt;/p&gt;
&lt;p&gt;前面两篇文章分别介绍了PoolNet的模型及其代码、BASNet中的深度
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="BASNet" scheme="https://chouxianyu.github.io/tags/BASNet/"/>
    
      <category term="BAPoolNet" scheme="https://chouxianyu.github.io/tags/BAPoolNet/"/>
    
      <category term="PoolNet" scheme="https://chouxianyu.github.io/tags/PoolNet/"/>
    
  </entry>
  
  <entry>
    <title>Boundary Aware PoolNet(2)：BASNet模型与代码介绍</title>
    <link href="https://chouxianyu.github.io/2021/05/31/Boundary-Aware-PoolNet-2-%EF%BC%9ABASNet%E6%A8%A1%E5%9E%8B%E4%B8%8E%E4%BB%A3%E7%A0%81%E4%BB%8B%E7%BB%8D/"/>
    <id>https://chouxianyu.github.io/2021/05/31/Boundary-Aware-PoolNet-2-：BASNet模型与代码介绍/</id>
    <published>2021-05-31T06:10:05.000Z</published>
    <updated>2021-05-31T07:06:23.837Z</updated>
    
    <content type="html"><![CDATA[<p>Boundary Aware PoolNet = PoolNet + BASNet，即使用BASNet中的Deep Supervision策略和Hybrid Loss改进PoolNet。</p><p>为理解Boundary Aware PoolNet，我们并不需要学习整个BASNet，只需要了解其中的<strong>Deep Supervision</strong>策略和<strong>Hybrid Loss</strong>即可。</p><p>本文将简单介绍BASNet的模型结构，重点介绍其<strong>Deep Supervision</strong>和<strong>Hybrid Loss</strong>的理论和代码实现。</p><p><strong>相关文章汇总：</strong></p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/359698101" target="_blank" rel="noopener">Boundary Aware PoolNet：基于PoolNet和BASNet的显著性目标检测</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698301" target="_blank" rel="noopener">Boundary Aware PoolNet(1)：PoolNet模型与代码介绍</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698320" target="_blank" rel="noopener">Boundary Aware PoolNet(2)：BASNet模型与代码介绍</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698332" target="_blank" rel="noopener">Boundary Aware PoolNet(3)：Boundary Aware PoolNet模型与代码介绍</a></p></li></ul><h1 id="BASNet"><a href="#BASNet" class="headerlink" title="BASNet"></a>BASNet</h1><h2 id="传送门"><a href="#传送门" class="headerlink" title="传送门"></a>传送门</h2><ul><li>BASNet论文：<a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.html" target="_blank" rel="noopener">https://openaccess.thecvf.com/content_CVPR_2019/html/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.html</a></li><li>BASNet代码：<a href="https://github.com/xuebinqin/BASNet" target="_blank" rel="noopener">https://github.com/xuebinqin/BASNet</a></li><li>BASNet论文阅读笔记：<a href="https://zhuanlan.zhihu.com/p/355420066" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/355420066</a></li></ul><h2 id="BASNet结构"><a href="#BASNet结构" class="headerlink" title="BASNet结构"></a>BASNet结构</h2><p><img src="https://pic1.zhimg.com/80/v2-ac5675e09682e8a137a434bb41b54677_720w.png" alt="img"></p><p>如上图所示，BASNet模型包括Predict Module和Refine Module。</p><ul><li><p>Predict Module</p><p>  a U-Net-like densely supervised Encoder-Decoder network，作用是predict saliency map from input images。</p><p>  其实这个Encoder-Decoder结构和FPN(特征金字塔网络)没什么区别吧。</p></li><li><p>Refine Module</p><p>  refines the resulting saliency map of the prediction module by learning the residuals between the saliency map and the ground truth。</p></li></ul><p>基于上述的2个Module，BASNet使用Deep Supervision(上图中的Sup1-8)和Hybrid Loss进行模型训练。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>Predict Module的代码在文件<code>./model/BASNet.py</code>中类<code>BASNet</code>中，Refine Module的代码在文件<code>./model/BASNet.py</code>中类<code>RefUnet</code>中。</p><h1 id="Deep-Supervision"><a href="#Deep-Supervision" class="headerlink" title="Deep Supervision"></a>Deep Supervision</h1><p>直白来讲，<strong>Deep Supervision</strong>即使用神经网络中多个层的Loss之和进行梯度下降。</p><p>如前文中BASNet结构图所示，BASNet作者计算了Predict Module中的7层和Refine Module中的最后1层的Loss并进行求和，然后进行梯度下降，以此实现Deep Supervision。在计算边路输出时，需要进行上采样和卷积使得边路输出的尺寸、通道数与输入相同。</p><p>Deep Supervision的代码在文件<code>./model/BASNet.py</code>的类<code>BASNet</code>的函数<code>forward()</code>中，可知类<code>BASNet</code>在<code>forward()</code>时返回了8个边路输出，后继计算这8层的Hybrid Loss并求和进行梯度下降。</p><h1 id="Hybrid-Loss"><a href="#Hybrid-Loss" class="headerlink" title="Hybrid Loss"></a>Hybrid Loss</h1><p>直白来讲，<strong>Hybrid Loss</strong>即在计算损失时使用BCE Loss、SSIM Loss、IOU Loss这3个损失之和而非只使用BCE损失函数。</p><p>Hybrid Loss的代码在文件<code>./basnet_trin.py</code>中的函数<code>muti_bce_loss_fusion()</code>中。该函数的输入为BASNet的8个边路输出和输入对应的标注，该函数使用函数<code>bce_ssim_loss()</code>计算1个边路输出与标注的3种Loss之和。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Boundary Aware PoolNet = PoolNet + BASNet，即使用BASNet中的Deep Supervision策略和Hybrid Loss改进PoolNet。&lt;/p&gt;
&lt;p&gt;为理解Boundary Aware PoolNet，我们并不需要学习整个
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="BASNet" scheme="https://chouxianyu.github.io/tags/BASNet/"/>
    
      <category term="BAPoolNet" scheme="https://chouxianyu.github.io/tags/BAPoolNet/"/>
    
  </entry>
  
  <entry>
    <title>Boundary Aware PoolNet(1)：PoolNet模型与代码介绍</title>
    <link href="https://chouxianyu.github.io/2021/05/31/Boundary-Aware-PoolNet-1-%EF%BC%9APoolNet%E6%A8%A1%E5%9E%8B%E4%B8%8E%E4%BB%A3%E7%A0%81%E4%BB%8B%E7%BB%8D/"/>
    <id>https://chouxianyu.github.io/2021/05/31/Boundary-Aware-PoolNet-1-：PoolNet模型与代码介绍/</id>
    <published>2021-05-31T06:05:05.000Z</published>
    <updated>2021-05-31T07:06:17.184Z</updated>
    
    <content type="html"><![CDATA[<p>Boundary Aware PoolNet = PoolNet + BASNet，即使用BASNet中的Deep Supervision策略和Hybrid Loss改进PoolNet。</p><p>为理解Boundary Aware PoolNet还是需要全面理解PoolNet的，因此本文将对PoolNet的模型结构及其代码实现进行介绍。</p><p><strong>相关文章汇总：</strong></p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/359698101" target="_blank" rel="noopener">Boundary Aware PoolNet：基于PoolNet和BASNet的显著性目标检测</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698301" target="_blank" rel="noopener">Boundary Aware PoolNet(1)：PoolNet模型与代码介绍</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698320" target="_blank" rel="noopener">Boundary Aware PoolNet(2)：BASNet模型与代码介绍</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698332" target="_blank" rel="noopener">Boundary Aware PoolNet(3)：Boundary Aware PoolNet模型与代码介绍</a></p></li></ul><h1 id="PoolNet"><a href="#PoolNet" class="headerlink" title="PoolNet"></a>PoolNet</h1><p>PoolNet的论文名称为：A Simple Pooling-Based Design for Real-Time Salient Object Detection，该论文来自南开程明明老师的实验室。</p><h2 id="传送门"><a href="#传送门" class="headerlink" title="传送门"></a>传送门</h2><ul><li>PoolNet论文：<a href="https://arxiv.org/abs/1904.09569" target="_blank" rel="noopener">https://arxiv.org/abs/1904.09569</a></li><li>PoolNet代码：<a href="https://github.com/backseason/PoolNet" target="_blank" rel="noopener">https://github.com/backseason/PoolNet</a></li><li>PoolNet论文阅读笔记：<a href="https://zhuanlan.zhihu.com/p/358445121" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/358445121</a></li></ul><h2 id="用一句话概括PoolNet"><a href="#用一句话概括PoolNet" class="headerlink" title="用一句话概括PoolNet"></a>用一句话概括PoolNet</h2><p>PoolNet基于<strong>FPN</strong>(Feature Pyramid Network，特征金字塔网络)构造了GGM(Global Guidance Module，全局指导模块)和<strong>FAM</strong>(Feature Aggregation Module，特征聚合模块)2个模块，实现了显著性目标检测。</p><h2 id="PoolNet结构"><a href="#PoolNet结构" class="headerlink" title="PoolNet结构"></a>PoolNet结构</h2><p>PoolNet模型结构如下图所示，其包括：</p><ol><li><p>GGM(下图中，“P”指PPM，橙色箭头指GGF)</p></li><li><p>FAM(即下图中的“A”)</p></li><li><p>FPN(即下图中的U型结构)</p><p> 因此下图中GGM、FAM之外的部分就是FPN。</p></li></ol><p><img src="https://pic2.zhimg.com/80/v2-be5246490726fc5895025fb5daba09fe_720w.jpeg" alt="img"></p><h2 id="GGM和FAM"><a href="#GGM和FAM" class="headerlink" title="GGM和FAM"></a>GGM和FAM</h2><p><img src="https://pic1.zhimg.com/80/v2-ff8a917b998c1710ac7c8a91cd48b2c7_720w.jpeg" alt="img"></p><p>如上图所示，<strong>GGM</strong>包括PPM(Pyramid pooling module，金字塔池化模块)和GGF(Global Guiding Flows，全局指导流)，其作用是在FPN自底向上路径的顶部提取显著目标的位置信息并将其提供给FPN自顶向下路径中的不同金字塔层；<strong>FAM</strong>位于FPN自顶向下路径中FUSE操作之前，其作用是使来自GGM的特征与FPN自顶向下路径中不同金字塔层的特征更好地融合。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>PoolNet的代码在文件<code>./networks/poolnet.py</code>中的类<code>PoolNet</code>中，其调用了Backbone(比如ResNet50)、GGM、FAM等各个模块。</p><p>注意：学习代码时不只可以看PoolNet官方代码，还可以看我的Boundary Aware PoolNet的代码(包含详细中文注释，链接：<a href="https://github.com/chouxianyu/Boundary-Aware-PoolNet)。" target="_blank" rel="noopener">https://github.com/chouxianyu/Boundary-Aware-PoolNet)。</a></p><p>接下来将分别介绍PoolNet模型中各个模块的理论和代码。</p><h1 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h1><h2 id="传送门-1"><a href="#传送门-1" class="headerlink" title="传送门"></a>传送门</h2><ul><li>FPN论文：<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">https://arxiv.org/abs/1612.03144</a></li><li>ResNet论文：<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">https://arxiv.org/abs/1512.03385</a></li><li>FPN论文阅读笔记：<a href="https://zhuanlan.zhihu.com/p/354140540" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/354140540</a></li><li>ResNet论文阅读笔记：<a href="https://zhuanlan.zhihu.com/p/353228657" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/353228657</a></li><li>ResNet50网络结构图及结构详解：<a href="https://zhuanlan.zhihu.com/p/353235794" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/353235794</a></li></ul><h2 id="FPN结构"><a href="#FPN结构" class="headerlink" title="FPN结构"></a>FPN结构</h2><p>FPN即Feature Pyramid Network(特征金字塔网络)，其结构如下图所示。FPN包含1条自底向上路径(从此输入)和1条自顶向下路径(得到输出)，形成1种U型结构。</p><p><img src="https://pic2.zhimg.com/v2-4bda90f7c75b31ff8d669a5f71860943_1440w.jpg?source=172ae18b" alt="FPN网络图解"></p><h2 id="自底向上路径"><a href="#自底向上路径" class="headerlink" title="自底向上路径"></a>自底向上路径</h2><p>图片从该路径输入，经过卷积等一系列操作，特征的尺寸越来越小、通道数越来越多。上图所示的自底向上路径中有5个长方体，这并不代表该路径中只有5层，而是表明在一系列操作中选出5层特征图以进一步利用。当然了，也可以选择数量更多或更少的层。</p><h2 id="自顶向下路径"><a href="#自顶向下路径" class="headerlink" title="自顶向下路径"></a>自顶向下路径</h2><p>自顶向下路径由多个FUSE操作组成，上图所示的自顶向下路径中包含4个FUSE操作(数量与自底向上路径中选取的层的数量对应)，将最后1个FUSE操作的输出作为整个模型的输出。</p><h2 id="FUSE"><a href="#FUSE" class="headerlink" title="FUSE"></a>FUSE</h2><p>一般FPN的FUSE操作有2个输入，其一是自底向上路径中的特征(对其进行1×1卷积)，其二是自顶向下路径中上1个FUSE操作的输出(对其进行上采样)，分别对这2个输入进行1×1卷积和上采样再将两者相加，最后进行3×3卷积得到该FUSE操作的输出。</p><p>注意：因为PoolNet引入了GGM，所以PoolNet中的FUSE操作有3个输入(除了上述的2个输入还有1个输入是GGF的输出)，因此PoolNet中FPN的FUSE操作如下图所示(绿色箭头代表GGF，可以将上采样理解为GGF的一部分)。</p><p><img src="https://pic4.zhimg.com/80/v2-17c94fb51fddbffb95a4a9acf10f703b_720w.png" alt="img"></p><h2 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h2><p>FPN更多是一种结构而非一种网络，基于FPN这种U型结构，我们可以更换<strong>Backbone</strong>(骨干网络)。什么是Backbone呢？暂时你可以将Backbone理解为自底向上路径中的网络。常见的Backbone有VGG16、ResNet系列网络等。PoolNet作者使用了VGG16和RestNet50。</p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p>由上可知，FPN结构可以分为自底向上路径和自顶向下路径。</p><p>自底向上路径即Backbone，ResNet50的代码在文件<code>./networks/deeplab_resnet.py</code>中的类<code>ResNet</code>中，VGG16的代码在文件<code>./networks/vgg.py</code>中的类<code>vgg16</code>中。</p><p>自顶向下路径由多个FUSE操作形成，PoolNet作者将FUSE操作的代码与FAM的代码一起放在了文件<code>./networks/poolnet.py</code>中的类<code>DeepPoolLayer</code>中。</p><p>FPN自底向上路径和自顶向下路径中间还有一个1×1卷积，PoolNet作者将该1×1卷积的代码和GGM的代码放在了一起。</p><p>注意：学习代码时不只可以看PoolNet官方代码，还可以看我的Boundary Aware PoolNet的代码(包含详细中文注释，链接：<a href="https://github.com/chouxianyu/Boundary-Aware-PoolNet)。" target="_blank" rel="noopener">https://github.com/chouxianyu/Boundary-Aware-PoolNet)。</a></p><h1 id="GGM"><a href="#GGM" class="headerlink" title="GGM"></a>GGM</h1><p>由前文可知，GGM包括PPM(Pyramid pooling module，金字塔池化模块)和GGF(Global Guiding Flows，全局指导流)，其作用是在FPN自底向上路径的顶部提取显著目标的位置信息并将其提供给FPN自顶向下路径中的不同金字塔层。</p><h2 id="传送门-2"><a href="#传送门-2" class="headerlink" title="传送门"></a>传送门</h2><ul><li>PSPNet论文(PPM出处)：<a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="noopener">https://arxiv.org/abs/1612.01105</a></li><li>PSPNet论文阅读笔记：<a href="https://zhuanlan.zhihu.com/p/354860476" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/354860476</a></li></ul><h2 id="PPM"><a href="#PPM" class="headerlink" title="PPM"></a>PPM</h2><p>PPM由PSPNet一文提出。PoolNet中使用的是修改过的PPM，其位置在自底向上路径的顶部。</p><p>PoolNet中使用的PPM包括4个子分支，PoolNet中PPM的实现如下(可结合代码理解)：</p><ol><li><p>用4个分支对Backbone的输出进行处理</p><p> 第1个分支为恒等映射层(即不进行处理)；后3个分支分别为输出尺寸为1×1的平均池化层(即全局平均池化层)、输出尺寸为3×3的平均池化层、输出尺寸为5×5的平均池化层，并且这3个平均池化层后面都有一个1×1卷积(不改变通道数)、ReLU、上采样层(使尺寸与Backbone输出的尺寸一致)。</p></li><li><p>将4个分支的输出拼接</p><p> 首先在通道维度上将4个子分支的输出拼接，然后用3×3卷积使通道数与Backbone输出的通道数一致，再加上1个ReLU。</p></li></ol><p>由上可知，PPM输出特征的尺寸、通道数与输入相同。</p><p>注意：FPN自底向上路径和自顶向下路径中间还有一个1×1卷积，PoolNet作者将这个1×1卷积放在了PPM之前。</p><h2 id="GGF"><a href="#GGF" class="headerlink" title="GGF"></a>GGF</h2><p>当Backbone为ResNet50时，则需要4个GGF。</p><p>每个GGF的输入都是PPM的输出，不同GGF之间的区别是输出特征的尺寸、通道数不相同同。</p><p>每个GGF的组成为：上采样(使尺寸与对应金字塔层的尺寸一致)、3×3卷积、ReLU。</p><h2 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h2><p>FPN自底向上路径和自顶向下路径中间的1×1卷积、PPM、GGF的代码都位于类<code>Backbone_locate</code>中。如果Backbone是ResNet50则代码位于文件<code>./networks/deeplab_resnet.py</code>的类<code>ResNet_locate</code>中，如果Backbone是VGG16则代码位于文件<code>./networks/vgg.py</code>的类<code>vgg16_locate</code>中。</p><p>注意：学习代码时不只可以看PoolNet官方代码，还可以看我的Boundary Aware PoolNet的代码(包含详细中文注释，链接：<a href="https://github.com/chouxianyu/Boundary-Aware-PoolNet)。" target="_blank" rel="noopener">https://github.com/chouxianyu/Boundary-Aware-PoolNet)。</a></p><h1 id="FAM"><a href="#FAM" class="headerlink" title="FAM"></a>FAM</h1><p>由上可知，PoolNet作者在FPN自顶向下路径中每个FUSE操作前引入了FAM，其可以使来自GGM的特征与FPN自顶向下路径中不同金字塔层的特征更好地融合。</p><p><img src="https://pic2.zhimg.com/80/v2-19fbea793459041b77a0992c566d4b4d_720w.png" alt="img"></p><p>每个FAM包含4个子分支，如上图所示，其实现如下：</p><ol><li><p>用4个分支对输入进行处理</p><p> 第1个分支为恒等映射层(即不进行处理)；后3个分支分别为下采样比例为2/4/8的平均池化层，每个平均池化层后为一个3×3卷积(不改变特征的通道数和尺寸)和上采样层(使尺寸与输入特征的尺寸一致)</p></li><li><p>将4个分支的输出相加，再送入一个3×3卷积(根据设置改变或不改变通道数)</p></li></ol><p>FAM和FPN的FUSE操作的代码在文件<code>./networks/poolnet.py</code>中的类<code>DeepPoolLayer</code>中。</p><p>注意：学习代码时不只可以看PoolNet官方代码，还可以看我的Boundary Aware PoolNet的代码(包含详细中文注释，链接：<a href="https://github.com/chouxianyu/Boundary-Aware-PoolNet)。" target="_blank" rel="noopener">https://github.com/chouxianyu/Boundary-Aware-PoolNet)。</a></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Boundary Aware PoolNet = PoolNet + BASNet，即使用BASNet中的Deep Supervision策略和Hybrid Loss改进PoolNet。&lt;/p&gt;
&lt;p&gt;为理解Boundary Aware PoolNet还是需要全面理解Poo
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="BAPoolNet" scheme="https://chouxianyu.github.io/tags/BAPoolNet/"/>
    
      <category term="PoolNet" scheme="https://chouxianyu.github.io/tags/PoolNet/"/>
    
  </entry>
  
  <entry>
    <title>Boundary Aware PoolNet：基于PoolNet和BASNet的显著性目标检测</title>
    <link href="https://chouxianyu.github.io/2021/05/31/Boundary-Aware-PoolNet%EF%BC%9A%E5%9F%BA%E4%BA%8EPoolNet%E5%92%8CBASNet%E7%9A%84%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <id>https://chouxianyu.github.io/2021/05/31/Boundary-Aware-PoolNet：基于PoolNet和BASNet的显著性目标检测/</id>
    <published>2021-05-31T06:00:05.000Z</published>
    <updated>2021-05-31T07:05:54.266Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Boundary-Aware-PoolNet"><a href="#Boundary-Aware-PoolNet" class="headerlink" title="Boundary-Aware-PoolNet"></a>Boundary-Aware-PoolNet</h1><p>Boundary Aware PoolNet = PoolNet + BASNet : Deeply supervised PoolNet using the hybrid loss in BASNet for Salient Object Detection</p><p>Boundary Aware PoolNet = PoolNet + BASNet，即使用BASNet中的Deep Supervision和Hybrid Loss改进PoolNet。</p><p>经过评估，BAPoolNet的性能超过了之前的SOTA方法（注：在DUTS-TE数据集上进行测试，暂未在其它数据集上进行测试）。</p><p><strong>相关文章汇总：</strong></p><ul><li><p><a href="https://zhuanlan.zhihu.com/p/359698101" target="_blank" rel="noopener">Boundary Aware PoolNet：基于PoolNet和BASNet的显著性目标检测</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698301" target="_blank" rel="noopener">Boundary Aware PoolNet(1)：PoolNet模型与代码介绍</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698320" target="_blank" rel="noopener">Boundary Aware PoolNet(2)：BASNet模型与代码介绍</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/359698332" target="_blank" rel="noopener">Boundary Aware PoolNet(3)：Boundary Aware PoolNet模型与代码介绍</a></p></li></ul><p><strong>代码链接：</strong></p><p><a href="https://github.com/chouxianyu/Boundary-Aware-PoolNet" target="_blank" rel="noopener">https://github.com/chouxianyu/Boundary-Aware-PoolNet</a></p><h1 id="BAPoolNet结构"><a href="#BAPoolNet结构" class="headerlink" title="BAPoolNet结构"></a>BAPoolNet结构</h1><p>在PoolNet中Backbone是ResNet50时，模型自顶向下路径中有5个FUSE操作，我借鉴BASNet中的Deep Supervision和Hybrid Loss使用这5层输出的混合Loss之和进行梯度下降，我将这整个模型称为<strong>BAPoolNet(Boundary Aware PoolNet)</strong>，其结构如下图所示。</p><p><img src="https://pic4.zhimg.com/80/v2-7854dd376ab3ea8529d00b7056343067_720w.jpeg" alt="img"></p><p>与PoolNet相比，BAPoolNet的不同之处为：</p><ol><li>添加5个边路输出以进行Deep Supervision</li><li>在计算Loss时使用BCE损失、SSIM损失、IOU损失之和</li></ol><p>除了对PoolNet的改进之外，BAPoolNet的其它实现细节和实验细节和PoolNet保持一致。</p><p>如何实现Boundary Aware PoolNet，具体请看代码。</p><h1 id="BAPoolNet性能"><a href="#BAPoolNet性能" class="headerlink" title="BAPoolNet性能"></a>BAPoolNet性能</h1><h2 id="5个边路输出可视化结果"><a href="#5个边路输出可视化结果" class="headerlink" title="5个边路输出可视化结果"></a>5个边路输出可视化结果</h2><p><img src="https://pic2.zhimg.com/80/v2-1ce1a1a0a3ad8f86bfd91c6755a6018a_720w.jpeg" alt="img"></p><h2 id="视觉对比"><a href="#视觉对比" class="headerlink" title="视觉对比"></a>视觉对比</h2><p><img src="https://pic4.zhimg.com/80/v2-b507a874646b5988e1e4a6b18c24ecf7_720w.jpeg" alt="img"></p><h2 id="量化对比"><a href="#量化对比" class="headerlink" title="量化对比"></a>量化对比</h2><p>下表中MAE和maxF为各方法在DUTS-TE数据集上的测试结果。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Conference</th><th style="text-align:center">Backbone</th><th style="text-align:center">Size(MB)</th><th style="text-align:center">MAE↓</th><th style="text-align:center">maxF↑</th></tr></thead><tbody><tr><td style="text-align:center">CapSal</td><td style="text-align:center">CVPR19</td><td style="text-align:center">ResNet-101</td><td style="text-align:center">-</td><td style="text-align:center">0.063</td><td style="text-align:center">0.826</td></tr><tr><td style="text-align:center">PiCANet</td><td style="text-align:center">CVPR18</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">197.2</td><td style="text-align:center">0.050</td><td style="text-align:center">0.860</td></tr><tr><td style="text-align:center">DGRL</td><td style="text-align:center">CVPR18</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">646.1</td><td style="text-align:center">0.049</td><td style="text-align:center">0.828</td></tr><tr><td style="text-align:center">BASNet</td><td style="text-align:center">CVPR19</td><td style="text-align:center">ResNet-34</td><td style="text-align:center">348.5</td><td style="text-align:center">0.047</td><td style="text-align:center">0.860</td></tr><tr><td style="text-align:center">U2Net</td><td style="text-align:center">CVPR20</td><td style="text-align:center">RSU</td><td style="text-align:center">176.3</td><td style="text-align:center">0.044</td><td style="text-align:center">0.873</td></tr><tr><td style="text-align:center">CPD</td><td style="text-align:center">CVPR19</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">183.0</td><td style="text-align:center">0.043</td><td style="text-align:center">0.865</td></tr><tr><td style="text-align:center">PoolNet</td><td style="text-align:center">CVPR19</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">260.0</td><td style="text-align:center">0.040</td><td style="text-align:center">0.880</td></tr><tr><td style="text-align:center"><strong>BAPoolNet</strong></td><td style="text-align:center">-</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">260.7</td><td style="text-align:center"><strong>0.035</strong></td><td style="text-align:center"><strong>0.892</strong></td></tr></tbody></table></div><h2 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h2><p>下图为各方法在DUTS-TE数据集上的测试结果。</p><p><img src="https://pic4.zhimg.com/80/v2-e6ad03d7c136cf33416ad169bf0f89fc_720w.png" alt="img"></p><h2 id="F-measure曲线"><a href="#F-measure曲线" class="headerlink" title="F-measure曲线"></a>F-measure曲线</h2><p>下图为各方法在DUTS-TE数据集上的测试结果。</p><p><img src="https://pic2.zhimg.com/80/v2-94a0c6ae4d9e04127fdc38ece5310ae7_720w.png" alt="img"></p><h1 id="BAPoolNet代码"><a href="#BAPoolNet代码" class="headerlink" title="BAPoolNet代码"></a>BAPoolNet代码</h1><p><strong>传送门：</strong></p><ul><li>PoolNet代码：<a href="https://github.com/backseason/PoolNet" target="_blank" rel="noopener">https://github.com/backseason/PoolNet</a></li><li>BASNet代码：<a href="https://github.com/xuebinqin/BASNet" target="_blank" rel="noopener">https://github.com/xuebinqin/BASNet</a></li><li>BAPoolNet代码：<a href="https://github.com/chouxianyu/Boundary-Aware-PoolNet" target="_blank" rel="noopener">https://github.com/chouxianyu/Boundary-Aware-PoolNet</a></li></ul><p>相比于PoolNet，BAPoolNet代码的改动之处有：</p><ol><li><p>BCE Loss计算方法</p><p> 设置为<code>reduction=mean</code>而非<code>reduction=sum</code>，并且用<code>sigmoid+BCE</code>代替<code>F.binary_cross_entropy_with_logits</code>。</p></li><li><p>PoolNet<code>forward()</code>返回结果</p><p> PoolNet类返回了5个边路输出而非最终输出</p></li><li><p>整体Loss计算方法</p><p> 使用Hybrid Loss和Deep Supervision计算整体Loss</p></li></ol><p>模型性能评估代码（MAE、F-measure等），我参考了：<a href="https://github.com/Hanqer/Evaluate-SOD" target="_blank" rel="noopener">https://github.com/Hanqer/Evaluate-SOD</a></p><p>除了对PoolNet的改进之外，BAPoolNet的其它实现细节和实验细节和PoolNet保持一致。</p><p><strong>Coding Environments：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Python <span class="number">3.7</span><span class="number">.3</span></span><br><span class="line">torch <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line">torchvision <span class="number">0.5</span><span class="number">.0</span></span><br><span class="line">tensorflow  <span class="number">2.0</span><span class="number">.0</span></span><br><span class="line">tensorboard <span class="number">2.0</span><span class="number">.2</span></span><br><span class="line">tensorboardX <span class="number">2.1</span></span><br><span class="line">……</span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Boundary-Aware-PoolNet&quot;&gt;&lt;a href=&quot;#Boundary-Aware-PoolNet&quot; class=&quot;headerlink&quot; title=&quot;Boundary-Aware-PoolNet&quot;&gt;&lt;/a&gt;Boundary-Aware-PoolN
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="BASNet" scheme="https://chouxianyu.github.io/tags/BASNet/"/>
    
      <category term="BAPoolNet" scheme="https://chouxianyu.github.io/tags/BAPoolNet/"/>
    
      <category term="PoolNet" scheme="https://chouxianyu.github.io/tags/PoolNet/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-14.4 Seq2Seq：Transformer</title>
    <link href="https://chouxianyu.github.io/2021/05/23/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-14-4-Seq2Seq%EF%BC%9ATransformer/"/>
    <id>https://chouxianyu.github.io/2021/05/23/李宏毅机器学习课程笔记-14-4-Seq2Seq：Transformer/</id>
    <published>2021-05-23T01:01:21.000Z</published>
    <updated>2021-05-23T01:19:54.477Z</updated>
    
    <content type="html"><![CDATA[<p>transformer最知名的应用就是BERT，BERT就是无监督训练的transformer，transformer就是具有<strong>Self-attention</strong>的Seq2Seq模型。</p><p>RNN常用于处理输入和输出都是sequence的任务，因为RNN是通过遍历输入的sequence而逐步输出一个sequence，所以RNN很难被并行化。因为CNN可以并行化，所以有人提出用CNN处理输入和输出是sequence的任务：每个卷积核将sequence中的一部分作为输入并输出一个sequence、多个卷积核就可以生成多个sequence。层数较少的CNN不能看到long-term的信息，层数很多的CNN才能看到long-term的信息，如果在浅层（比如第一层）就需要看到long-term的信息怎么办呢？</p><p>有人提出<strong>用Self-Attention Layer代替RNN</strong>，其输入和输出和RNN一样都是sequence，它的特别之处是和Bidirectional RNN一样在输出时就已经看过了输入的整个sequence，并且可以并行计算。</p><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>Self-Attention来自于谷歌的paper：《Attention is all you need》。</p><ol><li><p>假设现在有输入序列$x^1,x^2,x^3,x^4$</p></li><li><p>进行embedding：$a^i=Wx^i$</p></li><li><p>将$a^i$输入到Self-Attention Layer得到$q^i,k^i,v^i$</p><p> $q$代表query，它是要去和key进行match的，$q^i=W^qa^i$</p><p> $k$代表key，它是要被query进行match的，$k^i=W^ka^i$</p><p> $v$代表value，我们要通过attention从中进一步提取information，$v^i=W^va^i$</p></li><li><p>使用每个query对每个key做attention</p><p> 以$q^i$为例，使用$q^i$和4个key得到$\alpha_{i,1},\alpha_{i,2},\alpha_{i,3},\alpha_{i,4}$，可以使用Scaled Dot-Product Attention：$\alpha_{i,j}=\frac{q^i\cdot k^j}{\sqrt d}$，其中$d$是$q^i$和$k^j$的维数，除以$\sqrt d$是因为$q^i\cdot k^j$的大小会受$d$的大小的影响；再使用softmax函数得到$\hat\alpha_{i,1},\hat\alpha_{i,2},\hat\alpha_{i,3},\hat\alpha_{i,4}$</p></li><li><p>$b^i=\sum\limits_j\hat\alpha_{i,j}v^j$，这样在计算$b^i$时就是可以看到输入的整个sequence</p></li></ol><h2 id="Self-Attention如何并行化计算"><a href="#Self-Attention如何并行化计算" class="headerlink" title="Self-Attention如何并行化计算"></a>Self-Attention如何并行化计算</h2><p>Self-Attention是如何实现并行化计算的呢？上述Self-Attention的计算其实都是一些矩阵运算，因此可以使用GPU加速。</p><ol><li>输入为$I$</li><li>$Q=W^qI,K=W^kI,V=W^vI$</li><li>$A=K^TQ$</li><li>$\hat A=softmax(A)$</li><li>输出为$O=V\hat A$</li></ol><h2 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head Self-attention"></a>Multi-head Self-attention</h2><p>Self-attention有一种变形是Multi-head Self-attention，现以2个head的情况为例介绍Multi-head Self-attention。</p><p>Multi-head的作用在于不同head关注的东西可能不一样。</p><ol><li><p>假设现在有输入序列$x^1,x^2,x^3,x^4$</p></li><li><p>进行embedding：$a^i=Wx^i$</p></li><li><p>将$a^i$输入到Self-Attention Layer得到$q^{i,1},q^{i,2},k^{i,1},k^{i,2},v^{i,1},v^{i,2}$</p><p> 这里体现了Multi-head。</p><p> $q$代表query，它是要去和key进行match的，$q^{i,1}=W^{q,1}a^i,q^{i,2}=W^{q,2}a^i$</p><p> $k$代表key，它是要被query进行match的，$k^{i,1}=W^{k,1}a^i,k^{i,2}=W^{k,2}a^i$</p><p> $v$代表value，我们要通过attention从中进一步提取information，$v^{i,1}=W^{v,1}a^i,v^{i,2}=W^{v,2}a^i$</p></li><li><p>使用每个query对每个key做attention</p><p> 这里体现了Multi-head。</p><p> 以$q^{i,1},q^{i,2}$为例，$q^{i,1}$只会和$k^{i,1}$做attention，$q^{i,2}$只会和$k^{i,2}$做attention</p></li><li><p>最终得到$b^{i,1},b^{i,2}$，将两者直接concat，如果需要还可以通过乘以$W^O$修改维度</p></li></ol><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>上面讲的Self-Attention并没有考虑sequence中元素之间的顺序，所以需要Positional Encoding。</p><p>Positional Encoding即每个position都有一个独一无二的positional vector $e^i$，这些vector并不是从数据中学习到的，在将$x^i$embedding得到$a^i$后再加上$e^i$，然后再输入到Self-Attention Layer。</p><h2 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h2><p>如何将Self-Attention应用到一个Seq2Seq模型中呢？</p><p>一般的Seq2Seq模型中包括2个RNN分别作为Encoder和Decoder，我们可以使用Self-Attention Layer代替这2个RNN。</p><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>以中文翻译为英文的任务为例，假如要把“机器学习”翻译为“Machine Learning”。</p><p>Transformer也分为Encoder和Decoder，Encoder的输入是“机器学习”，先给Decoder一个输入<code>&lt;BOS&gt;</code>代表句子的开始（begin of sentence），然后Decoder会输出一个“Machine”，在下一个时刻把“Machine”输入到Decoder得到“Learning”，直到Decoder输出“句点”。</p><p>下面介绍Transformer的具体结构，如下图所示。</p><p><img src="https://pic1.zhimg.com/80/v2-4b53b731a961ee467928619d14a5fd44_720w.jpg" alt="img"></p><p>如上图所示，在Encoder中，首先将输入进行embedding，然后加上positional encoding，然后进入多个相同的Block。每个Block中首先是一个Multi-head Attention，然后再将Multi-head Attention的输入和输出加起来，然后做Layer Normalization，然后进入Feedforward Layer，再将Feedforward Layer的输入和输出加起来，然后做Layer Normalization。</p><p>如上图所示，在Decoder中，Decoder的输入是Decoder在前一个时刻的输出，然后加上positional encoding，然后进入多个相同的Block。每个Block中首先是一个Masked Multi-head Attention（Masked指Decoder在做Self-Attention时只会attend已经生成的sequence），然后将Masked Multi-head Attention的输入和输出加起来，再做Layer Normalization，然后将Layer Normalization的输出和Encoder的输出输入到一个Multi-head Attention中，然后将Multi-head Attention的输入和输出详见并做Layer Normalization，然后进入Feedforward Layer，再将Feedforward Layer的输入和输出加起来，然后做Layer Normalization。多个相同的Block结束以后，进入Linear层，然后进入Softmax层得到最终的输出。</p><p>如果一个任务可以用Seq2Seq模型完成，那就可以用Transformer。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;transformer最知名的应用就是BERT，BERT就是无监督训练的transformer，transformer就是具有&lt;strong&gt;Self-attention&lt;/strong&gt;的Seq2Seq模型。&lt;/p&gt;
&lt;p&gt;RNN常用于处理输入和输出都是sequence的
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Seq2Seq" scheme="https://chouxianyu.github.io/tags/Seq2Seq/"/>
    
      <category term="Attention" scheme="https://chouxianyu.github.io/tags/Attention/"/>
    
      <category term="Transformer" scheme="https://chouxianyu.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-14.3 Seq2Seq：Tips for Generation</title>
    <link href="https://chouxianyu.github.io/2021/05/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-14-3-Seq2Seq%EF%BC%9ATips-for-Generation/"/>
    <id>https://chouxianyu.github.io/2021/05/22/李宏毅机器学习课程笔记-14-3-Seq2Seq：Tips-for-Generation/</id>
    <published>2021-05-22T02:00:57.000Z</published>
    <updated>2021-05-22T02:01:52.261Z</updated>
    
    <content type="html"><![CDATA[<p>在训练一个可以产生句子的网络时，有哪些技巧呢？ </p><h2 id="Bad-Attention"><a href="#Bad-Attention" class="headerlink" title="Bad Attention"></a>Bad Attention</h2><p>假如要做video的caption generation，某视频有4个frame，即有4个时刻的图片。</p><p>用$\alpha^i_t$表示attention weight，其上标表示frame的索引、下标表示时刻的索引。在第1个时刻，产生attention $\alpha^1_1,\alpha^2_1,\alpha^3_1,\alpha^4_1$，生成第1个word $w_1$；在第2个时刻，产生attention $\alpha^1_2,\alpha^2_2,\alpha^3_2,\alpha^4_2$，生成第2个word $w_2$；以此类推……</p><p>这样有时候会产生一些<strong>bad attention</strong>。比如，如果4个时刻的attention都集中在某一个frame上，就会产生一些奇怪的结果，比如每次生成的word都是相同的。</p><p><strong>good attention</strong>需要关注到输入中的每个frame，对每个frame的关注度不能太多也不能太少并且应该是同等级的。那如何实现这种好的attention呢？比如使用正则项$\sum_i(\tau-\sum_t\alpha_t^i)$使得每个frame在4个时刻的attention weight之和都接近$\tau$，这个$\tau$是通过学习得到的，详见《Show, Attend and Tell: Neural  Image Caption Generation with Visual Attention》。</p><h2 id="Mismatch-between-Train-and-Test"><a href="#Mismatch-between-Train-and-Test" class="headerlink" title="Mismatch between Train and Test"></a>Mismatch between Train and Test</h2><p>假如用RNN生成sentence，在训练时模型refer了整个sentence，如果在某一步预测失败可以通过损失函数优化每一次预测；而在测试时，模型的输入是上一步的输出，如果一步出错，可能就会步步出错。这个问题叫做<strong>Exposure Bias</strong>。那么我们如何解决train和test之间的mismatch呢？</p><p>可以考虑修改训练方法，假设模型现在应该输出A，但现在模型输出了B，即使是错误的输出我们也应该让这个错误的输出作为下一次的输入，这样train和testing就是match的，但实际上这种训练方法是难以见效的。有一个可行的方法是<strong>Scheduled Sampling</strong>，按一定概率选择模型上一步的输出或者标注作为模型的输入，可以在刚开始时只使用标注作为输入然后慢慢开始使用模型上一步的输出作为输入。</p><h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><p>在一颗庞大的树中搜索一条最优路径时，我们无法穷举所有路径，贪心策略找到的路径也不一定是最优路径。Beam Search就是指在每一步保留最好的几条路径。</p><p>有人说直接把模型输出的分布直接作为下次的输入，但其实这样的结果会比较差，因为这样无法区分出接近的分布。</p><h2 id="Object-Level-V-S-Component-Level"><a href="#Object-Level-V-S-Component-Level" class="headerlink" title="Object Level  V.S.  Component Level"></a>Object Level  V.S.  Component Level</h2><p>假如我们要生成一个sentence，那我们就应该关注整个sentence(Object Level)而不仅仅是每个word(Component Level)。</p><p>如果是按照Component Level，那使用Cross Entropy计算损失的话，训练前期loss会下降得很快，但后期loss会下降得很慢（“The dog is is fast”和”The dog is running fast”的loss的差距很小）。</p><p>那有没有一个损失函数可以基于Object Level衡量两个句子间的差异呢？目前是没有的，因为模型输出的分布是离散的，如果微小改变模型参数但保证模型输出的句子相同，那损失函数的输出就是一样的，即微小扰动并没有对loss产生影响。</p><p>那怎么办呢？</p><h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p>可以利用强化学习进行generation，每次生成一个word并不计算reward，知道生成整个sentence后才利用所生成的sentence和标注计算reward，详见《SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS》。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在训练一个可以产生句子的网络时，有哪些技巧呢？ &lt;/p&gt;
&lt;h2 id=&quot;Bad-Attention&quot;&gt;&lt;a href=&quot;#Bad-Attention&quot; class=&quot;headerlink&quot; title=&quot;Bad Attention&quot;&gt;&lt;/a&gt;Bad Attention&lt;/
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Seq2Seq" scheme="https://chouxianyu.github.io/tags/Seq2Seq/"/>
    
      <category term="Attention" scheme="https://chouxianyu.github.io/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-14.2 Seq2Seq：Attention</title>
    <link href="https://chouxianyu.github.io/2021/05/20/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-14-2-Seq2Seq%EF%BC%9AAttention/"/>
    <id>https://chouxianyu.github.io/2021/05/20/李宏毅机器学习课程笔记-14-2-Seq2Seq：Attention/</id>
    <published>2021-05-20T01:25:50.000Z</published>
    <updated>2021-05-20T01:34:50.565Z</updated>
    
    <content type="html"><![CDATA[<p>输入和输出都是sequence的任务都是一种Sequence-to-sequence Learning，简称<strong>Seq2Seq</strong>。</p><p>Attention其实是一种<strong>Dynamic Conditional Generation</strong>。在前文描述的Conditional Generation中，我们在每个时间点都将Encoder输出的vector输入到Decoder中，其实我们可以进一步使得Decoder在每个时间点接收到的vector都是不一样的，这就是Attention。Attention有两个好处，第一是应对Encoder输出的vector无法充分有效地表示整个setence的情况，第二是使Decoder在不同时间点关注setence的不同部分(假如要把“机器学习”翻译为“machine learning”，那么在翻译“机器”时就不需要关注“学习”这个暂时无关的word)。</p><h2 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h2><p><img src="https://pic1.zhimg.com/80/v2-ebcc94e5c0101e6dbecb7f5438890f50_720w.png" alt="img"></p><p>如上图所示，$h^1,h^2,h^3,h^4$是作为Encoder的RNN中隐藏层的输出，一种方法是$z$为Decoder的输出，$z^0$即时间点为0时的一个初始值。</p><ol><li><p>使用<code>match</code>函数计算$h^i$和$z^0$的匹配程度$\alpha_0^1,\alpha_0^2,\alpha_0^3,\alpha_0^4$，举例来讲$\alpha_0^1$中上标$1$表示$h^1$、下标$0$表示时间点为0。</p><p> <code>match</code>函数是自定义的，它可以只是一个cosine similarity也可以是一个神经网络等等，如果<code>match</code>函数中包括参数，那这些参数也是可以通过学习得到的。</p></li><li><p>将$\alpha_0^1,\alpha_0^2,\alpha_0^3,\alpha_0^4$输入到softmax得到$\hat\alpha_0^1,\hat\alpha_0^2,\hat\alpha_0^3,\hat\alpha_0^4$</p><p> softmax使得这4个值之和为1，有人说这一步也可以不做</p></li><li><p>计算Encoder的输出$c^0=\sum\limits_i\hat\alpha_0^ih^i$</p><p> 若$\hat\alpha_0^1=\hat\alpha_0^2=0.5,\ \hat\alpha_0^3=\hat\alpha_0^4=0$，则代表Encoder只考虑了“机器”二字而没有考虑“学习”二字，由此实现Attention</p></li><li><p>将$c^0$输入到Decoder得到</p><p> 得到$z^1$后也就得到了“machine”</p></li></ol><p>按照上述步骤，使用$h^i$和$z^1$计算出$\alpha_1^1,\alpha_1^2,\alpha_1^3,\alpha_1^4$，再用softmax计算出$\hat\alpha_1^1,\hat\alpha_1^2,\hat\alpha_1^3,\hat\alpha_1^4$，再计算出$c^1=\sum\limits_i\hat\alpha_1^ih^i$，再将$c^1$输入到Decoder得到$z^2$，也就得到了“learning”。</p><p>重复上述过程直到生成句子的结束（句号）。</p><h2 id="Speech-Recognition"><a href="#Speech-Recognition" class="headerlink" title="Speech Recognition"></a>Speech Recognition</h2><p>Attention也可以应用于语音识别。在语音识别中， 输入是声音信号（可以用vector sequence表示），输出是word sequence。</p><p>Attention在Speech Recognition中的应用类似于其在Machine Translation中的应用，算法步骤差不多，谷歌有一篇相关论文：《Listen, Attend  and Spell》</p><h2 id="Image-Caption"><a href="#Image-Caption" class="headerlink" title="Image Caption"></a>Image Caption</h2><p>Attention在Image Caption中的应用类似于其在Machine Translation中的应用，可以将图片的每个region视为Machine Translation任务中的一个character，有一篇相关文章是《Show, Attend and Tell: Neural  Image Caption Generation with Visual Attention》。</p><p>Attention不只可以做单张Image的Caption，还可以做多张图片/视频的Caption，相关论文如《Describing Videos by Exploiting Temporal Structure》。</p><h2 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h2><p>Memory Network是在Memory上做Attention。Memory Network最开始是被用在Reading Comprehension上， 也就是给机器看一个document，然后问机器一个question并让它给出answer。</p><ol><li><p>将document表示为多个vector</p><p> document由很多个sentence组成，可以用一个vector表示一个sentence，因此一个document可以表示为$x^1,x^2,\dots,x^N$</p><p> 这一步可以和后面的DNN一起训练</p></li><li><p>将question表示为一个vector $q$</p></li><li><p>计算$q$和每个$x^n$之间match的分数$\alpha_1,\alpha_2,\dots,\alpha_N$</p></li><li><p>计算$e=\sum\limits_{n=1}\limits^N\alpha_nx^n$得到extracted information</p><p> 这一步的作用是提取出和question相关的setence，由此实现Attention</p></li><li><p>将extracted information输入到DNN，输出对应的answer</p></li></ol><p>Memory Network还有一个更加复杂的版本。可以将document表示为不同的两组vector，即使用两个vector $h^n$和$x^n$表示document中的每个sentence，然后计算$q$和每个$x^n$之间match的分数$a_n$，此时extracted information为$e=\sum\limits_{n=1}\limits^N\alpha_nh^n$。将extracted information输入到DNN得到对应的answer，同时还可以将extracted information和问题$q$加起来更新$q$(这叫做Hopping)。Hopping可以重复很多次，这就像机器在反复思考。</p><p>相关论文：《End-To-End Memory Networks》</p><h2 id="Neural-Turing-Machine"><a href="#Neural-Turing-Machine" class="headerlink" title="Neural Turing Machine"></a>Neural Turing Machine</h2><p>Neural Turing Machine不只是在Memory上做Attention、不只是可以读取memory，它还可以根据<code>match</code> 函数的结果修改存储在memory中的内容。</p><ol><li><p>在Neural Turing Machine中，memory是一个vector sequence</p></li><li><p>在初始时，memory表示为$m_0^1,m_0^2,\dots,m_0^N$，然后有一组初始的attention weight：$\hat\alpha_0^1,\hat\alpha_0^2,\dots,\hat\alpha_0^N$，计算$r^0=\sum\hat\alpha_0^im_0^i$</p></li><li><p>将$r^0$和第一个时间点的输入$x^1$输入到一个网络$f$，得到输出$k^1,e^1,a^1$</p><p> 这个网络$f$其实就是一个controller，它可以是一个DNN、LSTM、GRU等等都可以。</p><p> $k^1$的作用就是产生attention：$\alpha^i_1=cos(m_0^i,k^1)$，然后用softmax处理$\alpha^i_1$即可得到$\hat\alpha_1^i$，这里讲的生成attention的方法$\alpha^i_1=cos(m_0^i,k^1)$是简化过的版本，真正的通过$k^1$计算得到$\alpha^i_1$的方法更为复杂。</p><p> 根据attention weight $\hat\alpha_1^i$和$e^1,a^1$就可以修改memory，$e^1$的作用是把memory中的值清空(erase)，$a^1$的作用是把新的值写到memory里。$m_1^i=m_0^i-\hat\alpha_1^ie^1\odot m_0^i+\hat\alpha_1^ia^1$，通常attention weight $\hat\alpha$的分布会比较sharp，即其中某一维接近1而其它维都接近0，因此可以控制$e^1,a^1$清空或写入memory中的哪个值。</p></li></ol><h2 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h2><p>Pointer Network用来求解凸包（Convex Hull）。</p><p>求解凸包问题中，输入和输出都是一个point sequence。</p><p>假如使用Seq2Seq求解凸包问题，每个point用其x和y坐标表示，那输入就是多个point，输出则是point的索引，这样不可行。如果Encoder是RNN则可以处理point数量不确定的输入，但假如在训练时输入中最多只有50个point，而如果测试时需要输出100个point，那Decoder就无法输出51-100，因此这种方法是不可行的。</p><p>我们可以使用Attention Model，假设有一个key $z^0$，使用$z^0$为输入中的每个point计算attention weight，选择attention weight最大的那个point，模型输出即该point的索引，这样不管输入了多少个point，都能正确输出point的索引。使用输出的point的x和y坐标计算得到$z^1$，然后用$z^1$计算输入中每个point的attention weight，并选择attention weight最大的那个point，输出该point的索引。重复以上过程直到输出<code>END</code>。</p><ul><li><p>Summarization</p><p>  可以将summarization理解为求解凸包一样的问题：从一个document中选择几个word，详见《Get To The Point: Summarization with Pointer-Generator Networks》。</p></li><li><p>Machine Translation</p><p>  比如在将英语翻译为法语时，一些word并不用进行翻译，通过Pointer Network提取出来直接用就好了。</p></li><li><p>Chat-bot</p><p>  比如用户说“我叫臭咸鱼”，就可以通过Pointer Network将“臭咸鱼”这个word直接提取出来。</p></li></ul><h2 id="Recursive-Network"><a href="#Recursive-Network" class="headerlink" title="Recursive Network"></a>Recursive Network</h2><p>Recursive Network是Recurrent Neural Network的泛化版本，Recurrent Neural Network其实是Recursive Network的subset。</p><p>如下图所示，以Sentiment Analysis为例，输入为一个word sequence，输出为sentiment（假设是5级），输入的word sequence（假设有4个word）经过word embedding可以表示为vector sequence $x^1, x^2, x^3, x^4$，输出是一个5维的vector。</p><p><img src="https://pic4.zhimg.com/80/v2-fd3a4dd362763337f77a89867e6934bf_720w.png" alt="img"></p><p>如上图所示，如果用Recurrent Neural Network实现Sentiment Analysis，初始有一个值$h^0$，我们的RNN是$f$，将$h^0,x^1$输入到$f$得到$h^1$，再将$h^1,x^2$输入到$f$得到$h^2$，再将$h^2,x^3$输入到$f$得到$h^3$，再将$h^3,x^4$输入到$f$中得到$h^4$，将$h^4$输入到模型$g$（可以是几个层）中得到最终的sentiment。</p><p>如上图所示，如果用Recursive Structure实现Sentiment Analysis，需要先确定word和模型输出之间的关系，比如$x^1$和$x^2$、$x^3$和$x^4$分别是两组，我们的模型是$f$，将$x^1,x^2$输入到$f$中得到$h^1$，将$x^3,x^4$输入到$f$中得到$h^2$，将$h^1,h^2$输入到$f$中得到$h^3$，再把$h^3$输入输入到模型$g$（可以是几个层）中得到最终的sentiment。因为模型$f$的输入可以是$x^i$或$h^i$，所以需要使得$x^i$和$h^i$的维度相同。</p><p>与Recursive Network相关的模型有：Recursive Nerual Tensor Network、Matrix-Vector Recursive Network、Tree LSTM。</p><p>除了Sentiment Analysis，Recursive Network还可以用来处理其它与sentence相关的任务。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;输入和输出都是sequence的任务都是一种Sequence-to-sequence Learning，简称&lt;strong&gt;Seq2Seq&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Attention其实是一种&lt;strong&gt;Dynamic Conditional Generati
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Seq2Seq" scheme="https://chouxianyu.github.io/tags/Seq2Seq/"/>
    
      <category term="Attention" scheme="https://chouxianyu.github.io/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-14.1 Seq2Seq：Conditional Generation</title>
    <link href="https://chouxianyu.github.io/2021/05/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-14-1-Seq2Seq%EF%BC%9AConditional-Generation/"/>
    <id>https://chouxianyu.github.io/2021/05/19/李宏毅机器学习课程笔记-14-1-Seq2Seq：Conditional-Generation/</id>
    <published>2021-05-19T02:02:27.000Z</published>
    <updated>2021-05-19T02:06:21.675Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基于RNN的Generation"><a href="#基于RNN的Generation" class="headerlink" title="基于RNN的Generation"></a>基于RNN的Generation</h2><p>可以用RNN生成一个word、sentence、图片等等。</p><p>一个word有多个character组成，因此RNN每次生成一个character。一个sentence由多个word组成，因此RNN每次生成一个word。一张图片由多个像素组成，因此RNN每次生成一个像素。</p><p><img src="https://pic2.zhimg.com/80/v2-70729446c3202faedf198bb178ba5772_720w.png" alt="img"></p><p>从上到下、从左到右逐个生成像素这种方法(上图右上角)并没有充分考虑pixel之间的位置关系，有一种更充分地考虑了pixel之间位置关系的方法(上图右下角)叫做PixelRNN，PixelRNN根据多个邻居生成一个像素，这可以通过3维LSTM单元(上图左上角)实现。如上图左上角所示，3维LSTM单元有3组输入和3组输出，将几层(每层9个)3维LSTM单元排列在一起，就可以生成一张3×3的图片。</p><p>以下为一些使用RNN进行Generation的论文。</p><p><img src="https://pic1.zhimg.com/80/v2-be4bacf8ceeb49812e97cddcbc82fff3_720w.png" alt="img"></p><h2 id="Conditional-Generation"><a href="#Conditional-Generation" class="headerlink" title="Conditional Generation"></a>Conditional Generation</h2><p>只使用RNN进行Generation的话是不够的，因为我们希望生成的结果并不是随机的，比如我们希望机器生成的sentence是合乎情境的，假如我说了“Hello”，那机器就应该说“Nice to meet you”之类的内容，这就是<strong>Conditional Generation</strong>。</p><p>为了实现Conditional Generation，我们可以将<strong>condition</strong>转换成vector输入到RNN中。在Chat-bot和Machine Translation任务中，condition就是一个sentence；在Image Caption任务中，condition就是一张图片。</p><p>在实现Conditional Generation时通常使用<strong>Encoder-Decoder</strong>框架，其中Encoder负责将condition转换为一个vector、Decoder负责将condition vector转换成最后的输出。Encoder和Decoder通常是一起训练(jointly train)的，两者的参数可以相同也可以不同。</p><p>在Chat-bot和Machine Translation任务中，输入和输出都是sequence，所以这类任务都是一种Sequence-to-sequence Learning，简称<strong>Seq2Seq</strong>。</p><p>在Chat-bot任务中，机器应该要考虑聊天记录，比如机器说“Hello”然后我回复“Hi”，如果这时机器也回复“Hi”之类的就很智障了，所以机器需要考虑更长的context，如果用户说过了“我叫臭咸鱼”，机器就不应该再问“你的名字是什么/你叫什么”了。因此有一种做法可以是我们有一个双层的Encoder，首先用Encoder的第一层讲聊天记录作为condition表示为vector，然后输入到Encoder的第二层。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基于RNN的Generation&quot;&gt;&lt;a href=&quot;#基于RNN的Generation&quot; class=&quot;headerlink&quot; title=&quot;基于RNN的Generation&quot;&gt;&lt;/a&gt;基于RNN的Generation&lt;/h2&gt;&lt;p&gt;可以用RNN生成一个word
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Seq2Seq" scheme="https://chouxianyu.github.io/tags/Seq2Seq/"/>
    
      <category term="循环神经网络" scheme="https://chouxianyu.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-13.6模型压缩代码实战</title>
    <link href="https://chouxianyu.github.io/2021/05/08/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-13-6%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/"/>
    <id>https://chouxianyu.github.io/2021/05/08/李宏毅机器学习课程笔记-13-6模型压缩代码实战/</id>
    <published>2021-05-07T23:59:38.000Z</published>
    <updated>2021-05-08T00:14:21.040Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework7的记录。</p><p>全部课程PPT、数据和代码下载链接：</p><p>链接：<a href="https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg" target="_blank" rel="noopener">https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg</a> 提取码：tpmc</p><p>代码仓库：<a href="https://github.com/chouxianyu/LHY_ML2020_Codes" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes</a></p><h1 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h1><p>通过Architecture Design、Knowledge Distillation、Network Pruning和Weight Quantization这4种模型压缩策略，用一个非常小的model完成homework3中食物图片分类的任务。</p><h2 id="1-Architecture-Design"><a href="#1-Architecture-Design" class="headerlink" title="1.Architecture Design"></a>1.Architecture Design</h2><p>MobileNet提出了Depthwise &amp; Pointwise Convolution。我们在这里实现MobileNet v1这个比较小的network，后续使用Knowledge Distillation策略训练它，然后对它进行剪枝和量化。</p><h2 id="2-Knowledge-Distillation"><a href="#2-Knowledge-Distillation" class="headerlink" title="2.Knowledge Distillation"></a>2.Knowledge Distillation</h2><p>将ResNet18作为Teacher Net(使用torchvision中的ResNet18，仅将num_classes改成11，加载助教训练好的Accuracy约为88.4%的参数)，将上一步(1.Architecture Design)设计的小model作为Student Net，使用Knowledge_Distillation策略训练Student Net。</p><p>Loss计算方法为$Loss = \alpha T^2 \times KL(\frac{\text{Teacher’s Logits}}{T} || \frac{\text{Student’s Logits}}{T}) + (1-\alpha)(\text{Original Loss})$，关于为什么要对student进行logsoftmax可见<a href="https://github.com/peterliht/knowledge-distillation-pytorch/issues/2" target="_blank" rel="noopener">https://github.com/peterliht/knowledge-distillation-pytorch/issues/2</a></p><p>论文《Distilling the Knowledge in a Neural Network》：<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">https://arxiv.org/abs/1503.02531</a></p><h2 id="3-Network-Pruning"><a href="#3-Network-Pruning" class="headerlink" title="3.Network Pruning"></a>3.Network Pruning</h2><p>对上一步(2.Knowledge_Distillation)训练好的Student Net做剪枝。</p><p>根据论文《Learning Efficient Convolutional Networks through Network Slimming》，论文链接：<a href="https://arxiv.org/abs/1708.06519" target="_blank" rel="noopener">https://arxiv.org/abs/1708.06519</a><br>BatchNorm层中的gamma值和一些特定卷积核（或者全连接层的一个神经元）相关联，因此可以使用BatchNorm层中的gamma值判断相关通道的重要性。</p><p>Student Net中CNN部分有几个结构相同的Sequential，其结构、权重名称、实现代码、权重形状如下表所示。</p><div class="table-container"><table><thead><tr><th style="text-align:center">#</th><th style="text-align:left">name</th><th style="text-align:left">meaning</th><th>code</th><th>weight shape</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:left">cnn.{i}.0</td><td style="text-align:left">Depthwise Convolution</td><td>nn.Conv2d(x, x, 3, 1, 1, group=x)</td><td>(x, 1, 3, 3)</td></tr><tr><td style="text-align:center">1</td><td style="text-align:left">cnn.{i}.1</td><td style="text-align:left">Batch Normalization</td><td>nn.BatchNorm2d(x)</td><td>(x)</td></tr><tr><td style="text-align:center">2</td><td style="text-align:left"></td><td style="text-align:left">ReLU6</td><td>nn.ReLU6</td><td></td></tr><tr><td style="text-align:center">3</td><td style="text-align:left">cnn.{i}.3</td><td style="text-align:left">Pointwise Convolution</td><td>nn.Conv2d(x, y, 1),</td><td>(y, x, 1, 1)</td></tr><tr><td style="text-align:center">4</td><td style="text-align:left"></td><td style="text-align:left">MaxPooling</td><td>nn.MaxPool2d(2, 2, 0)</td></tr></tbody></table></div><p>独立剪枝prune_count次，每次剪枝的剪枝率按prune_rate逐渐增大，剪枝后微调finetune_epochs个epoch。</p><h2 id="4-Weight-Quantization"><a href="#4-Weight-Quantization" class="headerlink" title="4.Weight Quantization"></a>4.Weight Quantization</h2><p>对第二步(2.Knowledge_Distillation)训练好的Student Net做量化（用更少的bit表示一个value）。</p><p>torch预设的FloatTensor是32bit，而FloatTensor最低可以是16bit。</p><p>如何将32bit转成8bit的int呢？对每个weight进行min-max normalization，然后乘以$2^8-1$再四舍五入成整数，这样就可以转成uint8了。</p><h1 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h1><p>数据集为homework3中食物图片分类数据集。</p><p>11个图片类别，训练集中有9866张图片，验证集中有3430张图片，测试集中有3347张图片。</p><p>训练集和验证集中图片命名格式为<code>类别_编号.jpg</code>，编号不重要。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/chouxianyu/LHY_ML2020_Codes/tree/master/hw7_NetworkCompression" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes/tree/master/hw7_NetworkCompression</a></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework7的记录。&lt;/p&gt;
&lt;p&gt;全部课程PPT、数据和代码下载链接：&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg&quot; target
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型压缩" scheme="https://chouxianyu.github.io/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"/>
    
      <category term="知识蒸馏" scheme="https://chouxianyu.github.io/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"/>
    
      <category term="网络剪枝" scheme="https://chouxianyu.github.io/tags/%E7%BD%91%E7%BB%9C%E5%89%AA%E6%9E%9D/"/>
    
      <category term="参数量化" scheme="https://chouxianyu.github.io/tags/%E5%8F%82%E6%95%B0%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-13.5模型压缩之动态计算</title>
    <link href="https://chouxianyu.github.io/2021/05/07/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-13-5%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97/"/>
    <id>https://chouxianyu.github.io/2021/05/07/李宏毅机器学习课程笔记-13-5模型压缩之动态计算/</id>
    <published>2021-05-07T03:25:13.000Z</published>
    <updated>2021-05-07T03:25:50.584Z</updated>
    
    <content type="html"><![CDATA[<p>动态计算（Dynamic Computation）就是资源充足时就做到最好，资源不足时就减少运算量、先求有再求好(但也不要太差)。</p><p>一种方法是训练多个从小到大的model，然后选择合适的模型，这样的问题是需要存储很多个model。</p><p>另外一种方法是，<strong>训练一个在中间层就可以得到最终结果的model</strong>。因为网络浅层和深层提取到的特征一般分别是低级特征和高级特征，所以在网络浅层得到的结果一般要比在网络深层得到的结果差一些。在网络浅层就计算最终结果可能会迫使网络浅层学习一些高级特征，这会破坏网络从浅层到深层逐步提取低/高级特征的架构。那如何处理这些问题呢？可以看一看Multi-Scale Dense Convolutional Networks：<a href="https://arxiv.org/abs/1703.09844" target="_blank" rel="noopener">https://arxiv.org/abs/1703.09844</a></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;动态计算（Dynamic Computation）就是资源充足时就做到最好，资源不足时就减少运算量、先求有再求好(但也不要太差)。&lt;/p&gt;
&lt;p&gt;一种方法是训练多个从小到大的model，然后选择合适的模型，这样的问题是需要存储很多个model。&lt;/p&gt;
&lt;p&gt;另外一种方法是
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型压缩" scheme="https://chouxianyu.github.io/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"/>
    
      <category term="动态计算" scheme="https://chouxianyu.github.io/tags/%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-13.4模型压缩之架构设计</title>
    <link href="https://chouxianyu.github.io/2021/05/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-13-4%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    <id>https://chouxianyu.github.io/2021/05/06/李宏毅机器学习课程笔记-13-4模型压缩之架构设计/</id>
    <published>2021-05-06T00:23:13.000Z</published>
    <updated>2021-05-06T00:38:46.076Z</updated>
    
    <content type="html"><![CDATA[<p>调整Network的架构设计（Architecture Design），让它变得只需要比较少的参数，这是在实际操作中最有效的做法。</p><h2 id="Low-Rank-Approximation"><a href="#Low-Rank-Approximation" class="headerlink" title="Low Rank Approximation"></a>Low Rank Approximation</h2><p>如果是Fully Connected Network，前一层和后一层分别有N、M个neuron则需要$N\times M$个参数，我们可以在这两层中间加一个有K个neuron的层(不要激活函数)就需要$K(N+M)$个neuron，这样就可以减少参数量，但是根据线性代数的知识可知这3层的性能不一定比之前的2层好。</p><h2 id="Depthwise-amp-Pointwise-Convolution"><a href="#Depthwise-amp-Pointwise-Convolution" class="headerlink" title="Depthwise &amp; Pointwise Convolution"></a>Depthwise &amp; Pointwise Convolution</h2><p>在普通卷积中，每个filter（卷积核）要处理输入的所有channel。假设输入有$I$个channel，有$O$个尺寸为$k\times k$的filter，则需要$(k\times k\times I)\times O$个参数、输出$O$个channel。</p><p>深度可分离卷积(Depthwise Separable Convolution)又称为<strong>Depthwise&amp;Pointwise Convolution</strong>，分为以下2步，共需要$k\times k\times I+I\times O$个参数、输出$O$个channel。</p><ol><li><p>Depthwise Convolution</p><p> 在这一步中，filter的数量等于输入channel的数量，即<strong>每个filter只处理一个channel，这步的作用就是修改输入的尺寸</strong>。</p><p> 假设输入有$I$个channel，因此就有$I$个filter；假设每个filter的尺寸为$k\times k$，则需要$(k\times k\times1)\times I$个参数、输出$I$个channel。</p></li><li><p>Pointwise Convolution</p><p> 在这一步中，以上一步(Depthwise Convolution)的输出作为输入，<strong>每个filter的尺寸必须为$1\times 1$，和普通卷积核一样要处理输入的所有channel，这步的作用就是修改输入的通道数</strong>。</p><p> 假设有$O$个filter，则需要$(1\times1\times I)\times O$个参数、输出$O$个channel。</p></li></ol><h2 id="Group-Convolution"><a href="#Group-Convolution" class="headerlink" title="Group Convolution"></a>Group Convolution</h2><p>Group Convolution就是把输入的多个channel分成多个group，对每个group分别进行一次或多次普通卷积。Group Convolution算是普通卷积和Depthwise Convolution的折衷，当group数量和输入的通道数相同时它就相当于Depthwise Convolution，当group数量为1时它就相当于普通卷积。</p><h2 id="不同卷积的PyTorch实现"><a href="#不同卷积的PyTorch实现" class="headerlink" title="不同卷积的PyTorch实现"></a>不同卷积的PyTorch实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通卷积, weight数量 = in_chs * out_chs * kernel_size^2</span></span><br><span class="line">nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Group Convolution, Group数量可以自行控制，表示要分成几个group，其中in_chs和out_chs必须可以被groups整除</span></span><br><span class="line">nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, groups=groups)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Depthwise Convolution, 输入通道数=输出通道数=group数量, weight数量 = in_chs * kernel_size^2</span></span><br><span class="line">nn.Conv2d(in_chs, out_chs=in_chs, kernel_size, stride, padding, groups=in_chs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pointwise Convolution, 也就是1×1卷积, weight数量 = in_chs * out_chs</span></span><br><span class="line">nn.Conv2d(in_chs, out_chs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>Depthwise Separable Convolution被广泛地用在各种小型网络中：SqueezeNet、MobileNet、ShuffleNet、Xception，其中最知名的为MobileNet。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;调整Network的架构设计（Architecture Design），让它变得只需要比较少的参数，这是在实际操作中最有效的做法。&lt;/p&gt;
&lt;h2 id=&quot;Low-Rank-Approximation&quot;&gt;&lt;a href=&quot;#Low-Rank-Approximation&quot; c
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型压缩" scheme="https://chouxianyu.github.io/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-13.3模型压缩之参数量化</title>
    <link href="https://chouxianyu.github.io/2021/05/05/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-13-3%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E5%8F%82%E6%95%B0%E9%87%8F%E5%8C%96/"/>
    <id>https://chouxianyu.github.io/2021/05/05/李宏毅机器学习课程笔记-13-3模型压缩之参数量化/</id>
    <published>2021-05-05T01:49:34.000Z</published>
    <updated>2021-05-05T01:54:04.032Z</updated>
    
    <content type="html"><![CDATA[<p>参数量化就是Parameter Quantization。</p><ol><li><p>用更少的bit表示一个value</p><p> 比如说本来用32位表示一个weight，现在用16位表示一个weight，这样就缩小了一半。</p></li><li><p>Weight Clustering</p><p> 根据weight的值对weight进行聚类，每个类中的weight都用同一个value(比如该类中所有weight的平均值)表示。每个类有个id，2个bit就可以表示4个类的id(再进一步还可以使用哈夫曼编码)，在存储时只需要存储每个weight所属的类的id以及每个类对应的value即可。</p><p> 因为每个类中的weight都用了同一个value表示，所以模型会有一些精度损失。</p></li><li><p>Binary Weights</p><p> weight的值只有±1。</p><p> 有不少研究者提出直接训练一个Binary Network，最早的是Binary Connect（<a href="http://arxiv.org/abs/1511.00363），其它的还有Binary" target="_blank" rel="noopener">http://arxiv.org/abs/1511.00363），其它的还有Binary</a> Network（<a href="https://arxiv.org/abs/1602.02830）、XNOR-Net（https://arxiv.org/abs/1603.05279）。" target="_blank" rel="noopener">https://arxiv.org/abs/1602.02830）、XNOR-Net（https://arxiv.org/abs/1603.05279）。</a></p><p> Binary Connect在训练中有2个分别使用real value和binary value的model，暂称为R和B。首先初始化R的参数，然后找到和R最接近的B，再使用B的梯度更新R的参数，然后再找到和R最接近的B，循环该过程直到停止，最后就使用最终的B。</p><p> Binary Connect其实像是一种Regularization，它约束weight的值必须是±1。</p></li></ol><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参数量化就是Parameter Quantization。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;用更少的bit表示一个value&lt;/p&gt;
&lt;p&gt; 比如说本来用32位表示一个weight，现在用16位表示一个weight，这样就缩小了一半。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型压缩" scheme="https://chouxianyu.github.io/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"/>
    
      <category term="参数量化" scheme="https://chouxianyu.github.io/tags/%E5%8F%82%E6%95%B0%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-13.2模型压缩之知识蒸馏</title>
    <link href="https://chouxianyu.github.io/2021/05/04/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-13-2%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"/>
    <id>https://chouxianyu.github.io/2021/05/04/李宏毅机器学习课程笔记-13-2模型压缩之知识蒸馏/</id>
    <published>2021-05-04T01:37:48.000Z</published>
    <updated>2021-05-04T01:47:41.640Z</updated>
    
    <content type="html"><![CDATA[<p>知识蒸馏就是Knowledge Distillation。</p><p>Knowledge Distillation：<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">https://arxiv.org/abs/1503.02531</a></p><p>Do Deep Nets Really Need to be Deep?：<a href="https://arxiv.org/abs/1312.6184" target="_blank" rel="noopener">https://arxiv.org/abs/1312.6184</a></p><p>熟悉YOLO的读者，可以根据这个仓库感受一下剪枝和知识蒸馏：<a href="https://github.com/tanluren/yolov3-channel-and-layer-pruning" target="_blank" rel="noopener">https://github.com/tanluren/yolov3-channel-and-layer-pruning</a></p><h2 id="Student-and-Teacher"><a href="#Student-and-Teacher" class="headerlink" title="Student and Teacher"></a>Student and Teacher</h2><p>什么是Knowledge Distillation？</p><p>我们可以让一个较小的<strong>Student Net</strong>向较大的<strong>Teacher Net</strong>学习，使得Student Net的输出尽可能接近Teacher Net的输出。</p><p>普通的训练方式为仅在数据集上训练Student Net，而Knowledge Distillation的思路是：即使Teacher Net的输出并不一定是正确的，但<strong>Teacher Net可以提供一些数据集无法提供的信息</strong>，比如手写数字图片分类模型Teacher Net的输出为“1：0.7，7：0.2，9：0.1”，这不仅说明这张图片像1，<strong>还可以说明1和7、9很相似</strong>。</p><p>或者可以这么理解：学生自己直接做题目太难了，让学生学习下老师是怎么想的可能会更好。</p><h2 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h2><p>Knowledge Distillation有什么用呢？</p><p>打Kaggle比赛时很多人的做法是ensemble（将多个model的结果进行平均）。Ensemble通常可以得到更好的精度，但现实中设备上不可能放这么多个model，这时就可以利用Knowledge Distillation让Student Net向Teacher Net学习，最终设备上只运行Student Net就可以。</p><h2 id="Temperature"><a href="#Temperature" class="headerlink" title="Temperature"></a>Temperature</h2><p>在分类任务中，网络的最后一般有个softmax函数：$y_i=\frac{e^{x_i}}{\sum_je^{x^j}}$，其中$y_i$是输入属于类别$i$的置信度。</p><p>在Knowledge Distillation中，我们需要对softmax函数进行调整：$y_i=\frac{e^{\frac{x_i}{T}}}{\sum_je^{\frac{x^j}{T}}}$，其中$T$为Temperature，一般是一个大于1的数，它的作用是使得Teacher Net输出的属于各个类别的置信度更加接近，如下图所示。</p><p><img src="https://pic1.zhimg.com/80/v2-c06e6c918f5c8ac800f28f35deca8473_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;知识蒸馏就是Knowledge Distillation。&lt;/p&gt;
&lt;p&gt;Knowledge Distillation：&lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;htt
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型压缩" scheme="https://chouxianyu.github.io/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"/>
    
      <category term="知识蒸馏" scheme="https://chouxianyu.github.io/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-13.1模型压缩之网络剪枝</title>
    <link href="https://chouxianyu.github.io/2021/05/02/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-13-1%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%89%AA%E6%9E%9D/"/>
    <id>https://chouxianyu.github.io/2021/05/02/李宏毅机器学习课程笔记-13-1模型压缩之网络剪枝/</id>
    <published>2021-05-01T22:24:53.000Z</published>
    <updated>2021-05-01T22:40:07.243Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>网络剪枝（Network Pruning）就是删除一个较大网络中的一些weight或neuron得到一个更小的网络。</p><p>我们相信，<strong>通常情况下我们训练出的神经网络是over-parameterized</strong>，即其中存在很多weight或neuron是没有用的(比如有些neuron的输出总是0、有些weight非常接近0) ，因此我们可以把这些没有用的weight或neuron剪掉。</p><p>在90年代，Yann Le Cun就提出了“网络剪枝”，paper名称为Optimal Brain Damage。</p><p>有个问题是：为什么不直接使用较小Network而是对较大Network进行剪枝？常见的解释是：较小的Network训练出来的结果一般都不好，而较大的Network更容易optimize（李老师这个视频有讲解为什么：<a href="https://www.youtube.com/watch?v=_VuWvQUMQVk）。在训练神经网络时可能会遇到local" target="_blank" rel="noopener">https://www.youtube.com/watch?v=_VuWvQUMQVk）。在训练神经网络时可能会遇到local</a> minima和saddle point的问题，但如果Network够大这种问题就会不那么严重，现在有很多文献甚至可以证明只要Network够大就可以用梯度下降找到global optimal。</p><h2 id="How-to-Prune-a-Network"><a href="#How-to-Prune-a-Network" class="headerlink" title="How to Prune a Network"></a>How to Prune a Network</h2><ol><li><p>训练出一个较大的Network</p></li><li><p>评估该Network中每个weight和neuron的重要性</p><p> 这一步有很多种做法</p><ul><li><p>weight的重要性</p><p>  比如：如果其值接近0，则说明该weight不重要，因此可以计算weight的L1或L2判断weight的重要性。</p></li><li><p>neuron的重要性</p><p>  比如：给定dataset，如果某个neural的输出都是0那么该neural是不那么重要的</p></li></ul></li><li><p>根据重要性将weight和neuron排序并删除那些不那么重要的weight和neuron</p><p> 删除一些weight和neuron后，Network会变小但精度一般也会变低，因此还需要进行fine-tune</p><p> 一次最好不要删除太多neuron或weight，否则Network的精度会无法通过fine-tune恢复，最好是每次只删除一小部分然后进行fine-tune并重复该过程</p></li><li><p>fine-tune</p><p> 训练剪枝得到的较小的网络</p></li></ol><p>熟悉YOLO的读者，可以根据这个仓库（<a href="https://github.com/tanluren/yolov3-channel-and-layer-pruning）感受一下剪枝和知识蒸馏。" target="_blank" rel="noopener">https://github.com/tanluren/yolov3-channel-and-layer-pruning）感受一下剪枝和知识蒸馏。</a></p><h2 id="Lottery-Ticket-Hypothesis"><a href="#Lottery-Ticket-Hypothesis" class="headerlink" title="Lottery Ticket Hypothesis"></a>Lottery Ticket Hypothesis</h2><p>论文链接：<a href="https://arxiv.org/abs/1803.03635，这是ICLR2019的一篇论文" target="_blank" rel="noopener">https://arxiv.org/abs/1803.03635，这是ICLR2019的一篇论文</a></p><p>如下图所示，现有一个较大网络A，随机初始化其参数并记该参数为W，训练该较大网络A并进行剪枝得到较小网络B。有个现象是：如果我们随机初始化较小网络B的参数并进行训练，得到的结果就不行；但如果使用参数W中的对应参数初始化，得到的结果就可以。</p><p><img src="https://pic4.zhimg.com/80/v2-6bbbbfc39567d8a138f36ac44ef94849_720w.png" alt="img"></p><h2 id="Rethinking-the-Value-of-Network-Pruning"><a href="#Rethinking-the-Value-of-Network-Pruning" class="headerlink" title="Rethinking the Value of Network Pruning"></a>Rethinking the Value of Network Pruning</h2><p>论文链接：<a href="https://arxiv.org/abs/1810.05270，这是ICLR2019的一篇论文" target="_blank" rel="noopener">https://arxiv.org/abs/1810.05270，这是ICLR2019的一篇论文</a></p><p>这篇论文的结论和Lottery Ticket Hypothesis一文相反：现有剪枝后的网络，将其参数随机初始化是可以训练出好的结果的。</p><p>ICLR2019的review是开放的，网上可以搜到两篇文章作者的讨论，知乎上也有关于这两篇论文的讨论，后续也有人做了相关研究。</p><h2 id="Some-Issue-in-Weight-Pruning"><a href="#Some-Issue-in-Weight-Pruning" class="headerlink" title="Some Issue in Weight Pruning"></a>Some Issue in Weight Pruning</h2><p>如果是weight pruning，那剪枝后的Network会变得不规则（比如有些neuron有2个weight而有些neuron有4个weight）。这样的不规则的Network是不好用keras等代码框架实现的，并且GPU只能对矩阵运算进行加速而无法加速这样的Network。比较常见的实做方法是将需要剪掉的weight设成0，因此仍然可以用GPU加速，但这样其实并没有使网络变小。</p><p>实际上做weight pruning是很麻烦的，通常都进行neuron pruning，因为比较容易用代码实现、也容易达到加速的目的。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;网络剪枝（Network Pruning）就是删除一个较大网络中的一些wei
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型压缩" scheme="https://chouxianyu.github.io/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"/>
    
      <category term="网络剪枝" scheme="https://chouxianyu.github.io/tags/%E7%BD%91%E7%BB%9C%E5%89%AA%E6%9E%9D/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-12.4对抗攻击代码实战</title>
    <link href="https://chouxianyu.github.io/2021/05/01/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-12-4%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/"/>
    <id>https://chouxianyu.github.io/2021/05/01/李宏毅机器学习课程笔记-12-4对抗攻击代码实战/</id>
    <published>2021-05-01T00:13:08.000Z</published>
    <updated>2021-05-01T00:29:31.272Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework6的记录。</p><p>全部课程PPT、数据和代码下载链接：</p><p>链接：<a href="https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg" target="_blank" rel="noopener">https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg</a> 提取码：tpmc</p><p>代码仓库：<a href="https://github.com/chouxianyu/LHY_ML2020_Codes" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes</a></p><ul><li><p>任务描述</p><p>  选择一个Proxy Network实现<strong>Black Box</strong> Attack，通过FGSM(Fast Gradient Sign Method)实现Non-targeted Adversial Attack。</p></li><li><p>数据集描述</p><p>  有200张图片，命名格式为<code>编号.png</code>，尺寸为224×224。</p><p>  categories.csv：1000个类别，索引为[0,999]，</p><p>  labels.csv：每张图片的信息(包括类别索引)</p></li><li><p>评估指标</p><ul><li>所有输入图片$x^0$和攻击图片$x’$的L-infinity的平均值</li><li>攻击的成功率</li></ul></li><li><p>结果</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Original Proxy Network Accuracy: <span class="number">0.865</span></span><br><span class="line">After Attack(epsilon: <span class="number">0.1</span>) Accrucy: <span class="number">0.03</span></span><br><span class="line">Original Proxy Network Accuracy: <span class="number">0.865</span></span><br><span class="line">After Attack(epsilon: <span class="number">0.01</span>) Accrucy: <span class="number">0.27</span></span><br></pre></td></tr></table></figure><p>  使用预训练的VGG16作为Proxy Network，可知在攻击前Proxy Nerwork的准确率为0.865，而攻击后准确率为0.03(epsilon为0.1)、0.27(epsilon为0.01)</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework6的记录。&lt;/p&gt;
&lt;p&gt;全部课程PPT、数据和代码下载链接：&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg&quot; target
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
      <category term="对抗攻击与防御" scheme="https://chouxianyu.github.io/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B8%8E%E9%98%B2%E5%BE%A1/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-12.3对抗防御入门</title>
    <link href="https://chouxianyu.github.io/2021/04/30/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-12-3%E5%AF%B9%E6%8A%97%E9%98%B2%E5%BE%A1%E5%85%A5%E9%97%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/30/李宏毅机器学习课程笔记-12-3对抗防御入门/</id>
    <published>2021-04-30T01:55:55.000Z</published>
    <updated>2021-05-01T00:17:37.833Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Defense"><a href="#What-is-Defense" class="headerlink" title="What is Defense"></a>What is Defense</h2><p>有人说模型容易被攻破是因为过拟合，但其实并不是，因为weight regularization、dropout、model ensemble都不能抵挡Adversarial Attack，并且Attack可以攻击多个model。</p><p>Defense分为两类：</p><ul><li><p>Passive Defense</p><p>  不修改模型，而是在模型前加一个filter防止模型被攻击，其实这是Anomaly Detection的一个特例。</p></li><li><p>Proactive Defense</p><p>  训练模型时就对Attack进行防御</p></li></ul><p>如果攻击者知道Defense的具体实现，那攻击者一般仍然可以将Defense攻破。</p><h2 id="Passive-Defense"><a href="#Passive-Defense" class="headerlink" title="Passive Defense"></a>Passive Defense</h2><p>在模型前加一个filter防止模型被攻击，filter的作用就是扰动攻击信号$\Delta x$使其无效，这个filter并不需要很复杂，有时smoothing就可以，还有Gaussian Filter、Median Filter、Bilateral Filter等等。</p><ul><li><p>Feature Squeeze</p><p>  将图片输入模型得到输出P1，然后分别使用2个Squeezer对图片进行处理后再输入到模型得到输出P2、P3，如果P1和P2的差距、P2和P3的差距超过了某个值就判断该图片是攻击图片。</p><p>  详见：Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks(<a href="https://arxiv.org/abs/1704.01155" target="_blank" rel="noopener">https://arxiv.org/abs/1704.01155</a>)</p></li><li><p>Randomization at Inference Phase</p><p>  将图片随机稍微缩放然后随机padding，然后随机选择其中一个结果输入到模型。</p><p>  详见：Mitigating Adversarial Effects Through Randomization(<a href="https://arxiv.org/abs/1711.01991" target="_blank" rel="noopener">https://arxiv.org/abs/1711.01991</a>)</p></li></ul><h2 id="Proactive-Defense"><a href="#Proactive-Defense" class="headerlink" title="Proactive Defense"></a>Proactive Defense</h2><p>在训练模型时就找出模型的漏洞并进行改善。</p><p>首先用训练集训练模型，然后多次迭代，在每次迭代中使用某种攻击方法分别找到每个训练集样本对应的攻击样本$x’$，然后把这些攻击样本添加到训练集中进行训练(这有点像数据增强)，多次迭代的原因是基于新的训练集训练后模型可能会产生新的漏洞。</p><p>注意：假如在Proactive Defense时我们使用的攻击方法为A，那我们的模型也许可以防御他人的A攻击，但仍无法防御其它攻击方法。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;What-is-Defense&quot;&gt;&lt;a href=&quot;#What-is-Defense&quot; class=&quot;headerlink&quot; title=&quot;What is Defense&quot;&gt;&lt;/a&gt;What is Defense&lt;/h2&gt;&lt;p&gt;有人说模型容易被攻破是因为过拟合，但
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="对抗攻击与防御" scheme="https://chouxianyu.github.io/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B8%8E%E9%98%B2%E5%BE%A1/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-12.2对抗攻击进阶</title>
    <link href="https://chouxianyu.github.io/2021/04/28/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-12-2%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E8%BF%9B%E9%98%B6/"/>
    <id>https://chouxianyu.github.io/2021/04/28/李宏毅机器学习课程笔记-12-2对抗攻击进阶/</id>
    <published>2021-04-28T02:35:42.000Z</published>
    <updated>2021-04-28T02:36:53.301Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Universal-Adversarial-Attack"><a href="#Universal-Adversarial-Attack" class="headerlink" title="Universal Adversarial Attack"></a>Universal Adversarial Attack</h2><p>Attack可以是分别为每个输入$x^0$找到对应的攻击信号$\Delta x$，还可以是找到一个适用于所有输入的攻击信号$\Delta x$，这就是Universal Adversarial Attack，详见：Universal adversarial perturbations(<a href="https://arxiv.org/abs/1610.08401)。" target="_blank" rel="noopener">https://arxiv.org/abs/1610.08401)。</a></p><h2 id="One-Pixel-Attack"><a href="#One-Pixel-Attack" class="headerlink" title="One Pixel Attack"></a>One Pixel Attack</h2><p>One Pixel Attack就是攻击时的constraint为只能修改图片中一个pixel，即$d(x^0,x’)=||x^0-x’||_0=||\Delta x||_0\leq1$，其中可以将L0范数理解为非零元素的个数。</p><p>如何确定要攻击哪个像素呢？暴力求解速度会很慢。还有个问题是我们是否需要最佳的攻击方案？如果能够使用Loss梯度下降找到最佳的攻击方案，那当然是好的，但其实我们只需攻击成功即可，因为攻击的目标主要是使模型失效，只要可以攻击成功就行。比如原模型对某图片的分类结果是置信度为16.48%的Cup，我们能找到一个方案使模型分类结果是置信度为16.74%的Soup Bowl即可而不需要必须使得置信度为100%。同理，我们只要能找到某个可以击破的像素就行而并不需要找到最薄弱的像素。</p><p>那如何找到一个攻击方案呢？用Differential Evolution就行。Differential Evolution的好处是有较大的概率得到全局最优解并且不需要计算梯度也就不需要被攻击模型的参数。Differential Evolution与遗传算法非常类似，都包括变异、杂交和选择操作，但这些操作的具体定义与遗传算法有所不同。</p><p>详见One pixel attack for fooling deep neural networks(<a href="https://arxiv.org/abs/1710.08864)。" target="_blank" rel="noopener">https://arxiv.org/abs/1710.08864)。</a></p><h2 id="Adversarial-Reprogramming"><a href="#Adversarial-Reprogramming" class="headerlink" title="Adversarial Reprogramming"></a>Adversarial Reprogramming</h2><p>我们可以在不改变模型参数的情况下，通过Attack来修改模型的“功能”，比如将一个图片分类模型的功能改为方块计数(方块数量对应某种种类)，详见：</p><ul><li>Adversarial Reprogramming of Neural Networks(<a href="https://arxiv.org/abs/1806.11146v2" target="_blank" rel="noopener">https://arxiv.org/abs/1806.11146v2</a>)</li><li><a href="https://arxiv.org/abs/1705.09554" target="_blank" rel="noopener">https://arxiv.org/abs/1705.09554</a></li><li><a href="https://arxiv.org/abs/1707.05572" target="_blank" rel="noopener">https://arxiv.org/abs/1707.05572</a></li></ul><h2 id="Attack-in-the-Physical-World"><a href="#Attack-in-the-Physical-World" class="headerlink" title="Attack in the Physical World"></a>Attack in the Physical World</h2><p>有一个问题是，我们说的这些攻击在物理世界中会失效吗？比如相机等设备会不会像人一样无法识别那些攻击信号呢？答案是不一定。有人做了相关实验，将扰动得到的图片$x’$打印出来，然后用手机、相机等拍照再做分类或识别，仍然可以成功攻击模型，详见：Adversarial examples in the physical world(<a href="https://arxiv.org/abs/1607.02533v4)、https://www.youtube.com/watch?v=zQ_uMenoBCk&amp;feature=youtu.be" target="_blank" rel="noopener">https://arxiv.org/abs/1607.02533v4)、https://www.youtube.com/watch?v=zQ_uMenoBCk&amp;feature=youtu.be</a></p><p>攻击还可以用在人脸识别领域，比如我戴一个实体眼镜(用于攻击)之后可能就会被人脸识别系统识别为其他人，详见：<a href="https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf" target="_blank" rel="noopener">https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf</a></p><p>攻击还可以用在自动驾驶中，假如汽车在自动驾驶时需要看红绿灯等标志物，那在这些标志物上贴一些攻击信号也许就会导致车辆“失控”，详见：<a href="https://arxiv.org/abs/1707.08945" target="_blank" rel="noopener">https://arxiv.org/abs/1707.08945</a></p><h2 id="Attack-Text-amp-Audio"><a href="#Attack-Text-amp-Audio" class="headerlink" title="Attack Text &amp; Audio"></a>Attack Text &amp; Audio</h2><p>攻击并不仅限于Image，还可以攻击Text，可参考：<a href="https://arxiv.org/pdf/1707.07328.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.07328.pdf</a></p><p>攻击并不仅限于Image和Text，还可以攻击Audio，可参考：</p><ul><li><a href="https://nicholas.carlini.com/code/audio_adversarial_examples" target="_blank" rel="noopener">https://nicholas.carlini.com/code/audio_adversarial_examples</a></li><li><a href="https://adversarial-attacks.net" target="_blank" rel="noopener">https://adversarial-attacks.net</a></li><li>现实中ASR可能会受到攻击，ASR指Automatic Speech Recognition(自动语音识别)，即语音转文字，详见：<a href="https://nicholas.carlini.com/code/audio_adversarial_examples/" target="_blank" rel="noopener">https://nicholas.carlini.com/code/audio_adversarial_examples/</a></li><li>ASV也可能会受到攻击，ASV指Automatic Speaker Verification，即识别是谁讲话，详见：<a href="https://arxiv.org/abs/1911.01840" target="_blank" rel="noopener">https://arxiv.org/abs/1911.01840</a></li></ul><h2 id="Hidden-Voice-Attack"><a href="#Hidden-Voice-Attack" class="headerlink" title="Hidden Voice Attack"></a>Hidden Voice Attack</h2><p>Hidden Voice Attack就是生成一段人类无法听懂的audio但仍然可以骗过你的模型，比如用一段人类听不懂但被模型判定为“Hey, Siri”的audio启动你的苹果手机。</p><p>详见：<a href="https://arxiv.org/abs/1904.05734" target="_blank" rel="noopener">https://arxiv.org/abs/1904.05734</a></p><p>如下图所示，处理audio的步骤为Preprocessing、Signal Processing、Model Inference，Hidden Voice Attack就是在Signal Processing阶段进行攻击。</p><p><img src="https://pic4.zhimg.com/80/v2-c9deeab05746a3649cacdad7f5ff7c91_720w.png" alt="img"></p><p>如下图所示，对audio的扰动方式有4种，具体不再详细介绍。</p><p><img src="https://pic1.zhimg.com/80/v2-57afffde6ff0ea8a468ce57bbd9b3f83_720w.png" alt="img"></p><h3 id="Time-Domain-Inversion"><a href="#Time-Domain-Inversion" class="headerlink" title="Time Domain Inversion"></a>Time Domain Inversion</h3><p>Time Domain Inversion简称TDI，它利用了magnitude FFT多对一的性质(两个不同的signal经过mFFT可以得到相同结果)，所以我们可以将time domain中的signal进行处理，处理之后会影响人听懂但不影响模型。</p><h3 id="Random-Phase-Generation"><a href="#Random-Phase-Generation" class="headerlink" title="Random Phase Generation"></a>Random Phase Generation</h3><p>FFT返回了一个复数$a+bi$，而$magnitude=\sqrt{a^2+b^2}$，不同的$a$和$b$可以计算得到相同的magnitude，所以可以对$a$和$b$进行修改，修改后会影响人听懂但不影响模型。</p><h3 id="High-Frequency-Addition"><a href="#High-Frequency-Addition" class="headerlink" title="High Frequency Addition"></a>High Frequency Addition</h3><p>High Frequency Addition简称HFA。</p><p>在Preprocessing过程中会使用low-pass filter过滤掉比人声高很多的频段以增加Voice Processing System的准确率，那我们就可以在audio中加入高频段audio，这样会影响人听懂但不影响模型。</p><h3 id="Time-Scaling"><a href="#Time-Scaling" class="headerlink" title="Time Scaling"></a>Time Scaling</h3><p>将audio缩短到model能正确辨识但人听不懂。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Universal-Adversarial-Attack&quot;&gt;&lt;a href=&quot;#Universal-Adversarial-Attack&quot; class=&quot;headerlink&quot; title=&quot;Universal Adversarial Attack&quot;&gt;&lt;/a&gt;Un
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="对抗攻击与防御" scheme="https://chouxianyu.github.io/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B8%8E%E9%98%B2%E5%BE%A1/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-12.1对抗攻击入门</title>
    <link href="https://chouxianyu.github.io/2021/04/27/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-12-1%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E5%85%A5%E9%97%A8/"/>
    <id>https://chouxianyu.github.io/2021/04/27/李宏毅机器学习课程笔记-12-1对抗攻击入门/</id>
    <published>2021-04-27T02:23:23.000Z</published>
    <updated>2021-04-27T03:13:03.268Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>我们希望我们的模型不仅仅是在大多数情况下可用的，还希望它能够应对来自外界的“攻击”，特别是在垃圾邮件分类、恶意软件检测、网络入侵检测等任务中。</p><p>这个领域为对抗攻击与防御（Adversarial Attack and Defense），目前攻击是比较容易的而防御比较困难。</p><h2 id="What-is-Attack"><a href="#What-is-Attack" class="headerlink" title="What is Attack"></a>What is Attack</h2><p>attack就是往原输入$x^0$中添加一些特别的噪声$\Delta x$(并不是随机生成的)得到一个稍微有些不同的输入$x’=x^0+\Delta x$，而模型却得到一个与原输出截然不同的输出。如下图所示，在图像分类任务中，对一张“Tiger Cat”图片添加一些特别的噪声$\Delta x$后，人类还能看出它是“Tiger Cat”，但模型却认为它是其它的类别。</p><p><img src="https://pic1.zhimg.com/80/v2-5610b9b55bf8893362a8526352810834_720w.png" alt="img"></p><h2 id="Loss-Function-For-Attack"><a href="#Loss-Function-For-Attack" class="headerlink" title="Loss Function For Attack"></a>Loss Function For Attack</h2><p>攻击可以分为两种：Non-targeted Attack和Targeted Attack。</p><h3 id="How-To-Train"><a href="#How-To-Train" class="headerlink" title="How To Train"></a>How To Train</h3><p>我们是这样训练一个普通的神经网络的：将输入$x^0$输入到模型后，我们希望<strong>模型的输出$y^0$和标签$y^{true}$越接近越好</strong>，则损失函数为$L_{train}(\theta)=C(y^0,y^{true})$。<strong>此时输入$x^0$是固定的，我们需要不断调整模型参数$\theta$</strong>，使得$L_{train}(\theta)$最小。</p><h3 id="Non-targeted-Attack"><a href="#Non-targeted-Attack" class="headerlink" title="Non-targeted Attack"></a>Non-targeted Attack</h3><p>如果是Non-targeted Attack，将输入$x’=x+\Delta x$输入到模型后，我们希望<strong>模型的输出$y’$和标签$y^{true}$的差异越大越好</strong>，则损失函数为$L_{Non-targeted\ Attack}(x’)=-C(y’,y^{true})$，比$L_{train}(\theta)$多了一个负号。<strong>此时模型参数$\theta$是固定的，我们需要不断调整输入$x’$</strong>，使$L_{Non-targeted\ Attack}(x’)$最小。</p><h3 id="Targeted-Attack"><a href="#Targeted-Attack" class="headerlink" title="Targeted Attack"></a>Targeted Attack</h3><p>如果是Targeted Attack，将输入$x’=x+\Delta x$输入到模型后，我们希望<strong>模型的输出$y’$和标签$y^{true}$的差异越大越好并且模型的输出$y’$与某个$y^{false}$越接近越好</strong>，其中$y^{false}$需要人为选择，则损失函数为$L_{Targeted\ Attack}(x’)=-C(y’,y^{true})+C(y’,y^{false})$。<strong>此时模型参数$\theta$是固定的，我们需要不断调整输入$x’$</strong>，使$L_{Targeted\ Attack}(x’)$最小。</p><h2 id="Constraint-For-Attack"><a href="#Constraint-For-Attack" class="headerlink" title="Constraint For Attack"></a>Constraint For Attack</h2><p>在Attack中，除了要使$L_{Non-targeted\ Attack}(x’)$和$L_{Targeted\ Attack}(x’)$最小之外，我们还希望<strong>$x^0$和$x’$之间的差异较小</strong>，即$d(x^0,x’)\leq\epsilon$，其中$\epsilon$需要人为选择，这样才能实现真正的Attack。</p><p>主要有两种计算$d(x^0,x’)$的方法，但在不同的任务中应该有不同的计算方法，因为其代表着人类视角下$x^0$和$x’$之间的差异。</p><h3 id="L2-norm"><a href="#L2-norm" class="headerlink" title="L2-norm"></a>L2-norm</h3><p>L2-norm为$x^0$和$x’$中每个像素之差的平方和，即$d(x^0,x’)=||x^0-x’||_2=||\Delta x||_2=(\Delta x_1)^2+(\Delta x_2)^2+(\Delta x_3)^2+\dots$</p><h3 id="L-infinity"><a href="#L-infinity" class="headerlink" title="L-infinity"></a>L-infinity</h3><p>L-infinity为$x^0$和$x’$中每个像素之差的最大值，即$d(x^0,x’)=||x^0-x’||_{\infin}=||\Delta x||_{\infty}=max\{\Delta x_1,\Delta x_2,\Delta_3,\dots\}$</p><p>对于图像中的pixel来讲，也许L-infinity是更有效的计算方法。</p><h2 id="How-to-Attack"><a href="#How-to-Attack" class="headerlink" title="How to Attack"></a>How to Attack</h2><p>我们在Attack时要训练的参数是输入$x’$而非模型参数$\theta$，在$d(x^0,x’)\leq\epsilon$的情况下使得$L(x’)$最小，即$x^*=arg\mathop{min}_\limits {d(x^0,x’)\leq\epsilon}L(x’)$。</p><p>关于如何训练输入$x’$而非模型参数$\theta$，可以参考下Explainable AI的代码部分，其实就是设置输入$x’$的梯度是可追踪的并在定义优化器时传入输入$x’$而非模型参数$\theta$。</p><p><img src="https://pic1.zhimg.com/80/v2-cabb0848c34eca5fe5f4009181b49e7b_720w.png" alt="img"></p><p>如上图所示，在训练时有两个关键点。第一点是输入$x’$应该用$x^0$初始化；第二点是在每个iteration中需要判断保证$d(x^0,x’)\leq\epsilon$，具体来讲就是当发现$d(x^0,x’)&gt;\epsilon$时就需要将$x’$修正为所有满足$d(x^0,x)&gt;\epsilon$的$x$中与$x’$最接近的那个。那要怎么找到所有满足$d(x^0,x)&gt;\epsilon$的$x$中与$x’$最接近的那个呢？下图中圆形和正方形中的$x$均满足$d(x^0,x)&gt;\epsilon$，所以中心$x^0$与$x’$连线与圆形或正方形的交点就是要修正的结果。</p><p><img src="https://pic2.zhimg.com/80/v2-163a5c710e5ed504d06debfb7c509493_720w.png" alt="img"></p><h2 id="Attack-Approaches"><a href="#Attack-Approaches" class="headerlink" title="Attack Approaches"></a>Attack Approaches</h2><ul><li>FGSM (<a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">https://arxiv.org/abs/1412.6572</a>)</li><li>Basic iterative method (<a href="https://arxiv.org/abs/1607.02533" target="_blank" rel="noopener">https://arxiv.org/abs/1607.02533</a>) </li><li>L-BFGS (<a href="https://arxiv.org/abs/1312.6199" target="_blank" rel="noopener">https://arxiv.org/abs/1312.6199</a>)</li><li>Deepfool (<a href="https://arxiv.org/abs/1511.04599" target="_blank" rel="noopener">https://arxiv.org/abs/1511.04599</a>)</li><li>JSMA (<a href="https://arxiv.org/abs/1511.07528" target="_blank" rel="noopener">https://arxiv.org/abs/1511.07528</a>)</li><li>C&amp;W (<a href="https://arxiv.org/abs/1608.04644" target="_blank" rel="noopener">https://arxiv.org/abs/1608.04644</a>)</li><li>Elastic net attack (<a href="https://arxiv.org/abs/1709.04114" target="_blank" rel="noopener">https://arxiv.org/abs/1709.04114</a>)</li><li>Spatially Transformed (<a href="https://arxiv.org/abs/1801.02612" target="_blank" rel="noopener">https://arxiv.org/abs/1801.02612</a>) </li><li>One Pixel Attack (<a href="https://arxiv.org/abs/1710.08864" target="_blank" rel="noopener">https://arxiv.org/abs/1710.08864</a>)</li><li>……</li></ul><p>虽然有很多方法都可以进行attack，但它们的主要区别在于使用了不同的constraint或者使用了不同的optimization method。</p><h2 id="FGSM"><a href="#FGSM" class="headerlink" title="FGSM"></a>FGSM</h2><p>FGSM即Fast Gradient Sign Method，本次homework(hw6_AdversarialAttack)就使用了FGSM。</p><p>FGSM中输入的更新规则为$x^*=x^0-\epsilon\Delta x$。它首先计算出损失$L$关于$x$的每个维度的梯度，如果梯度大于0则修改为+1、小于0则修改为-1，也就是说$x^0$的所有维要么$+\epsilon$是要么是$-\epsilon$。</p><p>假设FGSM使用L-infinity计算$d(x^0,x^<em>)$，如果梯度指向左下角那么$x^</em>$就在方框的右上角；如果gradient指向左上角那么$x^<em>$就在方框的右下角；因此在FGSM中我们只在意梯度方向而不在意其大小。我们可以认为FGSM使用一个非常大的学习率使$x$飞出正方形，但因为要保证$d(x^0,x’)\leq\epsilon$所以$x^</em>$就会被限制到方形区域内部。所以就像是“一拳超人”，只攻击一次就达到好的效果。</p><p><img src="https://pic1.zhimg.com/80/v2-8c2bf52f38a95de4070288719e1eed4c_720w.png" alt="img"></p><h2 id="Black-Box-Attack"><a href="#Black-Box-Attack" class="headerlink" title="Black Box Attack"></a>Black Box Attack</h2><p>Attack可以分为White Box和Black Box。White Box Attack指<strong>模型参数$\theta$是已知的</strong>，Black Box Attack指<strong>模型参数$\theta$是未知的</strong>。</p><p>在Black Box Attack中，我们不知道Black Network的参数$\theta$。现假设我们知道Black Network的训练集，那我们就可以使用这份训练集自行训练出一个Proxy Network，然后基于Proxy Network和训练集就可以得到$x’$，这个$x’$一般也可以成功攻击Black Network。如果Black Network是一个在线API，我们既不知道Black Network的参数也不知道它的训练集，那上传大量输入后就可以得到大量对应的输出，并以这些输入输出对为训练集得到Proxy Network和$x’$。</p><p>有相关实验证明，Black Box Attack是非常有可能攻击成功的，详见：Delving into Transferable Adversarial Examples and Black-box Attacks(<a href="https://arxiv.org/abs/1611.02770" target="_blank" rel="noopener">https://arxiv.org/abs/1611.02770</a>)</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;我们希望我们的模型不仅仅是在大多数情况下可用的，还希望它能够应对来自外界的“
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="对抗攻击与防御" scheme="https://chouxianyu.github.io/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B8%8E%E9%98%B2%E5%BE%A1/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-11.5基于PyTorch的Explainabe AI实战</title>
    <link href="https://chouxianyu.github.io/2021/04/25/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-11-5%E5%9F%BA%E4%BA%8EPyTorch%E7%9A%84Explainabe-AI%E5%AE%9E%E6%88%98/"/>
    <id>https://chouxianyu.github.io/2021/04/25/李宏毅机器学习课程笔记-11-5基于PyTorch的Explainabe-AI实战/</id>
    <published>2021-04-25T01:48:58.000Z</published>
    <updated>2021-04-25T01:56:27.641Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework5的记录。</p><p>全部课程PPT、数据和代码下载链接：</p><p>链接：<a href="https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg" target="_blank" rel="noopener">https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg</a> 提取码：tpmc</p><p>代码仓库：<a href="https://github.com/chouxianyu/LHY_ML2020_Codes" target="_blank" rel="noopener">https://github.com/chouxianyu/LHY_ML2020_Codes</a></p><h1 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h1><p>在homework3中我们通过CNN实现了食物图片分类，详见我之前的文章《李宏毅机器学习课程笔记-7.4基于CNN和PyTorch的食物图片分类》，这次作业的任务就是探究这个CNN的可解释性，具体如下</p><ol><li><p><strong>Saliency Map</strong></p><p> 按照《Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps》，计算每个像素对最终分类结果的重要性。</p><p> 我們把一张图片输入到model，将model的输出与label进行对比计算得到loss，因此与loss相关的变量有image、model parameter和label这3者。</p><p> 通常情況下，我們训练模型时希望找到一组好的model parameter来拟合image和label，因此loss在backward时我们只在乎loss关于model parameter的梯度。但在数学上image本身也是continuous tensor，我们可以<strong>在model parameter和label都固定的情况下计算loss关于image的梯度，这个梯度代表稍微改变image的某个pixel value会对loss产生什么影响，我们习惯把这个影响的程度解读为该pixel对于结果的重要性（每个pixel都有自己的梯度）</strong>。</p><p> 因此将loss关于一张图片中每个pixel的梯度计算并画出来，就可以看出该图中哪些像素是model在计算结果时的重要依据。那如何用代码实现我们的这个想法呢？非常简单，在一般训练中我们都是在forward后计算模型输出与标签之间的loss，然后进行loss的backward，其实在PyTorch中这个backword计算的是loss对<strong>model parameter</strong>的梯度，因此我們只需要用一行代码<code>images.requires_grad_()</code>使得<strong>image</strong>也要被计算梯度。</p></li><li><p><strong>Filter Visualization</strong></p><p> 基于Gradient Ascent，实现Activation maximization，找到最能够激活某个filter的图片，以观察模型学到了什么。</p><p> 这里我们想要知道某一个filter到底学习到了什么，我们需要做两件事情：<strong>①Filter Visualization：挑几张图片看看某个filter的输出；②Filter Activation：看看什么图片可以最大程度地activate该filter</strong>。</p><p> 在代码实现方面，我们一般是直接把图片输入到model，然后直接forward，那要如何取出model中某层的输出呢？虽然我们可以直接修改model的forward函数使其返回某层的输出，但这样比较麻烦，还可能会因此改动其它部分的代码。因此PyTorch提供了方便的解决方法：<strong>hook</strong>。</p></li><li><p><strong>LIME</strong></p><p> 绿色代表一个component和结果正相关，红色则代表该component和结果负相关。</p><p> 《”Why Should I Trust You?”: Explaining the predictions of Any Classifier》</p><p> 注：根据助教的示例，我遇到了一个BUG<code>KeyError: &#39;Label not in explanation&#39;</code>，暂未解决……</p></li></ol><h1 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h1><p>使用homework3使用的数据集以及训练出的CNN模型。</p><h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><p><a href="https://github.com/utkuozbulak/pytorch-cnn-visualizations" target="_blank" rel="noopener">https://github.com/utkuozbulak/pytorch-cnn-visualizations</a></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework5的记录。&lt;/p&gt;
&lt;p&gt;全部课程PPT、数据和代码下载链接：&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1n_N7aoaNxxwqO03EmV5Bjg&quot; target
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="PyTorch" scheme="https://chouxianyu.github.io/tags/PyTorch/"/>
    
      <category term="ExplainableAI" scheme="https://chouxianyu.github.io/tags/ExplainableAI/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-11.4模型无关的局部解释(LIME)</title>
    <link href="https://chouxianyu.github.io/2021/04/24/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-11-4%E6%A8%A1%E5%9E%8B%E6%97%A0%E5%85%B3%E7%9A%84%E5%B1%80%E9%83%A8%E8%A7%A3%E9%87%8A-LIME/"/>
    <id>https://chouxianyu.github.io/2021/04/24/李宏毅机器学习课程笔记-11-4模型无关的局部解释-LIME/</id>
    <published>2021-04-24T02:36:52.000Z</published>
    <updated>2021-04-24T02:37:44.740Z</updated>
    
    <content type="html"><![CDATA[<p>模型无关的局部解释（Local Interpretable Model-Agnostic Explanations，LIME）指<strong>使用一个Interpretable Model拟合一个Uninterpretable Model的局部</strong>，比如使用Linear Model或者Decision Tree模拟Neural Network。</p><p>具体来讲，对于局部范围内的相同输入，我们希望两个模型的输出尽可能接近。这里要重点强调局部范围的概念，因为实际上Linear Model并不能模拟整个Neural Network但却可以模拟其中的一个Local Region，这也是LIME可行的原因。</p><h2 id="案例：使用Linear-Model拟合Neural-Network"><a href="#案例：使用Linear-Model拟合Neural-Network" class="headerlink" title="案例：使用Linear Model拟合Neural Network"></a>案例：使用Linear Model拟合Neural Network</h2><ol><li><p>定位：确定需要解释的data point(下图5个蓝点中最中间的蓝点)</p></li><li><p>取样：在上一步确定的data point周围sample获得更多的data point(下图5个蓝点)</p><p> sample的<strong>范围</strong>需要根据情况调整，一般范围小则拟合得更准确</p><p> sample的<strong>方法</strong>不同，结果也会不同</p></li><li><p>拟合：使用Linear Model模拟上一步确定的data point及其对应的Neural Network输出</p></li><li><p>解释：对Linear Model进行解释，进而解释Neural Network的局部</p></li></ol><p><img src="https://pic1.zhimg.com/80/v2-a29bafdc2312575cab096090938493b2_720w.png" alt="img"></p><h2 id="案例：LIME-For-Image-Classification"><a href="#案例：LIME-For-Image-Classification" class="headerlink" title="案例：LIME For Image Classification"></a>案例：LIME For Image Classification</h2><p>如何将LIME应用到Image Classification呢？假设有一张图片被分类为frog，下面只讲一些关键点。</p><ul><li><p>如何进行sample？</p><p>  首先可以将图片分成多个segment：$\{s_1,s_2,\dots,s_n\}$，随机去除其中的一些segment就可以得到该图片“周围”的一些图片</p></li><li><p>将LIME应用于图片时，一般要进行Feature Extraction，那如何做呢？</p><p>  使用$x_i$表示图片中的每个segment是否被删除，其中$i=1,…,n$，若$x_i$为1则表示该segment被删除，否则表示该segment未被删除</p></li><li><p>如何解释Linear Model？</p><p>  设Linear Model为$y=w_1x_1+..+w_nx_n$，$w_i$的值有以下3种情况</p><ul><li>$w_i\approx 0$表示$s_i$对分类为frog没有影响；</li><li>$w_i&gt; 0$表示$s_i$对分类为frog具有正面影响，即这个segment使得模型倾向于将图片分类为frog</li><li>$w_i&lt;0$表示$s_i$对分类为frog具有负面影响，即这个segment使得模型倾向于认为该图片不是frog类别</li></ul></li></ul><h2 id="Tree-Regularization"><a href="#Tree-Regularization" class="headerlink" title="Tree Regularization"></a>Tree Regularization</h2><p><strong>理论上可以用无深度限制的Decision Tree拟合完整的Neural Network，但Decision Tree的深度不可能没有限制</strong>，因此在使用Decision Tree拟合Neural Network时需要对Decision Tree的复杂度进行约束。</p><p>设Neural Network的参数为$\theta$、Decision Tree的参数为$T_\theta$，使用Decision Tree的平均深度表示其参数$T_\theta$的复杂度$O(T_\theta)$。在使用Decision Tree拟合Neural Network时，不仅要使两者输出相近还要使$O(T_\theta)$最小化，因此优化目标为$\theta^*=arg\ {\rm min}\ L(\theta) + \lambda O(T_\theta)$。</p><p>因此我们<strong>在训练神经网络时就可以使用Tree Regularization使得训练出的神经网络更具有可解释性</strong>，$O(T_\theta)$不能微分，解决方法详见《Beyond Sparsity: Tree Regularization of Deep Models for Interpretability》。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;模型无关的局部解释（Local Interpretable Model-Agnostic Explanations，LIME）指&lt;strong&gt;使用一个Interpretable Model拟合一个Uninterpretable Model的局部&lt;/strong&gt;，比如使用
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="ExplainableAI" scheme="https://chouxianyu.github.io/tags/ExplainableAI/"/>
    
      <category term="LIME" scheme="https://chouxianyu.github.io/tags/LIME/"/>
    
  </entry>
  
</feed>
