<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>臭咸鱼的缺氧瓶</title>
  
  <subtitle>快给我氧气！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://chouxianyu.github.io/"/>
  <updated>2021-04-01T04:18:04.287Z</updated>
  <id>https://chouxianyu.github.io/</id>
  
  <author>
    <name>臭咸鱼</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>李宏毅机器学习课程笔记-7.3CNN应用案例</title>
    <link href="https://chouxianyu.github.io/2021/04/01/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7-3CNN%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
    <id>https://chouxianyu.github.io/2021/04/01/李宏毅机器学习课程笔记-7-3CNN应用案例/</id>
    <published>2021-04-01T04:15:26.000Z</published>
    <updated>2021-04-01T04:18:04.287Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deep-Dream"><a href="#Deep-Dream" class="headerlink" title="Deep Dream"></a>Deep Dream</h2><p><a href="https://deepdreamgenerator.com/" target="_blank" rel="noopener">Deep Dream</a>是这样的：如果给机器一张图片$x$，Deep Dream会把机器看到的内容加到图片$x$中得到$x’$。那如何实现呢？</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221135654357-945005692.png"></p><p>如上图所示，将图片$x$输入到CNN中，然后取出CNN中某一层$L$（可以是卷积、池化阶段的隐藏层，也可以是FNN中的隐藏层）的输出$O$，然后将$L$中的正值调大、负值调小得到一个新的输出$O’$，然后通过梯度下降找到一张新的图片$x’$使层$L$的输出为$O’$，这个$x’$就是我们要的结果。直观理解的话，也就是<strong>让CNN夸大它所看到的内容</strong>。</p><p>然后就得到了如下结果……（看到的时候我惊了，真是十分哇塞）</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221135818664-1882676933.png"></p><h2 id="Deep-Style"><a href="#Deep-Style" class="headerlink" title="Deep Style"></a>Deep Style</h2><p><a href="https://dreamscopeapp.com/" target="_blank" rel="noopener">Deep Style</a>是这样的：如果给机器一张图片$x$和$y$，Deep Style可以把图片$y$的风格加到图片$x$上，也就是<strong>风格迁移</strong>。</p><p>那如何实现呢？论文：<a href="https://arxiv.org/abs/1508.06576" target="_blank" rel="noopener">A Neural Algorithm of Artistic Style</a>。</p><ol><li>把图片$x$传入CNN并得到输出，然后其输出作为图片$x$的内容$c_x$（<strong>content</strong>）；</li><li>把图片$y$传入CNN并得到输出，但不是考虑输出的值是什么，而是考虑输出层中各个filter输出之间的相关性（corelation）作为图片$y$的风格$s_y$（<strong>style</strong>）；</li><li>最后基于同一个CNN找到图片$z$，图片$z$传入CNN后得到的内容$c_z$像$c_x$、风格$s_z$像$s_y$。</li></ol><p>如下图所示</p><p><img src="https://pic1.zhimg.com/80/v2-fb5dd2b4083a6b896f57bb7e7a231c50_720w.png" alt="img"></p><h2 id="围棋"><a href="#围棋" class="headerlink" title="围棋"></a>围棋</h2><p>CNN不单单可以用在图像上，还可以用在其它方面，比如下围棋。</p><p>在下围棋这件事上，其实FNN就可以（输入和输出都是19×19=361的vector），但CNN的效果更好。当然还可以用强化学习。</p><p>为什么CNN可以用来下围棋呢？因为围棋具有图像的3个性质，不过AlphaGo并没有用Max Pooling因为它不需要。</p><h2 id="语音"><a href="#语音" class="headerlink" title="语音"></a>语音</h2><p>如下图所示，用<strong>语谱图（Spectrogram）</strong>表示语音。</p><p>语谱图的x轴是时间，y轴是频率，z轴是幅度。幅度用颜色表示（比如亮色表示高、暗色表示低）。</p><p>在语谱图中，CNN的卷积核往往只在y轴方向上移动，这样可以消除男生女生声音频率的差异；卷积核往往不在x轴上移动，因为时间域一般是在后面用LSTM等等进行处理，如下图所示。</p><p><img src="https://pic1.zhimg.com/80/v2-65e8d3ffdc43a45f4f9ca39ea16a9805_720w.png" alt="img"></p><h2 id="文本"><a href="#文本" class="headerlink" title="文本"></a>文本</h2><p>CNN也可以用在文字处理上，比如文本情感分析。具体不再讲，可以看李宏毅老师的<a href="https://www.bilibili.com/video/BV1JE411g7XF?p=17" target="_blank" rel="noopener">视频</a>。</p><p><img src="https://pic4.zhimg.com/80/v2-fec6cc1a30d3f436c71587f91f8f82ee_720w.png" alt="img"></p><h2 id="图片生成"><a href="#图片生成" class="headerlink" title="图片生成"></a>图片生成</h2><p>Deep Dream的方法还是不能画出图片，不过也有其它较为成功的方法，如下图所示。</p><p><img src="https://pic2.zhimg.com/80/v2-dc95952a08fcb965d30784de27789aef_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Deep-Dream&quot;&gt;&lt;a href=&quot;#Deep-Dream&quot; class=&quot;headerlink&quot; title=&quot;Deep Dream&quot;&gt;&lt;/a&gt;Deep Dream&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://deepdreamgenerator.co
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://chouxianyu.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-7.2CNN学到了什么</title>
    <link href="https://chouxianyu.github.io/2021/03/31/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7-2CNN%E5%AD%A6%E5%88%B0%E4%BA%86%E4%BB%80%E4%B9%88/"/>
    <id>https://chouxianyu.github.io/2021/03/31/李宏毅机器学习课程笔记-7-2CNN学到了什么/</id>
    <published>2021-03-31T05:36:40.000Z</published>
    <updated>2021-03-31T05:37:13.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="卷积核学到了什么"><a href="#卷积核学到了什么" class="headerlink" title="卷积核学到了什么"></a>卷积核学到了什么</h2><p>如果想知道下图中第1个卷积层中的每个卷积核的功能，因为它参数比较少而且其输入是原图片，所以我们直接结合原图片观察卷积核的参数就可以知道该卷积核的功能。</p><p><img src="https://pic2.zhimg.com/80/v2-cb97966c67656a905b06956b7925f645_720w.png" alt="img" style="zoom: 80%;"></p><p>如上图所示，CNN中第2个卷积层的输入不是直观的图片而且其卷积核的感受野比第1个卷积层中卷积核的感受野更大，因此我们无法通过观察卷积核参数了解卷积核学习到了什么。</p><p>上图中CNN第2个卷积层中有50个卷积核，每个卷积核的输出都是一个大小为11×11的矩阵。</p><p>现在定义函数$a^k=\sum_{i=1}^{11}\sum_{j=1}^{11}a^k_{ij}$来衡量某卷积核被“激活”的程度，即将卷积核所输出矩阵的元素之和作为其被“激活”程度，被“激活”程度指CNN输入与该卷积核有多匹配。</p><p>接下来，通过梯度下降求得$x^<em>=arg\ max_x\ a^k$，即找到最能“激活”卷积核的输入图片$x^</em>$，然后将其可视化希求反映该卷积核学习到的内容。</p><p>上图左下角可视化了最能“激活”第2个卷积层中某12个卷积核的12张图片，可以看出各卷积核适用于检测<strong>小的纹理</strong>（这个案例是数字识别）。</p><h2 id="全连接层学到了什么"><a href="#全连接层学到了什么" class="headerlink" title="全连接层学到了什么"></a>全连接层学到了什么</h2><p>如下图所示，我们同样可以用梯度下降找到最能“激活”全连接层中某个神经元的输入，将该输入其可视化，以此观察全连接层学习到了什么。和卷积核不同，卷积核学习到的是<strong>较小的pattern</strong>，全连接层中的神经元学习到的是<strong>尺寸较大的pattern</strong>。</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221130655109-1379139074.png" style="zoom: 50%;"></p><h2 id="输出层学到了什么"><a href="#输出层学到了什么" class="headerlink" title="输出层学到了什么"></a>输出层学到了什么</h2><p>如下图所示，我们同样可以用梯度下降找到最能“激活”输出层中某个神经元的输入，将该输入其可视化，以此观察输出层学习到了什么。在想象中这些输入应该是对应的数字，然而并不是（有人说眯着眼马马虎虎可以从中看到数字）。</p><p>在其它的案例中也有这样的情况，即<strong>机器学习到的内容和人类所理解的内容是不同的</strong>。<a href="https://www.youtube.com/watch?v=M2IebCN9Ht4" target="_blank" rel="noopener">这个视频</a>里讲了相关例子。</p><p>但这个方法(通过“激活”卷积核反映学习到的内容)确实是有效的，那如何改进呢？我们可以对输入做一些限制（constraint）。</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221131204130-1053266641.png" style="zoom: 50%;"></p><p>在该例中数字图片中底色是黑色，前景色（数字）是白色。</p><p>如下图所示，简单考虑的话，我们可以添加图片中大部分像素是黑色的限制（值接近0，也就是L1正则化），然后将求得的输入可视化。</p><p><img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201221133929054-200979562.png" style="zoom:50%;"></p><p>当然，我们可以用更好的方法（比如说更好的限制）找到人类更容易理解的输入……这也是Deep Dream的精神。</p><h2 id="神经网络可视化"><a href="#神经网络可视化" class="headerlink" title="神经网络可视化"></a>神经网络可视化</h2><p><img src="https://pic4.zhimg.com/80/v2-59efdfca9bf326af0199db3ece69dd28_720w.png" alt="img"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;卷积核学到了什么&quot;&gt;&lt;a href=&quot;#卷积核学到了什么&quot; class=&quot;headerlink&quot; title=&quot;卷积核学到了什么&quot;&gt;&lt;/a&gt;卷积核学到了什么&lt;/h2&gt;&lt;p&gt;如果想知道下图中第1个卷积层中的每个卷积核的功能，因为它参数比较少而且其输入是原图片，所以我
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://chouxianyu.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-7.1CNN入门详解</title>
    <link href="https://chouxianyu.github.io/2021/03/29/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-7-1CNN%E5%85%A5%E9%97%A8%E8%AF%A6%E8%A7%A3/"/>
    <id>https://chouxianyu.github.io/2021/03/29/李宏毅机器学习课程笔记-7-1CNN入门详解/</id>
    <published>2021-03-29T09:09:58.000Z</published>
    <updated>2021-03-29T10:54:37.445Z</updated>
    
    <content type="html"><![CDATA[<p>卷积神经网络（CNN）常常被用来做图像处理，当然也可以用一般的神经网络，那它们各自有什么优缺点呢？</p><h2 id="FNN用于图片处理的缺点"><a href="#FNN用于图片处理的缺点" class="headerlink" title="FNN用于图片处理的缺点"></a>FNN用于图片处理的缺点</h2><p>使用一般的全连接前馈神经网络（FNN）处理图片时的缺点：</p><ul><li><p>需要很多的参数</p><p>  假设有一张尺寸100×100的图片（尺寸已经算很小了），那输入层就有100×100×3=30K个像素，假设第一个隐藏层有1K个神经元（一个神经元包含30K个参数），这就已经需要30M个参数了……</p></li><li><p>该架构中每个神经元就是一个分类器，这是没必要的</p><p>  第一个隐藏层作为最基础的pattern分类器（比如判断有无绿色、边缘等），第二个隐藏层基于第一个隐藏层继续做pattern分类（比如木头、肉类），以此类推……</p></li></ul><p>按照人类的直观理解，我们不是像全连接神经网络一样去处理图片的。具体来看，有哪些方面呢？</p><h2 id="图片的一些性质"><a href="#图片的一些性质" class="headerlink" title="图片的一些性质"></a>图片的一些性质</h2><p>结合全连接前馈神经网络的缺点和人类对图片的直观理解，可以得到下述图片的3个性质。</p><h3 id="性质1：Some-patterns-are-much-smaller-than-the-whole-image"><a href="#性质1：Some-patterns-are-much-smaller-than-the-whole-image" class="headerlink" title="性质1：Some patterns are much smaller than the whole image."></a>性质1：Some patterns are much smaller than the whole image.</h3><p>在识别某个模式（pattern）时，一个神经元并不需要图片的所有像素点。对于一张人类全身照的图片，我们只需要看到头部而非整张图片就可以判断它是一个人脸。所以我们应该是可以用少量参数去识别这些pattern的。</p><p><img src="https://pic4.zhimg.com/80/v2-753ba3ca45cd6cc08d8a8d837d2ec425_720w.jpeg" alt="img" style="zoom:80%;"></p><h3 id="性质2：The-same-patterns-appear-in-different-regions"><a href="#性质2：The-same-patterns-appear-in-different-regions" class="headerlink" title="性质2：The same patterns appear in different regions."></a>性质2：The same patterns appear in different regions.</h3><p>比如说人脸可以在图片的中间区域，也可以在图片的某个角落区域。所以识别不同区域中的相同pattern的多个分类器（或detector）应该用同一组参数或者共享参数。</p><p><img src="https://pic1.zhimg.com/80/v2-cb9dfd6b582245692a8fe50979f5f77d_720w.png" alt="img" style="zoom:80%;"></p><h3 id="性质3：Subsampling-the-pixels-will-not-change-the-object"><a href="#性质3：Subsampling-the-pixels-will-not-change-the-object" class="headerlink" title="性质3：Subsampling the pixels will not change the object"></a>性质3：Subsampling the pixels will not change the object</h3><p>将图片缩小/下采样，并不会影响我们理解图片。所以我们可以通过将图片变小，进而用更少的参数处理图片。</p><p><img src="https://pic2.zhimg.com/80/v2-ee9888d41dfe87f1d8715949676ca025_720w.png" alt="img" style="zoom:80%;"></p><h2 id="CNN架构说明"><a href="#CNN架构说明" class="headerlink" title="CNN架构说明"></a>CNN架构说明</h2><p>2014年在ECCV上提出，针对上述的图片的3个性质，确定了CNN的架构如下。</p><p><img src="https://pic2.zhimg.com/80/v2-ea984acebb9fe399847d16b36d6fc559_720w.png" alt="img" style="zoom:80%;"></p><p>如上图所示，图片经过卷积层然后进行最大池化（max pooling），这个步骤可以进行多次；然后将数据展开（Flatten），然后将数据传进全连接前馈网络得到最后的图片分类结果。</p><h2 id="CNN架构作用探析"><a href="#CNN架构作用探析" class="headerlink" title="CNN架构作用探析"></a>CNN架构作用探析</h2><p><img src="https://pic1.zhimg.com/80/v2-544d4e0360461bfebe9624b4b20fedb5_720w.png" alt="img" style="zoom:80%;"></p><p>如上图所示，卷积是针对了图片的性质1和性质2，最大池化是针对了图片的性质3。</p><h2 id="卷积-Convolution-★"><a href="#卷积-Convolution-★" class="headerlink" title="卷积(Convolution) ★"></a>卷积(Convolution) ★</h2><p>假设有一张6×6的二值图，即一个6×6的矩阵。</p><h3 id="卷积核（Filter）"><a href="#卷积核（Filter）" class="headerlink" title="卷积核（Filter）"></a>卷积核（Filter）</h3><p>神经元就是一个计算/函数，卷积核其实就是神经元。</p><p>如下图所示，1个卷积层可以有多个卷积核，矩阵里元素的值就是需要通过学习得到的参数。</p><p>因为这里的输入是一个矩阵，所以卷积核也是1个矩阵（卷积核的通道数等于输入的通道数）。</p><p>假设卷积核大小是3×3，这对应了图片的性质1，即用小的卷积核识别一个小的pattern。</p><p><img src="https://pic1.zhimg.com/80/v2-dd755bcb25fdadf2bc819d96e03879f8_720w.png" alt="img" style="zoom:80%;"></p><h3 id="怎么做卷积"><a href="#怎么做卷积" class="headerlink" title="怎么做卷积"></a>怎么做卷积</h3><p>如下图所示</p><p><img src="https://pic3.zhimg.com/80/v2-be01890275fa85d7b665e5cc70131f30_720w.png" alt="img" style="zoom:80%;"></p><ul><li><p>卷积区域</p><p>  根据该卷积核的大小（以3×3为例），选择图片中相同大小的区域进行卷积。</p></li><li><p>卷积的计算方法</p><p>  从图片中扫描得到的3×3矩阵和卷积核的3×3矩阵，这2个矩阵相同位置的元素相乘可以得到9个值并求和（也就是内积）得到1个值，这就是1次卷积操作。</p></li><li><p>卷积顺序和方向</p><p>  卷积核按照从左到右、从上到下的顺序，从图片左上角开始移动，移动步长（stride）可以设置（以1为例）。在扫描到的每个区域中，都进行1次卷积。1个卷积核移动结束后，则得到1个新的矩阵（大小为4×4），即<strong>1个卷积核的输出是1个矩阵</strong>。</p><p>  卷积层有多个卷积核，每个卷积核都按照该方式进行卷积得到多个矩阵，这些矩阵合起来就形成了1个卷积层的<strong>特征图（Feature Map）</strong>，这个特征图也就是卷积层的输出。</p><p>  <strong>卷积层特征图的通道数等于该卷积层中卷积核的数量，即某卷积层有多少个卷积核，那该卷积层的特征图就有多少个通道</strong>。</p></li></ul><h3 id="卷积的作用"><a href="#卷积的作用" class="headerlink" title="卷积的作用"></a>卷积的作用</h3><p>在上图中，卷积核1的“对角线”的值都是1，就可以识别出图片中哪个区域具有“对角线”。在上图得到的4×4矩阵中，我们可以看到左上角和左下角3个元素的值为3，即有“对角线”。这对应了图片的性质2，我们用1个filter/1组参数识别了不同位置的相同pattern。</p><h3 id="卷积核是矩阵还是张量"><a href="#卷积核是矩阵还是张量" class="headerlink" title="卷积核是矩阵还是张量"></a>卷积核是矩阵还是张量</h3><p>卷积核可以是矩阵，也可以是张量，要根据其输入决定。</p><p><strong>卷积核的通道数等于其所在卷积层输入的通道数（即其所在卷积层前一层的特征图的通道数）</strong>，即单个卷积核会考虑输入的所有通道。</p><p>假如输入是一张RGB图片（大小为3×N×N，即有3个通道、每个通道中矩阵大小为N×N），那卷积核就是1个张量（大小为3×M×M，即有3个通道、每个通道中矩阵大小为M×M），卷积计算方法和上面一样是求内积（两个3×M×M的张量求内积）。</p><h3 id="卷积中如何做梯度下降"><a href="#卷积中如何做梯度下降" class="headerlink" title="卷积中如何做梯度下降"></a>卷积中如何做梯度下降</h3><p>因为一个卷积核要对图片的不同区域进行多次卷积，每次卷积都会有一个梯度，最终把这些梯度取平均值就好了。</p><p>大概这么个意思，实操中这种底层的东西不需要我们实现。</p><h2 id="Convolution-VS-Fully-Connected"><a href="#Convolution-VS-Fully-Connected" class="headerlink" title="Convolution VS Fully Connected"></a>Convolution VS Fully Connected</h2><p>其实，卷积核就是一个神经元，它相当于全连接层中的神经元并没有全连接（没有使用图片的所有像素，当然这样的层也就不能称为全连接层了），这样有两个好处</p><h3 id="模型的参数更少"><a href="#模型的参数更少" class="headerlink" title="模型的参数更少"></a>模型的参数更少</h3><p>如下图所示，因为没有使用所有像素、不是全连接，因此需要的参数更少。</p><p><img src="https://pic2.zhimg.com/80/v2-d386d44d45dc643867b81dc8a4a6a719_720w.png" alt="img" style="zoom: 80%;"></p><h3 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h3><p>如下图所示，因为用同1个卷积核在图片的不同区域做卷积，即用1组参数对不同区域的像素进行计算，得到了多个值、达成了多个神经元的效果，实现了参数共享。比如下图中神经元“3”处理图片第1个像素和神经元“-1”处理图片第2个像素就使用了同一个参数“1”（卷积核的第1个参数）。</p><p>因为实现了参数共享，卷积就比全连接进一步减少了参数量。</p><p><img src="https://pic2.zhimg.com/80/v2-ce7e944ef9280d1bb8505b4fc5f831cf_720w.png" alt="img" style="zoom: 80%;"></p><h2 id="最大池化（Max-Pooling）"><a href="#最大池化（Max-Pooling）" class="headerlink" title="最大池化（Max Pooling）"></a>最大池化（Max Pooling）</h2><p>最大池化是一种下采样（Subsample）,下采样不一定要取最大值，也可以取平均值。</p><h3 id="怎么做最大池化"><a href="#怎么做最大池化" class="headerlink" title="怎么做最大池化"></a>怎么做最大池化</h3><p><img src="https://pic1.zhimg.com/80/v2-b42884461b41f67fa084cc8e2a9d8ac3_720w.png" alt="img" style="zoom: 80%;"></p><p>如上图所示，假如某Max Pooling层的上1层是1个有2个卷积核的卷积层，该卷积层的输出是一个张量（大小为2×4×4，即2个通道、每个通道中矩阵的大小为4×4）。</p><p>对于输入中的每个通道，Max Pooling将4×4的矩阵划分成4个2×2的子矩阵（子矩阵大小可以人为设定），只取出每个子矩阵中的最大值就得到一个2×2的矩阵。又因为该Max Pooling层的输入有2个通道，所以其输出就是一个大小为2×2×2的张量。</p><h3 id="取最大值的话，该怎么微分？"><a href="#取最大值的话，该怎么微分？" class="headerlink" title="取最大值的话，该怎么微分？"></a>取最大值的话，该怎么微分？</h3><p>见笔记“Tips for Training DNN”Maxout部分，Max Pooling和Maxout其实是一样的。</p><h2 id="Flatten-amp-FNN"><a href="#Flatten-amp-FNN" class="headerlink" title="Flatten &amp; FNN"></a>Flatten &amp; FNN</h2><p>经过一系列的卷积和最大池化，将得到的特征图展开排列，作为FNN的输入，最后输出结果。</p><p><img src="https://pic4.zhimg.com/80/v2-591c85b68d94cb41194b8a70303a063c_720w.png" alt="img" style="zoom: 80%;"></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;卷积神经网络（CNN）常常被用来做图像处理，当然也可以用一般的神经网络，那它们各自有什么优缺点呢？&lt;/p&gt;
&lt;h2 id=&quot;FNN用于图片处理的缺点&quot;&gt;&lt;a href=&quot;#FNN用于图片处理的缺点&quot; class=&quot;headerlink&quot; title=&quot;FNN用于图片处理的缺
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积神经网络" scheme="https://chouxianyu.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-4.5逻辑回归Python实战</title>
    <link href="https://chouxianyu.github.io/2021/03/28/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4-5%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92Python%E5%AE%9E%E6%88%98/"/>
    <id>https://chouxianyu.github.io/2021/03/28/李宏毅机器学习课程笔记-4-5逻辑回归Python实战/</id>
    <published>2021-03-28T02:40:20.000Z</published>
    <updated>2021-03-28T03:34:31.552Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework2的记录。</p><p>关注我的公众号：<code>臭咸鱼</code>，回复<code>LHY</code>可获取课程PPT、数据和代码下载链接。</p><ul><li><p>任务描述（Task Description）</p><p>  二分类（Binary Classification）</p><p>  根据个人资料，判断每个人的年收入是否超过50000美元。</p></li><li><p>数据集描述（Dataset Description）</p><ul><li>train.csv</li><li>test_no_label.csv</li><li>x_train.csv</li><li>Y_train.csv</li><li>X_test.csv</li></ul></li><li><p>参考链接</p><p>  <a href="https://colab.research.google.com/drive/1JaMKJU7hvnDoUfZjvUKzm9u-JLeX6B2C" target="_blank" rel="noopener">https://colab.research.google.com/drive/1JaMKJU7hvnDoUfZjvUKzm9u-JLeX6B2C</a></p></li><li><p>代码</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">np.random.seed(<span class="number">0</span>) <span class="comment"># 使每次随机生成的数字相同</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 函数定义</span></span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalize</span><span class="params">(X, train=True, specified_column=None, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalizes specific columns of X.</span></span><br><span class="line">    <span class="comment"># The mean and standard variance of training data will be reused when processing testing data.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#     X: data to be processed</span></span><br><span class="line">    <span class="comment">#     train: 'True' when processing training data, 'False' for testing data</span></span><br><span class="line">    <span class="comment">#     specific_column: indexes of the columns that will be normalized. If 'None', all columns</span></span><br><span class="line">    <span class="comment">#         will be normalized.</span></span><br><span class="line">    <span class="comment">#     X_mean: mean value of training data, used when train = 'False'</span></span><br><span class="line">    <span class="comment">#     X_std: standard deviation of training data, used when train = 'False'</span></span><br><span class="line">    <span class="comment"># Outputs:</span></span><br><span class="line">    <span class="comment">#     X: normalized data</span></span><br><span class="line">    <span class="comment">#     X_mean: computed mean value of training data</span></span><br><span class="line">    <span class="comment">#     X_std: computed standard deviation of training data</span></span><br><span class="line">    <span class="keyword">if</span> specified_column <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        specified_column = np.arange(X.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X[:, specified_column], axis=<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        X_std = np.std(X[:, specified_column], axis=<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    X[:, specified_column] = (X[:, specified_column] - X_mean) / (X_std + <span class="number">1e-8</span>)</span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集划分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_valid_split</span><span class="params">(X, Y, valid_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and validation set.</span></span><br><span class="line">    train_size = int(len(X) * (<span class="number">1</span> - valid_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据打乱</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two equal-length list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> (X[randomize], Y[randomize])</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmoid function can be used to calculate probability.</span></span><br><span class="line">    <span class="comment"># To avoid overflow, minimum/maximum output value is set.</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - ( <span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This is the logistic regression function, parameterized by w and b</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguements:</span></span><br><span class="line">    <span class="comment">#     X: input data, shape = [batch_size, data_dimension]</span></span><br><span class="line">    <span class="comment">#     w: weight vector, shape = [data_dimension, ]</span></span><br><span class="line">    <span class="comment">#     b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#     predicted probability of each row of X being positively labeled, shape = [batch_size, ]</span></span><br><span class="line">    <span class="keyword">return</span> _sigmoid(np.matmul(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X </span></span><br><span class="line">    <span class="comment"># by rounding the result of logistic regression function.</span></span><br><span class="line">    <span class="keyword">return</span> np.round(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算精度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes the cross entropy.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguements:</span></span><br><span class="line">    <span class="comment">#     y_pred: probabilistic predictions, float vector</span></span><br><span class="line">    <span class="comment">#     Y_label: ground truth labels, bool vector</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#     cross entropy, scalar</span></span><br><span class="line">    <span class="keyword">return</span> -np.dot(Y_label, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label), np.log(<span class="number">1</span> - y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradient</span><span class="params">(X, Y_label, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes the gradient of cross entropy loss with respect to weight w and bias b.</span></span><br><span class="line">    Y_pred = _f(X, w, b)</span><br><span class="line">    pred_error = Y_label - Y_pred</span><br><span class="line">    w_grad = -np.sum(pred_error * X.T, axis=<span class="number">1</span>)</span><br><span class="line">    b_grad = -np.sum(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, b_grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 文件路径</span></span><br><span class="line">X_train_fpath = <span class="string">'../data/X_train.csv'</span></span><br><span class="line">Y_train_fpath = <span class="string">'../data/Y_train.csv'</span></span><br><span class="line">X_test_fpath = <span class="string">'../data/X_test.csv'</span></span><br><span class="line">output_fpath = <span class="string">'output.csv'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 读取数据</span></span><br><span class="line"><span class="keyword">with</span> open(X_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    X_train = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float) <span class="comment"># 不要第一列的ID</span></span><br><span class="line">    <span class="comment"># print(X_train)</span></span><br><span class="line"><span class="keyword">with</span> open(Y_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    Y_train = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float)<span class="comment"># 不要第一列的ID，只取第二列</span></span><br><span class="line">    <span class="comment"># print(Y_train)</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    X_test = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float)</span><br><span class="line">    <span class="comment"># print(X_test)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 数据集处理</span></span><br><span class="line"><span class="comment"># 训练集和测试集normalization</span></span><br><span class="line">X_train, X_mean, X_std = _normalize(X_train, train=<span class="keyword">True</span>)</span><br><span class="line">X_test, _, _ = _normalize(X_test, train=<span class="keyword">False</span>, specified_column=<span class="keyword">None</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"><span class="comment"># 训练集验证集划分</span></span><br><span class="line">X_train, Y_train, X_valid, Y_valid = _train_valid_split(X_train,Y_train, valid_ratio=<span class="number">0.1</span>)</span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">valid_size = X_valid.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line">print(<span class="string">'Size of training set: &#123;&#125;'</span>.format(train_size))</span><br><span class="line">print(<span class="string">'Size of validation set: &#123;&#125;'</span>.format(valid_size))</span><br><span class="line">print(<span class="string">'Size of testing set: &#123;&#125;'</span>.format(test_size))</span><br><span class="line">print(<span class="string">'Dimension of data: &#123;&#125;'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练（使用小批次梯度下降法，Mini-batch training）</span></span><br><span class="line"><span class="comment"># 参数初始化</span></span><br><span class="line">w = np.zeros((data_dim, ))</span><br><span class="line">b = np.zeros((<span class="number">1</span>, ))</span><br><span class="line"><span class="comment"># 训练参数</span></span><br><span class="line">max_iter = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"><span class="comment"># 保存每个epoch的loss以作图</span></span><br><span class="line">train_loss = []</span><br><span class="line">valid_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">valid_acc = []</span><br><span class="line">step = <span class="number">1</span></span><br><span class="line"><span class="comment"># 迭代</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(max_iter):</span><br><span class="line">    <span class="comment"># 打乱训练集</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(int(np.floor(X_train.shape[<span class="number">0</span>] / batch_size))):</span><br><span class="line">        <span class="comment"># 取batch</span></span><br><span class="line">        X = X_train[idx * batch_size : idx * batch_size + batch_size]</span><br><span class="line">        Y = Y_train[idx * batch_size : idx * batch_size + batch_size]</span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line">        <span class="comment"># 梯度下降（learning rate decay with time）</span></span><br><span class="line">        w = w - learning_rate / np.sqrt(step) * w_grad</span><br><span class="line">        b = b - learning_rate / np.sqrt(step) * b_grad</span><br><span class="line">        step = step + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 计算训练集和验证集的loss和精度</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.round(y_train_pred)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size)</span><br><span class="line">    y_valid_pred = _f(X_valid, w, b)</span><br><span class="line">    Y_valid_pred = np.round(y_valid_pred)</span><br><span class="line">    valid_acc.append(_accuracy(Y_valid_pred, Y_valid))</span><br><span class="line">    valid_loss.append(_cross_entropy_loss(y_valid_pred, Y_valid) / valid_size)</span><br><span class="line">print(<span class="string">'Training loss: &#123;&#125;'</span>.format(train_loss[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">'Validation loss: &#123;&#125;'</span>.format(valid_loss[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">'Training accuracy: &#123;&#125;'</span>.format(train_acc[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">'Validation accuracy: &#123;&#125;'</span>.format(valid_acc[<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练过程可视化</span></span><br><span class="line"><span class="comment"># loss可视化</span></span><br><span class="line">plt.plot(train_loss)</span><br><span class="line">plt.plot(valid_loss)</span><br><span class="line">plt.title(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'valid'</span>])</span><br><span class="line">plt.savefig(<span class="string">'Loss.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># accuracy可视化</span></span><br><span class="line">plt.plot(train_acc)</span><br><span class="line">plt.plot(valid_acc)</span><br><span class="line">plt.title(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'valid'</span>])</span><br><span class="line">plt.savefig(<span class="string">'Accuracy.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 预测测试集</span></span><br><span class="line">predictions = _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id,label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;,&#123;&#125;\n'</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 寻找最重要的10个维度的特征</span></span><br><span class="line">index = np.argsort(np.abs(w))[::<span class="number">-1</span>] <span class="comment"># 将w按绝对值从大到小排序</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    features = np.array(f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index[:<span class="number">10</span>]:</span><br><span class="line">        print(features[i], w[i])</span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework2的记录。&lt;/p&gt;
&lt;p&gt;关注我的公众号：&lt;code&gt;臭咸鱼&lt;/code&gt;，回复&lt;code&gt;LHY&lt;/code&gt;可获取课程PPT、数据和代码下载链接。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;任务描述（Task Desc
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="https://chouxianyu.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="https://chouxianyu.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="分类" scheme="https://chouxianyu.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-4.4概率生成模型Python实战</title>
    <link href="https://chouxianyu.github.io/2021/03/28/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-4-4%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8BPython%E5%AE%9E%E6%88%98/"/>
    <id>https://chouxianyu.github.io/2021/03/28/李宏毅机器学习课程笔记-4-4概率生成模型Python实战/</id>
    <published>2021-03-28T02:12:34.000Z</published>
    <updated>2021-03-28T02:56:16.379Z</updated>
    
    <content type="html"><![CDATA[<p>本文为作者学习李宏毅机器学习课程时参照样例完成homework2的记录。</p><p>关注我的公众号：<code>臭咸鱼</code>，回复<code>LHY</code>可获取课程PPT、数据和代码下载链接。</p><ul><li><p>任务描述（Task Description）</p><p>  二分类（Binary Classification）</p><p>  根据个人资料，判断每个人的年收入是否超过50000美元。</p></li><li><p>数据集描述（Dataset Description）</p><ul><li>train.csv</li><li>test_no_label.csv</li><li>x_train.csv</li><li>Y_train.csv</li><li>X_test.csv</li></ul></li><li><p>参考链接</p><p>  <a href="https://colab.research.google.com/drive/1JaMKJU7hvnDoUfZjvUKzm9u-JLeX6B2C" target="_blank" rel="noopener">https://colab.research.google.com/drive/1JaMKJU7hvnDoUfZjvUKzm9u-JLeX6B2C</a></p></li><li><p>代码</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 文件路径</span></span><br><span class="line">X_train_fpath = <span class="string">'../data/X_train.csv'</span></span><br><span class="line">Y_train_fpath = <span class="string">'../data/Y_train.csv'</span></span><br><span class="line">X_test_fpath = <span class="string">'../data/X_test.csv'</span></span><br><span class="line">output_fpath = <span class="string">'output.csv'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 函数定义</span></span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalize</span><span class="params">(X, train=True, specified_column=None, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalizes specific columns of X.</span></span><br><span class="line">    <span class="comment"># The mean and standard variance of training data will be reused when processing testing data.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#     X: data to be processed</span></span><br><span class="line">    <span class="comment">#     train: 'True' when processing training data, 'False' for testing data</span></span><br><span class="line">    <span class="comment">#     specific_column: indexes of the columns that will be normalized. If 'None', all columns</span></span><br><span class="line">    <span class="comment">#         will be normalized.</span></span><br><span class="line">    <span class="comment">#     X_mean: mean value of training data, used when train = 'False'</span></span><br><span class="line">    <span class="comment">#     X_std: standard deviation of training data, used when train = 'False'</span></span><br><span class="line">    <span class="comment"># Outputs:</span></span><br><span class="line">    <span class="comment">#     X: normalized data</span></span><br><span class="line">    <span class="comment">#     X_mean: computed mean value of training data</span></span><br><span class="line">    <span class="comment">#     X_std: computed standard deviation of training data</span></span><br><span class="line">    <span class="keyword">if</span> specified_column <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        specified_column = np.arange(X.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X[:, specified_column], axis=<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        X_std = np.std(X[:, specified_column], axis=<span class="number">0</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    X[:, specified_column] = (X[:, specified_column] - X_mean) / (X_std + <span class="number">1e-8</span>)</span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集划分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_valid_split</span><span class="params">(X, Y, valid_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and validation set.</span></span><br><span class="line">    train_size = int(len(X) * (<span class="number">1</span> - valid_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmoid function can be used to calculate probability.</span></span><br><span class="line">    <span class="comment"># To avoid overflow, minimum/maximum output value is set.</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - ( <span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This is the logistic regression function, parameterized by w and b</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguements:</span></span><br><span class="line">    <span class="comment">#     X: input data, shape = [batch_size, data_dimension]</span></span><br><span class="line">    <span class="comment">#     w: weight vector, shape = [data_dimension, ]</span></span><br><span class="line">    <span class="comment">#     b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#     predicted probability of each row of X being positively labeled, shape = [batch_size, ]</span></span><br><span class="line">    <span class="keyword">return</span> _sigmoid(np.matmul(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X </span></span><br><span class="line">    <span class="comment"># by rounding the result of logistic regression function.</span></span><br><span class="line">    <span class="keyword">return</span> np.round(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算精度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 读取数据</span></span><br><span class="line"><span class="keyword">with</span> open(X_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    X_train = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float) <span class="comment"># 不要第一列的ID</span></span><br><span class="line">    <span class="comment"># print(X_train)</span></span><br><span class="line"><span class="keyword">with</span> open(Y_train_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    Y_train = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float)<span class="comment"># 不要第一列的ID，只取第二列</span></span><br><span class="line">    <span class="comment"># print(Y_train)</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    next(f) <span class="comment"># 不需要第一行的表头</span></span><br><span class="line">    X_test = np.array([line.strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)[<span class="number">1</span>:] <span class="keyword">for</span> line <span class="keyword">in</span> f], dtype=float)</span><br><span class="line">    <span class="comment"># print(X_test)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 数据集处理</span></span><br><span class="line"><span class="comment"># 训练集和测试集normalization</span></span><br><span class="line">X_train, X_mean, X_std = _normalize(X_train, train=<span class="keyword">True</span>)</span><br><span class="line">X_test, _, _ = _normalize(X_test, train=<span class="keyword">False</span>, specified_column=<span class="keyword">None</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 计算每个类别的样本的平均值和协方差</span></span><br><span class="line"><span class="comment"># 区分类别</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y==<span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y==<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 计算每个类别的样本的平均值</span></span><br><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>) <span class="comment"># 计算每个维度特征的平均值</span></span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 计算每个类别的样本的协方差矩阵（可以研究下协方差矩阵是如何计算的以及为什么）</span></span><br><span class="line">cov_0 = np.zeros((data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros((data_dim, data_dim))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>] <span class="comment"># transpose没有参数的话，就是转置，计算协方差矩阵时需要转置</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 计算共享协方差矩阵（Shared covariance is taken as a weighted average of individual in-class covariance）</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train_0.shape[<span class="number">0</span>] + X_train_1.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 计算权重和偏置</span></span><br><span class="line"><span class="comment"># 计算协方差矩阵的逆矩阵</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and accurately.</span></span><br><span class="line">u, s, v = np.linalg.svd(cov, full_matrices=<span class="keyword">False</span>)</span><br><span class="line">inv = np.matmul(v.T * <span class="number">1</span> / s, u.T)</span><br><span class="line"><span class="comment"># 计算weight和bias</span></span><br><span class="line">w = np.dot(inv, mean_0 - mean_1)</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.dot(inv, mean_0)) + <span class="number">0.5</span> * np.dot(mean_1, np.dot(inv, mean_1)) + np.log(float(X_train_0.shape[<span class="number">0</span>])) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 计算在训练集上的准确率</span></span><br><span class="line">Y_train_pred = <span class="number">1</span> - _predict(X_train, w, b)</span><br><span class="line">print(<span class="string">'Training accuracy: &#123;&#125;'</span>.format(_accuracy(Y_train_pred, Y_train)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 预测测试集结果</span></span><br><span class="line">predictions = <span class="number">1</span> - _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id,label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;,&#123;&#125;\n'</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 寻找最重要的10个维度的特征</span></span><br><span class="line">index = np.argsort(np.abs(w))[::<span class="number">-1</span>] <span class="comment"># 将w按绝对值从大到小排序</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    features = np.array(f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index[:<span class="number">10</span>]:</span><br><span class="line">        print(features[i], w[i])</span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文为作者学习李宏毅机器学习课程时参照样例完成homework2的记录。&lt;/p&gt;
&lt;p&gt;关注我的公众号：&lt;code&gt;臭咸鱼&lt;/code&gt;，回复&lt;code&gt;LHY&lt;/code&gt;可获取课程PPT、数据和代码下载链接。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;任务描述（Task Desc
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类" scheme="https://chouxianyu.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
      <category term="概率生成模型" scheme="https://chouxianyu.github.io/tags/%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>《基于深度学习的显著性目标检测综述》阅读笔记</title>
    <link href="https://chouxianyu.github.io/2021/03/20/%E3%80%8A%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://chouxianyu.github.io/2021/03/20/《基于深度学习的显著性目标检测综述》阅读笔记/</id>
    <published>2021-03-20T09:57:10.000Z</published>
    <updated>2021-03-20T09:58:22.201Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  基于深度学习的显著性目标检测综述</p></li><li><p>作者</p><p>  史彩娟等</p></li><li><p>发表时间</p><p>  2020年</p></li><li><p>来源</p><p>  知网</p></li></ul><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>本文基本研究对象为：基于深度学习的SOD算法。</p><p>下文中将显著性目标检测简写为SOD（Salient Object Detection），将基于深度学习的显著性目标检测算法简写为DSOD（Deep Learning Based SOD）。</p><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><ul><li><p>知识</p><ul><li><p>低层特征中包含更多的边界信息</p><ul><li>通过编码低层特征距离来检测边界信息可能导致部分边界细节丢失。</li><li>通过引入相关操作来增强低层特征具有的边界信息，可以获得更清晰的边界，但容易造成显著性目标检测主体不准确的问题。</li></ul></li><li><p>高层特征中包含更多的语义信息。</p><ul><li>只对高层特征所包含的语义进行增强，有时会造成显著性目标边界模糊或者多个显著性目标重合。</li></ul></li><li><p>只进行边界增强容易造成显著性目标不准确，而只进行语义增强则会引起显著性目标的边界不准确，所以可以同时增强两者。</p><p>  因为通过语义增强可以减小无效目标的干扰，更好地定位显著性目标的位置；通过边界增强可以获得清晰的显著性目标边界。</p><ul><li>金字塔结构可以处理高低层的特征</li></ul></li><li><p>全局信息（颜色，纹理，背景/前景等）包含显著性目标的位置信息，而局部信息可以增强显著性目标边界。</p></li></ul></li><li><p>一些未知的东西</p><ul><li>BASNet是怎么实现的？其中的混合Loss是什么</li><li>Boundary-Enhanced Loss是什么？</li><li>注意力机制（Attention）是什么？</li><li>金字塔结构是什么？</li><li>层次递归卷积神经网络(Hierarchical Recurrent Convolutional Neural Network，HRCNN)是什么？</li><li>字幕网络(Image Captioning Network，ICN)是什么？</li><li>SqueezeNet是什么？3种设计原则？</li><li>MobileNet是什么？深度可分离卷积？</li><li>可变形卷积是什么？</li><li>评估指标<ul><li>F-度量($F-measure,F_\beta$)</li><li>加权F-度量($Weighted\  F-measure,F_\beta^\omega$)</li><li>P-R曲线</li><li>平均绝对误差(Mean Absolute Error, MAE)</li></ul></li></ul></li><li><p>思考</p><ul><li>研究至少有2个思路：横向（分类）和纵向（深入）。</li></ul></li></ul><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><ol><li><p>根据原理不同，分3类介绍并定性分析比较DSOD。</p><p> 3个类别为：边界/语义增强、全局/局部结合、辅助网络</p></li><li><p>简单介绍DSOD的常用数据集和评估准则</p></li><li><p>现有DSOD方法在多个数据集上进行多方面的性能比较</p><p> 包括定量比较、P-R曲线和视觉比较</p></li><li><p><strong>现有DSOD算法在复杂背景、小目标、实时性检测等方面的不足</strong></p></li><li><p><strong>DSOD的未来发展方向，如复杂背景、实时、小目标、弱监督</strong></p></li></ol><h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><ul><li>传统SOD方法主要利用<strong>人类直观感觉或启发式先验</strong>，如利用色度比较，背景比较和边界点先验等，通过人工提取特征来检测目标，但人工提取特征非常耗时。</li><li>DSOD可自动学习到<strong>多尺度</strong>特征，精度速度大幅提升，但也存在<strong>不足</strong>：复杂背景下的性能有待提升、实时性需提高、模型复杂度需降低。</li></ul><h1 id="2-基于深度学习的显著性目标检测方法"><a href="#2-基于深度学习的显著性目标检测方法" class="headerlink" title="2 基于深度学习的显著性目标检测方法"></a>2 基于深度学习的显著性目标检测方法</h1><ul><li>传统方法中人工提取特征耗时或者迁移性较差</li><li>DSOD可分为3个类别：边界/语义增强、全局/局部结合、辅助网络</li><li>==图1==给出了今年DSOD的发展历程和主要算法</li></ul><h2 id="2-1-基于边界-语义增强的SOD"><a href="#2-1-基于边界-语义增强的SOD" class="headerlink" title="2.1 基于边界/语义增强的SOD"></a>2.1 基于边界/语义增强的SOD</h2><h3 id="2-1-1-基于边界增强的SOD方法"><a href="#2-1-1-基于边界增强的SOD方法" class="headerlink" title="2.1.1 基于边界增强的SOD方法"></a>2.1.1 基于边界增强的SOD方法</h3><ul><li><p>边界增强是指<strong>通过增强深度特征中的低层特征来获得更多的边界信息，从而更好的定位显著目标边界</strong>。</p><ul><li><p>ELD算法和KSR算法<strong>通过编码低层特征距离来检测边界信息</strong>，定位显著性目标轮廓，但是轮廓边界有时会模糊，导致部分边界细节丢失。</p></li><li><p>DCL算法和DSS算法<strong>通过引入相关操作来增强低层特征具有的边界信息</strong>。</p><p>  相较于直接编码低层特征距离的算法，这类方法获得的显著性目标<strong>边界更加清晰</strong>。但是，这些操作的引入容易引起显著性目标检测<strong>主体不准确</strong>，比如只有1个目标却检测出了2个。</p><p>原文中==图2==展示并对比了上述4个算法的检测效果。</p></li></ul></li><li><p>还可以<strong>直接对显著性目标的边界进行检测</strong>，比如GearNet、AFNet（<strong>采用BEL，Boundary-Enhanced Loss</strong>）、BASNet。</p><p>  这类方法能够提取清晰的显著性目标边界，边界细节相对较好，显著性目标的检测准确度较高(无关的显著性目标较少)，效果见原文==图3==。</p></li></ul><h3 id="2-1-2-基于语义增强的SOD方法"><a href="#2-1-2-基于语义增强的SOD方法" class="headerlink" title="2.1.2 基于语义增强的SOD方法"></a>2.1.2 基于语义增强的SOD方法</h3><ul><li><p>语义增强是指从<strong>高层特征</strong>中获得丰富的<strong>语义</strong>信息，从而更好的定位显著性目标，使显著性目标更加突出。</p><p>  比如R_FCN算法、CPD算法和PoolNet算法等，算法效果见原文==图4==。</p><p>  这类方法可以<strong>准确定位</strong>显著性目标，但是由于<strong>仅针对</strong>高层特征所包含的语义进行增强，有时会造成显著性目标<strong>边界模糊</strong>或者多个显著性目标重合。</p></li><li><p>还可以通过引入<strong>注意力机制</strong>进一步增强语义信息，如PiCANet和RAS算法。主体准确性和边界准确性都不错，算法效果见原文==图5==。</p></li></ul><h3 id="2-1-3-基于边界-语义增强的SOD方法"><a href="#2-1-3-基于边界-语义增强的SOD方法" class="headerlink" title="2.1.3 基于边界/语义增强的SOD方法"></a>2.1.3 基于边界/语义增强的SOD方法</h3><ul><li><p>只进行边界增强容易造成显著性目标模糊，而只进行语义增强则会引起显著性目标的边界模糊，所以可以<strong>同时对两者进行增强</strong>。</p><p>  因为通过语义增强可以减小无效目标的干扰，更好地定位显著性目标的位置；通过边界增强可以获得清晰的显著性目标边界</p><p>  这类算法有Amulet、BDMPM等，算法效果见原文==图6==。</p></li><li><p>还可以采用<strong>金字塔结构</strong>同时对高低层特征进行处理，以同时增强显著性目标边界和语义。</p><p>  这类算法有SRM算法、PAGE算法、FPA算法，算法效果见原文==图7==。</p></li></ul><h2 id="2-2-基于全局-局部结合的SOD"><a href="#2-2-基于全局-局部结合的SOD" class="headerlink" title="2.2 基于全局/局部结合的SOD"></a>2.2 基于全局/局部结合的SOD</h2><ul><li><p><strong>全局信息</strong>（颜色，纹理，背景/前景等）包含显著性目标的位置信息，而<strong>局部信息</strong>可以增强显著性目标边界。</p></li><li><p>一些检测方法采用<strong>递归</strong>操作、<strong>多分辨率</strong>操作和<strong>注意力机制</strong>等将全局/局部相结合以 获得更好的显著性目标检测性能</p></li><li><p>这类算法有DHSNet、GRL、NLDF、PAGR算法等，算法效果见原文==图8==。</p></li></ul><h2 id="2-3-基于辅助网络的SOD"><a href="#2-3-基于辅助网络的SOD" class="headerlink" title="2.3 基于辅助网络的SOD"></a>2.3 基于辅助网络的SOD</h2><ul><li>基于辅助网络的显著性目标检测是指采用其它领域已有模型作为辅助网络来提升显著性目标检测性能</li><li>这类算法有MDF、C2S-Net、CapSal、MLMSNet算法等，算法效果见原文==图9==。</li></ul><h2 id="2-4-不同类型SOD方法分析比较"><a href="#2-4-不同类型SOD方法分析比较" class="headerlink" title="2.4 不同类型SOD方法分析比较"></a>2.4 不同类型SOD方法分析比较</h2><ul><li>现有DSOD方法边界模糊的原因和解决办法<ol><li>深度模型包含许多下采样操作，上采样后的特征难以恢复原有的空间信息，融合后引起边界模糊。因此，为了减小<strong>下采样操作引起的多尺度融合损失</strong>，引入一些特定操作，如 PoolNet 算法中采用<strong>功能聚合模块</strong>等。</li><li>针对不同因素对边界检测的影响，通过<strong>编码低层特征距离</strong>来检测边界信息，定位显著性目标轮廓，如ELD算法和KSR算法；或者是<strong>设计新的损失函数</strong>， 通过反向传播调整模型参数，如AFNet算法和BASNet算法。</li><li>基础<strong>模型简易导致检测的边界模糊</strong>，可以通过<strong>多尺度操作</strong>增强原有的特征效果，如DSS算法、SRM算法和PAGE算法等，或添加<strong>注意力机制</strong>来提取更有效的低层特征，如PFA算法等。</li></ol></li><li>基于深度学习的显著性目标检测方法中常常引入注意力机制，大致可分为3类<ol><li><strong>时空域注意力</strong>，比较适合同时具有时序及空域特征的场景，通过递归神经网络 （Recurrent Neural Network，RNN）设计注意力机制，如PAGR算法。</li><li><strong>软注意力</strong>，是一种确定性的注意力，可以直接通过网络生成，它也是可微的， 可以通过神经网络算出梯度并且前向传播和后向 反馈来学习得到注意力的权重，如PFA算法和RAS算法；</li><li><strong>硬注意力</strong>，从输入信息中选择重要的特征，如PiCANet算法每个像素生成注意力图，这种方式更高效和直接。</li></ol></li></ul><h1 id="3-常用数据集及评估标准"><a href="#3-常用数据集及评估标准" class="headerlink" title="3 常用数据集及评估标准"></a>3 常用数据集及评估标准</h1><h2 id="3-1-常用数据集"><a href="#3-1-常用数据集" class="headerlink" title="3.1 常用数据集"></a>3.1 常用数据集</h2><ul><li><p>SOD数据集</p><p>  MSRA、SOD、MSRA10K、HKU-IS、DUTS、SED、ECSSD、DUTO-OMRON、PASCAL-S</p></li><li><p>常用DSOD数据集</p><ul><li>MSRA10K：边界框级别的显著性真值标定</li><li>HKU-IS：4447个图像，多个断开连接的显著性目标，多目标的边界重合和色彩对比度较低</li><li>DUTS：10553个训练图像和5019个测试图像，训练和测试集都包含非常重要的场景</li><li>SOD：300张图像，像素级注释，大部分图像包含多个显著性目标，并且目标与背景的颜色对比度较低。</li><li>ECSSD：1000张图像，图像具有复杂的结构和背景</li><li>DUTO-OMRON：5168个高质量图像，图像具有多个显著性目标，背景相对复杂</li><li>PASCAL-S：8 个类别，850张图像，用于评估具有复杂背景、多个目标场景的模型性能</li></ul></li></ul><h2 id="3-2-常用评估准则"><a href="#3-2-常用评估准则" class="headerlink" title="3.2 常用评估准则"></a>3.2 常用评估准则</h2><p>DSOD常用评估准则</p><ul><li><p>F-度量($F-measure,F_\beta$)</p><p>  对精度和召回率进行总体评估，最终值越大表明性能越好，其中$\beta$是一个参数，一般取$\beta^2=0.3$。公式暂略。</p></li><li><p>加权F-度量($Weighted\  F-measure,F_\beta^\omega$)</p><p>  加权F-度量是F-度量的推广，通过交替计算精度和召回率得到。</p><p>  加权F-度量为了解决邻域信息的不同， 为不同位置的不同误差分配了不同的权重。公式暂略。</p></li><li><p>P-R曲线</p><p>  以Precision和Recall作为纵-横轴坐标的二维曲线，即查准率-查全率曲线，选取不同阈值时对应的精度和召回率绘制。P-R曲线围起来 的面积是AP(Average Precision)值，AP值越高，模型性能越好。公式暂略。</p></li><li><p>平均绝对误差(Mean Absolute Error, MAE)</p><p>  MAE值越小表示模型越好。公式暂略。</p></li></ul><h1 id="4-基于深度学习的显著性目标检测方法性能比较"><a href="#4-基于深度学习的显著性目标检测方法性能比较" class="headerlink" title="4 基于深度学习的显著性目标检测方法性能比较"></a>4 基于深度学习的显著性目标检测方法性能比较</h1><ul><li>在数据集ECSSD、DUT-OMRON、HKU-IS和DUTS-TE上进行实验，采用F-度量和平均绝对误差MAE作为评估准则</li></ul><h1 id="5-基于深度学习的显著性目标检测算法的不足与未来展望"><a href="#5-基于深度学习的显著性目标检测算法的不足与未来展望" class="headerlink" title="5 基于深度学习的显著性目标检测算法的不足与未来展望"></a>5 基于深度学习的显著性目标检测算法的不足与未来展望</h1><ul><li><p>不足与未来展望</p><ul><li><p>复杂背景</p><ul><li>设计适应复杂背景（对背景敏感或者前景背景对比度低等）的显著性目标检测模型</li><li>建立包含复杂背景的图像数据集</li></ul></li><li><p>实时性</p><ul><li>设计轻量化网络，比如遵守SqueezeNet特有的三种设计原则</li><li>采用深度可分离卷积，比如MobileNet</li><li>对网络直接进行压缩与编码</li></ul></li><li><p>小目标</p><ul><li>使用分辨率更高的卷积特征图以及残差模块来增强对小目标的检测能力</li></ul></li><li><p>矩形框定位</p><ul><li>可变形卷积</li></ul></li><li><p>完全监督学习</p><p>  无监督或弱监督</p></li></ul></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文名称&lt;/p&gt;
&lt;p&gt;  基于深度学习的显著性目标检测综述&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;作者&lt;/p
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>PoolNet论文详解</title>
    <link href="https://chouxianyu.github.io/2021/03/19/PoolNet%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/"/>
    <id>https://chouxianyu.github.io/2021/03/19/PoolNet论文详解/</id>
    <published>2021-03-19T12:20:04.000Z</published>
    <updated>2021-03-19T12:25:47.568Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  A Simple Pooling-Based Design for Real-Time Salient Object Detection</p></li><li><p>作者</p><p>  Jiang-Jiang Liu, Qibin Hou, <strong>Ming-Ming Cheng</strong>等</p></li><li><p>发表时间</p><p>  2020年</p></li><li><p>来源</p><p>  CVPR2019</p></li></ul><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><ul><li>知识<ul><li>low-level和high-level可以指人类意识中的低级和高级，也可以指CNN物理结构中的浅层（低层）和深层（高层）。</li><li><strong>因为</strong>CNN类似于金字塔的结构特点；其浅层阶段具有较大尺寸并保留丰富的低级信息；其深层阶段包含更多高级语义信息也<strong>更容易从中得到显著目标的位置</strong>，但很coarse（粗糙）。</li></ul></li><li>一些未知的东西<ul><li>FPN</li><li>上采样</li><li>ResNet</li><li>PPM</li><li>Richer convolutional features for edge detection</li><li>SRM（A stagewise refinement model for detecting salient objects in images）</li><li>weight decay</li><li>balanced binary cross entropy loss</li><li>standard binary cross entropy loss，BCE？</li></ul></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ol><li><p>思路</p><p> 通过扩展<strong>池化</strong>在CNN中的作用，实现SOD。</p></li><li><p>实现</p><p> 基于<strong>U-shape</strong> architecture，构造2个模块</p><ol><li><p>GGM（global guidance module）</p><p> 自底向上，用来<strong>为不同层的特征图提供</strong>潜在显著目标的位置信息。</p></li><li><p>FAM（feature aggregation module）</p><p> 从顶向下，使coarse-level的语义信息和fine-level的特征<strong>较好地融合</strong>。</p><p> 在自顶向下的融合操作之后添加FAM，可以将GGM中coarse-level的特征与不同尺度的特征无缝融合。</p><p>这2个模块使高级语义信息逐渐完善，生成信息丰富的显著性图。</p></li></ol></li><li><p>通过锐化细节，和SOTA相比可以更精确地定位目标。</p></li><li><p>300×400的图片，FPS超过30，代码<a href="http://mmcheng.net/poolnet/。" target="_blank" rel="noopener">http://mmcheng.net/poolnet/。</a></p></li></ol><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li><p>U-shape结构</p><ul><li>多数传统方法通过手工设计的特征，单独或同时捕捉局部细节和全局上下文信息，但其性能因缺乏高层语义信息而受到限制，而CNN可以在不同尺度空间中提取高级语义信息和低级细节特征。</li><li><strong>因为</strong>CNN类似于金字塔的结构特点，其浅层阶段具有较大尺寸并保留丰富的低级信息，其深层阶段包含更多高级语义信息也<strong>更容易从中得到显著目标的位置</strong>。</li><li><strong>基于上述知识</strong>，学者设计了许多结构，其中U-shape的结构最受关注，其通过在分类网络上构建自上而下的路径来构建丰富的特征图。</li></ul></li><li><p>U-shape结构还存在提升空间</p><ul><li><p>存在问题</p><ol><li>因为U-shape结构中高级语义信息（位于深层网络）是逐步传到浅层的，与此同时深层捕捉到的位置信息可能会被稀释。</li><li>一个CNN的感受野大小和其深度是不成比例的。</li></ol></li><li><p>解决方法</p><ul><li><p>现有方法</p><p>  现有方法（见原文参考文献）通过引入注意力机制、以循环方式细化特征图、结合多尺度特征信息、向显著图添加额外约束（例如边界loss）等方法解决上述U-shape结构的问题。</p></li><li><p>本文方法</p><p>  本文提出的方法是基于U-shape并扩展池化技术，其中GGM和FAM都是基于FPN（feature pyramid networks）的。</p></li></ul></li></ul></li><li><p><strong>PoolNet</strong></p><p>  因为GGM和FAM都基于池化，所以将本文方法取名为PoolNet。</p><ul><li><p>GGM</p><p>  GGM包括一个修改过的PPM（pyramid pooling module）和一系列GGF（global guiding flows）。</p><ul><li><p>PPM</p><p>  和SRM（A stagewise refinement model for detecting salient objects in images）直接将PPM插入U-shape结构不同，本文提出的GGM是一个独立的模块。具体来讲，即把PPM放在backbone顶部用来捕捉全局指导信息（显著目标的位置）</p></li><li><p>GGF</p><p>  通过GGF，PPM收集到的高级语义信息可以传送到所有金字塔层，弥补了U-shape网络自上而下信号逐渐被稀释的问题。</p></li></ul></li><li><p>FAM</p><p>  考虑到来自GGF的coarse-level特征图与金字塔不同尺度特征图的融合问题，本文提出了一种FAM，其输入为融合后的特征图。</p><p>  该模块首先将融合（==应该是FPN操作？==）得到的特征图转换到多个特征空间，以捕获不同尺度的局部上下文信息，然后将这些信息进行组合以更好地权衡融合的输入特征图的组成。（==最后半句没懂，“权衡”一词具体指什么？==）</p></li></ul></li><li><p>edge detection branch</p><p>  还使用了边缘检测分支（edge detection branch），通过和边缘检测协同训练以锐化显著物体的细节。</p></li><li><p>性能</p><ul><li><p>精度</p><p>  <strong>Without bells and whistles，大幅超过之前的SOTA。</strong></p></li><li><p>速度</p><ul><li>一个NVIDIA Titan Xp GPU，图片尺寸300 × 400，<strong>速度超过30FPS</strong>。</li><li>不使用边缘检测分支时，5000张图片，训练时长不超过6小时，比多数方法快很多，因为池化操作比较快速。</li></ul></li></ul></li></ul><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><p>暂略</p><h1 id="3-PoolNet"><a href="#3-PoolNet" class="headerlink" title="3. PoolNet"></a>3. PoolNet</h1><ul><li>high-level semantic features are helpful for discovering the specific locations of salient objects.</li><li>low- and midlevel features are also essential for improving the features extracted from deep layers from coarse level to fine level.</li></ul><h2 id="3-1-Overall-Pipeline"><a href="#3-1-Overall-Pipeline" class="headerlink" title="3.1. Overall Pipeline"></a>3.1. Overall Pipeline</h2><ul><li>基于FPN（一种U型结构，从底向上和自顶向下，优点是可以组合多层特征）<ul><li>在从底向上之后引入GGM，提取高级语义信息后将其与各个金字塔层融合。</li><li>GGM之后，引入FAM保证不同尺度的特征图可以无缝融合。</li></ul></li></ul><h2 id="3-2-Global-Guidance-Module"><a href="#3-2-Global-Guidance-Module" class="headerlink" title="3.2. Global Guidance Module"></a>3.2. Global Guidance Module</h2><ul><li><p>现有不足</p><ul><li><p>FPN的问题</p><p>  问题之一：自顶向下是在自底向上之后的，就是高级特征被传给低层时会逐渐稀释。</p></li><li><p>CNN的问题</p><p>  根据实验，CNN的感受野比理论上要小得多，<strong>特别是对于较深的层</strong>，所以整个网络的感受野并不足够大以捕捉输入的全局信息。</p></li><li><p>直接影响</p><p>  可以看原文图2，只能检测到显著目标的局部。</p></li></ul></li><li><p>解决方法：GGM</p><p>  使得每个尺度的特征图都可以感知显著目标的位置。</p><ul><li><p>PPM</p><p>  包括4个子分支，作用是<strong>捕获</strong>输入图像的上下文信息。</p><p>  第一和最后一个子分支分别是一个identity mapping layer和一个global average pooling layer。</p><p>  中间的两个分支，我们采用adaptive average pooling layer，以确保它们输出的特征图分别具有3×3和5×5的空间大小。</p></li><li><p>GGF</p><p>  作用是将PPM捕捉到的信息与接下来自顶向下中不同金字塔层的特征图合理地<strong>融合</strong>在一起。</p><p>  与SRM（A stagewise refinement model for detecting salient objects in images）不同，它是将PPM视为U形结构的一部分，而本文中的GGM独立于U形结构。</p><p>  如原文图1中的绿色箭头，通过引入一系列GGF（identity mappings），可以将高级语义信息传递到各个级别的特征。</p><p>  这样，我们在自上而下路径的每个部分中显式增加了全局导航信息的权重，以<strong>确保在构建FPN时不会稀释位置信息</strong>。</p><p>  可以看原文图2，观察GGF的具体作用。</p></li></ul></li></ul><h2 id="3-3-Feature-Aggregation-Module"><a href="#3-3-Feature-Aggregation-Module" class="headerlink" title="3.3. Feature Aggregation Module"></a>3.3. Feature Aggregation Module</h2><ul><li><p>解决的问题</p><p>  使GGM的粗略特征图与金字塔不同尺度的特征地图无缝融合。</p><p>  具体来讲，在原始的FPN（VGGNet版本）中，高层特征图上采样比率为2，所以在上采样后边加一个3×3的卷积可以减少其带来的aliasing effect。</p><p>  但是，<strong>GGF还需要更大的上采样比率</strong>，比如8。所以使用FAM充分、高效地处理GGF和不同金字塔层特征图之间巨大的尺寸差异。</p></li><li><p>FAM</p><ul><li><p>结构</p><p>  每个FAM包含4个子分支，如原文图3所示。在forward过程中，输入的特征图先以不同比率进行下采样（平均池化），然后再以不同比率进行上采样，然后将4个分支融合（sum），然后送入一个3×3的卷积层。</p></li><li><p>优点</p><ul><li>减少上采样带来的aliasing effect，特别是当上采样比率较大（比如8）的时候。</li><li>使每个spatial location（空间位置）可以看到不同尺度空间的局部上下文信息，而且增大了整个网络的感受野。</li></ul></li></ul></li><li><p>实验</p><ul><li><p>原文图4</p><p>  将FAM替换成2个卷积层进行对比，把FAM模块附近的特征图可视化，证明FAM可以更好地捕捉显著目标的位置和细节信息。</p></li><li><p>原文图2</p><p>  f列和g列（尤其是第2行）进行对比，证明引入FAM可以sharpen显著目标的细节信息。</p><p>在下文的实验部分，会给出更多的数值结果。</p></li></ul></li></ul><h1 id="4-Joint-Training-with-Edge-Detection"><a href="#4-Joint-Training-with-Edge-Detection" class="headerlink" title="4. Joint Training with Edge Detection"></a>4. Joint Training with Edge Detection</h1><ul><li><p>问题</p><p>  第3节描述的网络结构已经在多个常用评估准则上超过了之前所有SOTA的单个模型的结果。</p><p>  但是原文作者发现<strong>许多不准确（incomplete or over-predicted）的预测是由于不清晰的目标边界造成的</strong>。</p></li><li><p>Edge Detection Branch</p><p>  在第3节描述的结构中添加1个预测分支，<strong>用来estimate显著目标的边界</strong>，具体结构见原文图1。</p><p>  在3个不同尺度的FAM之后添加3个residual block，<strong>用来information transformation</strong>，这3个residual block和<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>的设计相似并且具有<code>{128,256,512}</code>个通道（从fine level到coarse level）；和<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Richer_Convolutional_Features_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Richer convolutional features for edge detection</a>一文相同，每个residual block后面都有1个16通道的3×3卷积层（<strong>用来feature compression</strong>），还有1个单通道的1×1卷积层（<strong>用来边缘检测</strong>）。</p><p>  将上述3个16通道的3×3特征图进行拼接（concatenate）然后将其送入3个连续的48通道3×3卷积层，以将捕捉到的边缘信息传递给显著性目标检测分支，<strong>用来增强细节</strong>。</p></li><li><p>Train Edge Detection Branch taking the boundaries of salient objects as GT</p><p>  和<a href="http://arxiv.org/abs/1704.03604" target="_blank" rel="noopener">Instancelevel salient object segmentation</a>一文相似，本文在训练阶段将显著目标的边界作为GT<strong>用来联合训练</strong>，然而这并没有带来任何性能提升并且仍然缺少目标边界的细节信息。如图5的c列，当场景的前后景对比度较低时，得到的显著性图和边界图仍然很模糊。<strong>导致这个问题的原因可能是来自显著目标的GT边界图仍然缺少显著目标的大部分细节信息</strong>。GT边界只告诉我们显著目标的外边界的位置，特别是当显著目标之间有重叠的时候。</p></li><li><p>Train Edge Detection Branch taking the boundaries of salient objects as GT</p><p>  根据上述内容，本文尝试了和边缘检测任务实现协同训练，使用和和<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Richer_Convolutional_Features_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Richer convolutional features for edge detection</a>一文中相同的数据集。在训练时，来自显著目标检测数据集和边缘检测数据集的图像被交替输入。如图5所示，和边缘检测任务实现协同训练大幅提升了检测到的显著目标的细节。在下文实验部分，会给出更多的定量分析。</p></li></ul><h1 id="5-Experimental-Results"><a href="#5-Experimental-Results" class="headerlink" title="5. Experimental Results"></a>5. Experimental Results</h1><h2 id="5-1-Experiment-Steup"><a href="#5-1-Experiment-Steup" class="headerlink" title="5.1. Experiment Steup"></a>5.1. Experiment Steup</h2><ul><li><p>实现细节</p><p>  使用PyTorch框架，所有实验中的学习率优化器为Adam（5e-4的weight decay，初始学习率为5e-5然后15个epoch之后除以10）。</p><p>  本文的网络共训练24个epoch。</p><p>  网络backbone（如VGG-16、ResNet-50）的参数通过在ImageNet数据集上预训练的对应模型进行初始化，剩余参数随机初始化。</p><p>  如果没有特别声明，本文中消融实验（ablation experiments）默认使用VGG-16作为backbone，并使用和<a href="http://arxiv.org/abs/1704.03604" target="_blank" rel="noopener">Instancelevel salient object segmentation</a>一文相同的联合数据集（MSRA-B和HKU-IS）。</p><p>  本文在数据增强方面只使用了水平翻转。</p><p>  在训练和测试中，和<a href="http://arxiv.org/pdf/1611.04849v4.pdf" target="_blank" rel="noopener">Deeply supervised salient object detection with short connections</a>一文中一样，输入图片的尺寸保持不变。</p></li><li><p>数据集和损失函数</p><p>  在6个常用数据集（ECSSD、PASCALS、DUT-OMRON、HKU-IS、SOD和DUTS）上开展实验以评估性能。</p><p>  显著性目标检测中使用使用standard binary cross entropy loss，边缘检测使用balanced binary cross entropy loss。</p></li><li><p>评估标准</p><p>  使用3个广泛应用的指标（PR曲线、F-measure score和MAE）评估本文提出的方法。</p></li></ul><h2 id="5-2-Ablation-Studies"><a href="#5-2-Ablation-Studies" class="headerlink" title="5.2. Ablation Studies"></a>5.2. Ablation Studies</h2><p>ablation的译文是消融。</p><p>该section首先研究GGM和FAM的有效性，然后开展实验研究如何配置GGM，最后研究协同训练对性能的影响。</p><ul><li><p>Effectiveness of GGM and FAMs</p><p>  基于FPN的baseline，以VGG-16为backbone，研究GGM和FAMs的有效性。除了GGM和FAMs的不同组合，其它所有配置都相同。原文表1展示了其在数据集DUT-O和SOD上的性能，对应的视觉比较可以在原文图2中看到。</p><ul><li><p>GGM Only</p><p>  原文表1第4行数据说明GGM提升了F-measure和MAE。<strong>GGM生成的全局指导信息使网络更多地关注显著目标的完整性，大幅提升了所得显著性图的质量。因此，显著目标的细节（这些细节容易被感受野有限的模型错误预测为背景，比如原文图2的最后1行）可以被增强</strong>。</p></li><li><p>FAMs Only</p><p>  原文表1第5行的数据说明简单地将FAMs嵌入到原文图1所示的FPN baseline中提升了F-measure和MAE。这可能是因为<strong>FAM中的池化操作扩大了整个网络的感受野</strong>，并且FPN baseline仍然需要融合不同尺度的特征图，这说明<strong>FAM了缓解上采样aliasing effect的有效性</strong>。</p></li><li><p>GGM &amp; FAMs</p><p>  原文表1最后1行的数据说明同时引入GGM和FAMs可以得到更优的F-measure和MAE，<strong>这说明GGM和FAM是互补的。通过它们可以精确地定位显著目标并改善其细节（如图2所示）。原文图6中包含更多的定性结果。</strong></p></li></ul></li><li><p>Configuration of GGM</p><p>  为更好地了解GGM，独立使用PPM和GGF开展实验，数据分别在原文表1的第2行和第3行。这2个实验的结果都比使用GGM时的结果（原文表1第4行的数据）。<strong>这说明PPM和GGF在GGM中都起着重要作用。</strong></p></li><li><p>The Impact of Joint Training</p><p>  如原文表2所示，在3个数据集上，将显著目标边界（SalEdge）作为GT进行训练并没有提升baseline的性能，而使用标准的边缘（StdEdge）作为GT可以大幅提升baseline的性能，特别是MAE。这说明<strong>引入详细的边缘信息有助于显著性目标检测</strong>。</p></li></ul><h2 id="5-3-Comparisons-to-the-State-of-the-Arts"><a href="#5-3-Comparisons-to-the-State-of-the-Arts" class="headerlink" title="5.3. Comparisons to the State-of-the-Arts"></a>5.3. Comparisons to the State-of-the-Arts</h2><p>该section比较了本文方法和13个SOTA方法（具体是哪13个方法见原文，此处省略）。为公平比较，这13个方法的结果是原结果或者使用初始公开代码得到的结果。所有结果都不经过任何后处理，所有预测得到的显著性图都使用同一份代码进行评估。</p><ul><li><p>Quantitative Comparisons</p><p>  如原文表3所示，分别使用VGG-16和ResNet-50作为backbone，并在多份训练集上开展实验以<strong>排除潜在的性能波动</strong>。可以看到，在相同的训练集上，使用相同的backbone，PoolNet超过了之前所有的SOTA方法。平均速度（FPS）对比如原文表4所示。</p></li><li><p>PR Curves</p><p>  原文图7为在3个数据集上的PR曲线，可以看到PoolNet的PR曲线优于其它算法。随着Recall值趋于1，PoolNet的Precision比其它算法高很多。这说明PooNet得到的显著性图的错误正样本（false positives）较少。</p></li><li><p>Visual Comparisons</p><p>  原文图6给出了PoolNet和其它算法的定性对比。从上到下，分别是<strong>透明目标、小目标、大目标、复杂形状和前背景低对比度</strong>。可以看出，在几乎所有环境下，PoolNet不仅可以正确找出显著目标，还可以增强它们的边缘。</p></li></ul><h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h1><p> 本文设计GGM和FAM两个模块，提出PoolNet，并和边缘检测任务实现协同训练，在6个常用数据集上的效果优于之前所有SOTA方法。</p><p>GGM和FAM是独立于网络结构的，<strong>可以灵活地迁移到任何基于金字塔的模型</strong>。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文名称&lt;/p&gt;
&lt;p&gt;  A Simple Pooling-Based Design for Real-
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="PoolNet" scheme="https://chouxianyu.github.io/tags/PoolNet/"/>
    
  </entry>
  
  <entry>
    <title>BASNet论文详解</title>
    <link href="https://chouxianyu.github.io/2021/03/08/BASNet%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/"/>
    <id>https://chouxianyu.github.io/2021/03/08/BASNet论文详解/</id>
    <published>2021-03-08T03:04:00.000Z</published>
    <updated>2021-03-21T12:34:49.946Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  BASNet: Boundary-Aware Salient Object Detection</p></li><li><p>作者</p><p>  Xuebin Qin等</p></li><li><p>发表时间</p><p>  2019年</p></li><li><p>来源</p><p>  CVPR2019</p></li></ul><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><ul><li><p>知识</p><ul><li><p>本文混合损失函数中的3种loss</p></li><li><p>本文核心是混合损失函数以及refine module，其中refine module借用了ResNet中的思路：$S_{refined}=S_{coarse}+S_{residual}$。</p></li><li><p>如图3所示，coarse saliency map存在2方面的问题：①the blurry and noisy boundaries(边界不准确、不清晰), ②the unevenly predicted regional probabilities(同类区域中像素概率不均匀)</p></li><li><p>RRM_LC和RRM_MS等模块比较shallow，所以很难捕捉到可用于refinement的high level information。</p></li><li><p>评估指标</p><ul><li><p>PR Curve</p><p>  PR Curve是1种评估预测所得显著性图的标准方式。1张显著性图的precision和recall通过比较二值化的显著性图及其ground truth计算。<strong>1个二值化threshold对应的1对precision和recall是数据集中所有显著性图的平均precision和recall</strong>。threshold从0到1变化，可以得到<strong>1个precision-recall pair序列</strong>，画出来就是PR Curve。</p></li><li><p>F-measure</p><p>  F-measure可以全面地衡量precision和recall，其<strong>基于1对precision和recall进行计算</strong>。在进行算法比较时，一般采用最大的F-measure进行比较。</p></li><li><p>MAE</p><p>  MAE指显著性图与其ground truth的average absolute per-pixel difference。<strong>模型对于1个数据集的MAE为所有显著性图的MAE的平均值</strong>。</p></li></ul></li></ul></li><li><p>一些未知的东西</p><ul><li>U-Net</li><li>SegNet</li><li>dilation convolution</li><li>感受野的计算方法</li></ul></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul><li>背景：多数前人工作只关注region accuracy而非boundary quality。</li><li>本文提出：a predict-refine architecture: BASNet and a new hybrid loss for Boundary-Aware Salient object detection<ul><li>BASNet<ul><li>a densely supervised Encoder-Decoder network, in charge of saliency prediction</li><li>a residual refinement module(RRM), in charge of saliency map refinement</li></ul></li><li>The hybrid loss<ul><li>3级层次结构：pixel-level, patch-level, map-level</li><li>方式：混合3种loss，Binary Cross Entropy (BCE), Structural SIMilarity (SSIM) and Intersection-over-Union (IoU) losses</li></ul></li></ul></li><li><strong>效果</strong>：effectively segment the salient object regions and accurately predict <strong>the fine structures with clear boundaries</strong><ul><li>精度：在6个数据集上，ofregional and boundary evaluation measures超过SOTA。</li><li>速度：over 25 fps on a single GPU</li></ul></li><li>Code：<a href="https://github.com/NathanUA/BASNet" target="_blank" rel="noopener">https://github.com/NathanUA/BASNet</a></li></ul><h1 id="1-Inroducion"><a href="#1-Inroducion" class="headerlink" title="1. Inroducion"></a>1. Inroducion</h1><ul><li><p>背景</p><p>  近期FCN被用于显著性目标检测，性能优于传统算法，但their predicted saliency maps are still <strong>defective in fine structures and/or boundaries</strong>，如图1(c)和(d)所示。</p></li><li><p><strong>two main challenges</strong> in accurate salient object detection</p><ol><li>network：networks that <strong>aggregate multi-level deep features</strong> are needed</li><li>loss：models trained with CE loss usually have <strong>low confidence</strong> in differentiating boundary pixels, leading to <strong>blurry</strong> boundaries. Other losses such as Intersection over Union (IoU) loss, F-measure loss and Dice-score loss are not specifically designed for capturing <strong>fine structures</strong>。</li></ol></li><li><p>BASNet: the prediction module and RRM</p><p>  To capture both <strong>global (coarse)</strong> and <strong>local (fine)</strong> contexts，a new <strong>predict-refine</strong> network is proposed。</p><ul><li><p>the prediction module</p><p>  a <strong>U-Net-like</strong> densely supervised Encoder-Decoder network，transfers the input image to a probability map</p></li><li><p>RRM</p><p>  a novel residual refinement module，refines the predicted map by learning the residuals between the coarse saliency map and ground truth</p></li></ul></li><li><p>the hybrid loss</p><p>  To obtain <strong>high confidence</strong> saliency map and <strong>clear</strong> boundary, we propose a hybrid loss that combines Binary Cross Entropy (<strong>BCE</strong>), Structural SIMilarity (<strong>SSIM</strong>) and <strong>IoU</strong> losses, which are expected to learn from ground truth information in <strong>pixel-, patch- and map- level</strong>, respectively.</p><p>  <strong>Rather than using explicit boundary losses</strong> (NLDF+ , C2S ), we implicitly inject the goal of accurate boundary prediction in the hybrid loss, contemplating that it may help reduce spurious error from cross propagating the information learned on the boundary and the other regions on the image。</p><p>  毕竟任务不是边缘检测，所以不能只检测边缘。</p></li></ul><h1 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2. Related Works"></a>2. Related Works</h1><ul><li><p>Traditional Methods</p><p>  早期方法根据1个<strong>预定义</strong>的基于<strong>手工特征</strong>计算的saliency measure<strong>搜索像素</strong>以检测显著目标。Borji等人提供了1个全面的综述。</p></li><li><p>Patch-wise Deep Methods</p><p>  受深度卷积神经网络推动图片分类发展的启发，早期的deep方法基于从单/多尺度提取的<strong>local image patches</strong>将image pixels和super pixels分类为显著/非显著。这些方法通常生成<strong>coarse outputs</strong>因为在全连接层中<strong>spatial information会丢失</strong>。</p></li><li><p>FCN-based Methods</p><p>  相比于Traditional Methods和Patch-wise Deep Methods，基于FCN的方法取得了重要进展，因为FCN is able to <strong>capture richer spatial and multiscale information</strong>。</p><p>  UCF、Amulet、NLDF+、DSS+、HED、RAS、LFR、BMPM。</p></li><li><p>Deep Recurrent and attention Methods</p><p>  PAGRN、RADF+、RFCNPiCANetR</p></li><li><p>Coarse to Fine Deep Methods</p><p>  为捕捉finer structures和more accurate boundaries，学者提出了大量refinement策略。</p><p>  SRM、R3Net+、DGRL</p></li></ul><h1 id="3-BASNet"><a href="#3-BASNet" class="headerlink" title="3. BASNet"></a>3. BASNet</h1><h2 id="3-1-Overview-of-Network-Architecture"><a href="#3-1-Overview-of-Network-Architecture" class="headerlink" title="3.1. Overview of Network Architecture"></a>3.1. Overview of Network Architecture</h2><p>BASNet包括2个Module：Predict Module和Refine Module，如图2所示。</p><ul><li><p>Predict Module</p><p>  a U-Net-like densely supervised Encoder-Decoder network，作用是predict saliency map from input images</p></li><li><p>Refine Module</p><p>  Residual Refinement Module，<strong>refines the resulting saliency map</strong> of the prediction module by learning the residuals between the saliency map and the ground truth。</p></li></ul><h2 id="3-2-Predict-Module"><a href="#3-2-Predict-Module" class="headerlink" title="3.2. Predict Module"></a>3.2. Predict Module</h2><ul><li><p>整体</p><ul><li>受U-Net和SegNet启发，predict module是1个<strong>Encoder-Decoder</strong>网络，这种结构可以同时捕捉<strong>high level global contexts and low level details</strong>。</li><li>受HED启发，为减少过拟合，通过ground truth对decoder每个stage的最后1层进行监督，如图2所示。</li></ul></li><li><p>encoder</p><p>  encoder部分包含<strong>1个输入卷积层</strong>和<strong>6个由basic res-blocks组成的stage</strong>，其中输入卷积层和前4个stage是修改过的<strong>ResNet34</strong>。</p><p>  改动为本文中的输入卷积层有64个步长为1的3×3卷积核而非步长为2的7×7卷积核，并且在输入卷积层之后没有pooling，这使得在前几层获得更大尺寸的特征图但也减小了整体的感受野。</p><p>  为获得和ResNet34相同的感受野，本文<strong>在ResNet34又加了2个stage</strong>，每个stage均由size为2的不重叠maxpooling层及3个512filter的basic res-block组成。</p></li><li><p>bridge</p><p>  为进一步捕捉global infomation，本文<strong>在encoder和decoder之间添加了1个bridge</strong>。</p><p>  该bridge包括3个512核的dilation为2的3×3卷积层，其中每个卷积层后都有1个BN层和ReLU。</p></li><li><p>decoder</p><p>  decoder几乎和encoder完全对称。decoder的每个stage包含3个卷积层，每个卷积层后有1个BN层和ReLU。</p><p>  <strong>每个stage的输入是</strong>其前1个stage的输出和其对应的encoder中的stage的输出的<strong>concatenate</strong>结果。</p><p>  为得到side-output saliency maps，bridge和decoder中每个stage的输出被进行处理，处理过程为：<strong>1个3×3卷积、上采样、sigmoid</strong>。因此输入1张图片，本文的predict module在训练过程中输出<strong>7个saliency maps</strong>，其中<strong>最后1个saliency map</strong>的accuracy最高，所以其作为predict module的最终输出传入refine module。</p></li></ul><h2 id="3-3-Refine-Module"><a href="#3-3-Refine-Module" class="headerlink" title="3.3. Refine Module"></a>3.3. Refine Module</h2><ul><li>Refinement Module通常被定义为1个<strong>residual block</strong>，其通过学习saliency map和ground truth之间的residual$S_{residual}$来refine预测得到的coarse saliency map$S_{coarse}$，公式为$S_{refined}=S_{coarse}+S_{residual}$。</li><li>如图3所示，<strong>coarse包含2方面的含义</strong>：①the blurry and noisy boundaries, ②the unevenly predicted regional probabilities，模型预测所得的coarse saliency map中这2方面的问题都会有。</li><li><strong>RRM_LC</strong>(residual refinement module based on local context)起初被提出是用于boundary refinement，因为其感受野较小，Islam和Deng等人iteratively或recurrently在不同尺度上使用它refine saliency maps。Wang等人使用了PPM(pyramid pooling module)，其中3个尺度的pyramid pooling features被concatenate。为避免池化操作导致细节损失，<strong>RRM_MS</strong>使用kernel size和dilation不同的卷积层捕捉multi-scale contexts。然而这些模块是shallow的，所以<strong>很难捕捉到可用于refinement的high level information</strong>。</li><li>本文的<strong>RRM和predict module结构相似</strong>但简单很多，其包含1个输入层、1个encoder、1个bridge、1个decoder和1个输出层。encoder和decoder都包含<strong>4个stage</strong>，<strong>每个stage</strong>只包含1个64核的3×3卷积层，<strong>每个卷积层</strong>后面都有1个BN层和1个ReLU。bridge和1个stage结构相同，也包含1个64核的3×3卷积层(后面跟着1个BN层和1个ReLU)。encoder中下采样时使用maxpooling，decoder中上采样时使用bilinear interpolation。RRM的输出即本文整个模型最终的输出。</li></ul><h2 id="3-4-Hybrid-Loss"><a href="#3-4-Hybrid-Loss" class="headerlink" title="3.4. Hybrid Loss"></a>3.4. Hybrid Loss</h2><ul><li><p>训练中的Loss是各个side output的loss的加权和，每个side output的loss公式都是1个hybrid loss(3种loss之和)。本文模型对8个side output进行深度监督，其中7个side output来自Predict Module、1个side output来自Refine Module。</p><ul><li>BCE Loss：在二分类和分割任务中，二值交叉熵损失函数（Binary Cross Entropy Loss，BCE Loss）是最常用的损失函数。公式略</li><li>SSIM Loss：结构相似损失（Structural Similarity Loss，SSIM Loss）在提出时被用于图像质量评价。它可以捕捉一张图片中的结构信息，因此本文将其集成于混合损失函数中，以获取显著目标标注中的结构信息。公式略</li><li>IoU Loss：交并比损失（Intersection over Union Loss，IoU Loss）在提出时被用来衡量2个集合的相似性，后来被作为目标检测和分割的标准评估指标。最近，它也被用在了显著性目标检测的训练中。公式略</li></ul></li><li><p>本文阐述了3种loss的作用，图5中的热力图展示了每个像素的loss随训练过程的变化，3列分别代表训练过程中的不同阶段，3行分别是不同的loss。</p><ul><li><p>BCE Loss是<strong>pixel-level</strong>的measure，它并不考虑周围像素的label并且foreground像素和background像素的权重相同，<strong>有助于所有像素的收敛</strong>。</p></li><li><p>SSIM Loss是<strong>patch-level</strong>的measure，它考虑每个像素的local neighborhood，<strong>对边界具有更高的权重</strong>(即使预测前景的概率相同，但边界附近的loss比前景中心的loss更高)。</p><p>  在训练过程的初始阶段，边界周围像素的loss是最大的(见图5第2行)，<strong>这帮助集中于边界附近像素的收敛</strong>。随着训练过程，foreground的SSIM Loss减小而<strong>background的loss成为主导项</strong>。但只有当background的预测非常接近ground truth(0)时 background的loss才会起作用，<strong>这时loss会从1急速下降到0</strong>，因为通常只有在训练晚期BCE loss平滑(flat)时background的预测才会到0。SSIM Loss保证有足够的梯度使得网络继续学习。因为预测被push到0，background的预测看起来会更clean。</p></li><li><p>IoU Loss是<strong>map-level</strong>的measure，但是本文为了阐述所以根据式6画出了每个像素的IoU Loss。</p><p>  随着foreground的预测越来越接近1，foreground的loss最终变成0。</p><p>把3个loss混合起来，<strong>利用BCE使每个像素都有smooth gradient，利用IoU给予foreground更多注意力，通过SSIM基于图像结构使得边界的loss更大</strong>。</p></li></ul></li></ul><h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4. Experimental Results"></a>4. Experimental Results</h1><h2 id="4-1-Datasets"><a href="#4-1-Datasets" class="headerlink" title="4.1. Datasets"></a>4.1. Datasets</h2><p>在6个常用数据集上对模型进行了评估。具体是哪些数据集、这些数据集有哪些特点请见原文。</p><h2 id="4-2-Implementation-and-Experimental-Setup"><a href="#4-2-Implementation-and-Experimental-Setup" class="headerlink" title="4.2. Implementation and Experimental Setup"></a>4.2. Implementation and Experimental Setup</h2><ul><li><p>训练</p><ul><li>使用DUTS-TR训练模型，训练前进行离线数据增强(将图片水平翻转)。</li><li>将图片resize到256×256并随机裁剪成224×224。</li><li>encoder的部分参数使用ResNet34的预训练模型进行初始化，其它卷积层通过Xavier初始化。</li><li>使用Adam进行训练，超参数为默认值（初始学习率1e-3，betas=(0.9, 0.999)，eps=1e-8，weight decay=0）</li><li>一直训练到loss收敛，不使用validation set。最终经历了400k iterations，batch size为8，耗时125小时c</li></ul></li><li><p>测试/推理</p><p>  将原图resize到256×256，再将最终得到的显著图resize back到原图大小</p></li><li><p>软/硬件环境</p><p>  训练和测试的软硬件环境一致。</p><p>  PyTorch0.4.0，An eight-core PC with an AMD Ryzen 1800x 3.5 GHz CPU (with 32GB RAM) and a GTX 1080ti GPU (with 11GB memory)</p><p>  256×256图片的推理耗时为0.04秒。</p></li></ul><h2 id="4-3-Evaluation-Metrics"><a href="#4-3-Evaluation-Metrics" class="headerlink" title="4.3. Evaluation Metrics"></a>4.3. Evaluation Metrics</h2><p>PR Curve、F-measure、MAE、relaxed F-measure of boundary ($relaxF^b_{\beta}$)。对这几个评估指标的具体介绍请见原文。</p><ul><li><p>PR Curve</p><p>  PR Curve是1种评估预测所得显著性图的标准方式。1张显著性图的precision和recall通过比较二值化的显著性图及其ground truth计算。<strong>1个二值化threshold对应的1对precision和recall是数据集中所有显著性图的平均precision和recall</strong>。threshold从0到1变化，可以得到<strong>1个precision-recall pair序列</strong>，画出来就是PR Curve。</p></li><li><p>F-measure</p><p>  F-measure可以全面地衡量precision和recall，其<strong>基于1对precision和recall进行计算</strong>。在进行算法比较时，一般采用最大的F-measure进行比较。</p></li><li><p>MAE</p><p>  MAE指显著性图与其ground truth的average absolute per-pixel difference。<strong>模型对于1个数据集的MAE为所有显著性图的MAE的平均值</strong>。</p></li><li><p>relaxed F-measure of boundary</p><p>  此处省略，请见原文。</p></li></ul><h2 id="4-4-Ablation-Study"><a href="#4-4-Ablation-Study" class="headerlink" title="4.4. Ablation Study"></a>4.4. Ablation Study</h2><p>这个section验证了本文模型中的每个关键component。消融实验包括architecture ablation和loss ablation2个部分。消融实验是在ECSSD数据集上进行的。</p><ul><li><p>architecture ablation</p><p>  为证明BASNet的有效性，本文提供了BASNet与其它相关结构的量化对比结果。</p><p>  Loss都使用BCE，首先以U-Net作为baseline，然后是Encoder-Decoder Network、Deep Supervision、RRM_LC、RRM_MS、RRM_Ours，结果如表1所示，可见本文提出的<strong>BASNet在这5个实验中性能最优</strong>。</p></li><li><p>loss ablation</p><p>  为阐述本文提出的fusion loss的有效性，本文基于BASNet使用不同loss进行了1系列实验。表1中的结果证明<strong>本文提出的hybrid loss极大地提升了性能</strong>，特别是边界的质量。</p><p>  为进一步阐述损失函数对于BASNet预测质量的影响，使用不同Loss对BASNet进行训练的结果如图7所示，很明显可以看出<strong>本文提出的混合损失函数达到了最优的质量</strong>。</p></li></ul><h2 id="4-5-Comparison-with-State-of-the-arts"><a href="#4-5-Comparison-with-State-of-the-arts" class="headerlink" title="4.5. Comparison with State-of-the-arts"></a>4.5. Comparison with State-of-the-arts</h2><p>和15个SOTA算法进行比较。公平起见，使用原文作者提供的显著性图或者使用原文作者公开的模型。</p><ul><li><p>Quantitative evaluation</p><p>  <strong>图6</strong>展示了在5个数据集上的PR曲线和F-measure曲线，表2展示了在6个数据集上的maximum region-based F-measure、MAE、the relaxed boundary Fmeasure。数据提了很多个percent（略）</p></li><li><p>Qualitative evaluation</p><p>  <strong>图8</strong>展示对比了8种算法对<strong>不同类型图片</strong>的识别效果图，图片类型有images with low contrast、fine structures、large object touching image boundaries、complex object boundaries。</p></li></ul><h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h1><p>略</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文名称&lt;/p&gt;
&lt;p&gt;  BASNet: Boundary-Aware Sal
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="显著性目标检测" scheme="https://chouxianyu.github.io/tags/%E6%98%BE%E8%91%97%E6%80%A7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="BASNet" scheme="https://chouxianyu.github.io/tags/BASNet/"/>
    
  </entry>
  
  <entry>
    <title>PSPNet论文阅读笔记</title>
    <link href="https://chouxianyu.github.io/2021/03/05/PSPNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://chouxianyu.github.io/2021/03/05/PSPNet论文阅读笔记/</id>
    <published>2021-03-05T07:53:52.000Z</published>
    <updated>2021-03-05T08:14:43.765Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  Pyramid Scene Parsing Network</p></li><li><p>作者</p><p>  Hengshuang Zhao等</p></li><li><p>发表时间</p><p>  2016年</p></li><li><p>来源</p><p>  CVPR2017</p></li></ul><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><h2 id="知识"><a href="#知识" class="headerlink" title="知识"></a>知识</h2><ul><li><p>scene parsing的目标是给图片中每个像素赋予1个类别标签，其可以提供对场景的完整理解，预测每个element的标签、位置和形状。</p></li><li><p>本文主要贡献</p><ul><li>提出PSPNet，将difficult scenery context features嵌入1个FCN based pixel prediction framework。</li><li>基于deeply supervised loss，为ResNet提供1个高效的优化策略。</li><li>为SOTA的scene parsing and semantic segmentation构建了1个practical system，并公开了所有关键细节。</li></ul></li><li><p>本文方法原理</p><p>  为获得合适的global features，本文提出PSPNet。除了<strong>使用传统的dilated FCN实现pixel prediction</strong>，我们<strong>将pixel-level feature扩展到了专门设计的global pyramid pooling的特征中</strong>，局部和全局的clues共同使得最终的预测更可靠。我们还提出了1种使用deeply supervised loss的优化策略。我们给出了所有实验细节（对本文模型的性能很重要），并公开了代码和训练好的模型。</p></li><li><p><strong>本文核心思路</strong></p><p>  scene parsing还面临着diverse scenes和unrestricted vocabulary的困难，其导致一些外表相似的物体被错误预测，但当有了context prior之后，理应可以得到正确的预测。目前基于FCN算法的主要问题也是缺乏利用global scene category clue的合适策略。所以PSPNet基于FCN将<strong>PPM（pyramid pooling module）</strong>作为高效的global context prior。</p></li><li><p><strong>PPM核心思路</strong></p><p>  感受野的大小可以大概表示我们能用多少context information。而CNN的实验感受野比其理论感受野小得多，特别是网络深层。这使得许多网络没有充分融合重要的global scenery prior。</p><p>  Global average pooling直接将所有像素混合得到1个vector可能会失去spatial relation并导致ambiguity。考虑到这一点，Global context information和sub-region context有助于区分各种category，1个更有力的representation可以是来自不同sub-regions的具有这些感受野的信息的融合。</p></li><li><p>其它</p><ul><li>FCN用卷积层代替全连接层进行分类</li><li>dilated convolution可以增大神经网络的感受野。</li><li>本文baseline network是FCN和dilated network(《Semantic image segmentation with deep convolutional nets and fully connected crfs》)。</li><li>一些工作主要探索了2个方向：multi-scale feature ensembling和structure prediction，这2个方向都都改善了scene parsing的localization ability。</li><li>开创性的《Semantic image segmentation with deep convolutional nets and fully connected crfs》使用条件随机场(CRF)作为post processing以精化分割结果。</li><li>《Parsenet: Looking wider to see better》证明了global average pooling with FCN可以提升语义分割结果。</li></ul></li></ul><h2 id="一些未知的东西"><a href="#一些未知的东西" class="headerlink" title="一些未知的东西"></a>一些未知的东西</h2><ul><li>scene和vocabulary具体指什么？</li><li>context prior具体指什么？</li><li>knowledge graph（知识图谱）是什么？</li><li>dilated convolution是什么？</li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul><li><p>背景</p><p>  对于unrestricted open vocabulary和diverse scenes，scene parsing(场景解析)具有挑战性。</p></li><li><p>方法</p><p>  本文使用PPM(pyramid pooling module)和提出的PSPNet(pyramid scene parsing network)，<strong>实现了通过融合different-region-based context获取全局context信息的能力</strong>。本文中的global prior presentation(全局先验表征)可在scene pareing任务中有效产生高质量结果，而PSPNet提供了1个出色的pixel-level prediction framework。</p></li><li><p>结果</p><p>  本文提出的方法在多个数据集上实现了SOTA，取得ImageNet scene parsing challenge 2016、PASCAL VOC 2012 benchmark和Cityscapes benchmark的第1名。单独1个PSPNet在PASCAL VOC 2012取得mIoU accuracy 85.4%的新纪录，在Cityscapes取得mIoU accuracy 80.2%的新纪录。</p></li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li><p>背景和意义</p><p>  基于semantic segmentation的scene parsing是计算机视觉中的基本主题，其目标是<strong>给图片中每个像素赋予1个类别标签</strong>。<strong>scene parsing可提供对场景的完整理解，它可以预测每个element的标签、位置和形状</strong>。对于自动驾驶、机器人感应等潜在应用，该主题引起了广泛兴趣。</p></li><li><p>困难</p><p>  <strong>scene parsing的困难与scene和label的多样性紧密相关</strong>。首创的scene parsing任务是对LMO数据集中2688张图片的33个场景进行分类。更多最近的PASCAL VOC语义分割数据集和PASCAL context数据集中包括更多context相似的标签，比如chair和sofa、horse和cow等等。新的ADE20K是最具挑战性的1个数据集数据集，其具有大量无限制开放的vocabulary和更多的scene类别，图1展示了其中一些具有代表性的图片。为这些数据集找到1个有效的算法需要克服许多困难。</p></li><li><p>目前进展</p><p>  scene parsing的SOTA算法多数是基于全卷积神经网络（FCN）。基于CNN的算法推动了dynamic object understanding，但考虑到<strong>diverse scenes和unrestricted vocabulary</strong>则还面临着挑战。图2第1行中1个boat被错误识别为car，出现这些错误是由于<strong>similar appearance of objects</strong>。但<strong>当图片的context prior为1条河附近的boathouse时，理应得到正确的预测</strong>。</p><p>  为了精准的场景感知，知识图谱依赖于scene context的先验信息(prior information)。我们发现<strong>目前基于FCN算法的主要问题是缺乏利用global scene category clue的合适策略</strong>。对于complex scene understanding，为了获取1个global image-level feature，<strong>spatial pyramid poolin</strong>g被广泛应用，因为spatial statistics为overall scene interpretation提供了较好的descriptor。<strong>Spatial pyramid pooling network</strong>进一步增强了这个能力。</p></li><li><p>本文方法原理</p><p>  与上述方法不同，为获得合适的global features，本文提出PSPNet。除了<strong>使用传统的dilated FCN实现pixel prediction</strong>，我们<strong>将pixel-level feature扩展到了专门设计的global pyramid pooling的特征中</strong>，局部和全局的clues共同使得最终的预测更可靠。我们还提出了1种使用deeply supervised loss的优化策略。我们给出了所有实验细节（对本文模型的性能很重要），并公开了代码和训练好的模型。</p></li><li><p>本文方法性能</p><p>  在所有公开数据集上达到SOTA，是ImageNet scene parsing challenge 2016的冠军、PASCALVOC 2012 semantic segmentation benchmark的第1名、urban scene Cityscapes data的第1名。这证明PSPNet为pixellevel prediction tasks指明了1个方向，它甚至可以帮助基于CNN的stereo matching、optical flow、depth estimation等。</p></li><li><p>本文主要贡献</p><ul><li>提出PSPNet，将difficult scenery context features嵌入1个FCN based pixel prediction framework。</li><li>基于deeply supervised loss，为ResNet提供1个高效的优化策略。</li><li>为SOTA的scene parsing and semantic segmentation构建了1个practical system，并公开了所有关键细节。</li></ul></li></ul><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><ul><li><p>受<strong>FCN用卷积层代替全连接层进行分类</strong>的启发，scene parsing和semantic segmentation等pixel-level prediction任务取得了巨大进步。为增大神经网络的感受野，《Semantic image segmentation with deep convolutional nets and fully connected crfs》和《Multi-scale context aggregation by dilated convolutions》使用了<strong>dilated convolution</strong>。Noh等人提出了1个coarse-to-fine structure with deconvolution network以学习segmentation mask。<strong>本文baseline network是FCN和dilated network(《Semantic image segmentation with deep convolutional nets and fully connected crfs》)</strong>。</p></li><li><p>其它工作主要探索了2个方向。</p><ul><li>第1个方向是multi-scale feature ensembling。因为在深度网络中，higher-layer feature包含更多semantic meaning，很少location information。<strong>融合多尺度的特征</strong>可以提高性能。</li><li><p>第2个方向是基于structure prediction。开创性的《Semantic image segmentation with deep convolutional nets and fully connected crfs》<strong>使用条件随机场(CRF)作为post processing以精化分割结果</strong>。还有一些方法通过end-to-end modeling精化了网络。</p><p>这2个方向都都改善了scene parsing的<strong>localization ability</strong>（predicted semantic boundary fits objects），然而在复杂场景中还有很大空间可以更加有效地利用必要信息。</p></li></ul></li><li><p>为充分利用global image-level priors(for diverse scene understanding)，一些方法使用传统方法而非神经网络提取了global context information。在object detection frameworks下也有了相似的提升。Liu等人证明了<strong>global average pooling with FCN可以提升语义分割结果</strong>。然而本文实验证明，对于ADE20K数据集，<strong>这些global descriptors还不足够representative</strong>。因此本文使用PSPNet实现了通过融合<strong>different-region-based context</strong>获取全局context信息的能力。</p></li></ul><h1 id="3-Pyramid-Scene-Parsing-Network"><a href="#3-Pyramid-Scene-Parsing-Network" class="headerlink" title="3. Pyramid Scene Parsing Network"></a>3. Pyramid Scene Parsing Network</h1><p>在把应用FCN到scene parsing时，我们观察到一些具有代表性的失败案例并对其进行了分析。这激发了我们<strong>将pyramid pooling module 作为高效global context prior</strong>的思路。图3展示了PSPNet的结构。</p><h2 id="3-1-Important-Observations"><a href="#3-1-Important-Observations" class="headerlink" title="3.1. Important Observations"></a>3.1. Important Observations</h2><p>ADE20K数据集包含150个stuff/object category labels（比如wall、sky、tree）和1038张imagelevel scene descriptors（比如airport terminal、bedroom、street），所以形成了大量的label和分布广阔的scene。检查了《Semantic understanding of scenes through the ADE20K dataset》提供的FCN baseline的预测结果，我们总结出了complex-scene parsing的几个普遍问题：Mismatched Relationship、Confusion Categories和Inconspicuous Classes（具体内容见原文）。总结这些问题，很多错误与<strong>不同感受野的contextual relationship和global information</strong>部分或完全相关。因此1个具有合适<strong>global-scene-level prior</strong>的神经网络可以大量提高scene parsing的性能。</p><h2 id="3-2-Pyramid-Pooling-Module"><a href="#3-2-Pyramid-Pooling-Module" class="headerlink" title="3.2. Pyramid Pooling Module"></a>3.2. Pyramid Pooling Module</h2><p>实验证明，pyramid pooling module是高效的global contextual prior。</p><p>在1个神经网络中，<strong>感受野的大小可以大概表示我们能用多少context information</strong>。虽然理论上ResNet的感受野已经超过输入图片的大小了，但Zhou等人发现<strong>CNN的实验感受野比其理论感受野小得多，特别是网络深层</strong>。这使得<strong>许多网络没有充分融合重要的global scenery prior</strong>。本文提出1个高效的global prior representation来解决这个问题。</p><p>Global average pooling可以很好地作为global contextual prior的baseline model，常常被用在图片分类任务中，也被用于semantic segmentation。考虑到ADE20K数据集中图片scene的复杂性，该策略并不足以涵盖必要信息：<strong>图片的每个像素被标注为许多stuff/objects，直接将它们混合得到1个vector可能会失去spatial relation并导致ambiguity</strong>。考虑到这一点，<strong>Global context information和sub-region context有助于区分各种category</strong>，1个更有力的representation可以是来自不同sub-regions的具有这些感受野的信息的融合。一些场景/图片分类方面的工作[引]也得到了相似结论。</p><p>在《Hypercolumns for object segmentation and fine-grained localization》中，pyramid pooling生成的不同层次的feature maps最终被flatten和concatenate，然后送入1个全连接层进行分类。该global prior的目标是移除CNN用于图片分类时fixed-size的constraint。为进一步减少不同sub-region之间的context information loss，本文提出1个hierarchical global prior，<strong>包含不同尺度和不同sub-region的信息</strong>，称其为pyramid pooling module，用来在神经网络最后1个特征图上构建global scene prior，如图3(c)所示。</p><p><strong>PPM的具体结构：</strong></p><p>PPM融合4个不同金字塔尺度的特征。最coarse的level是<strong>global pooling</strong>，生成1个single bin output。接下来的pyramid level将特征图<strong>划分成不同的sub-region并为不同location形成pooled representation</strong>。PPM中不同level的输出包含不同尺寸的特征图。为了保持global feature的占比，我们在每1个pyramid level后使用<strong>1×1卷积</strong>以将context representation的dimension减少到1。然后将这些1维特征图<strong>上采样</strong>以使其与初始特征图大小相同。最终将不同level的特征<strong>concatenate</strong>后作为最终的pyramid pooling global feature。</p><p>注意pyramid level的数量和每个level的size是<strong>可以修改</strong>的，它们与输入至pyramid pooling layer的特征图的size相关。该结构采用size不同、stride不同的pooling kernel以提取不同sub-region的特征。因此这些kernel在representation上应该保持合理的gap。本文中的PPM包含<strong>4个level</strong>，bin的size分别是<strong>1×1、2×2、3×3和6×6</strong>。关于选择最大池化还是平均池化，本文5.2节中做了大量实验。</p><h2 id="3-3-Network-Architecture"><a href="#3-3-Network-Architecture" class="headerlink" title="3.3. Network Architecture"></a>3.3. Network Architecture</h2><p>如图3所示，输入1张图片，使用预训练的<strong>ResNet</strong>模型并使用<strong>dilated network的策略</strong>提取得到feature map。最后feature map的size是输入图片size的1/8。然后使用PPM获取context information。通过本文4个level的PPM，可以涵盖图片的whole、half和small portions。然后将PPM4个分支的输出和PPM的输入concatenate。最后再用1个卷积层获得最后的prediction。</p><p>PSPNet为pixel-level scene parsing提供了高效的global contextual prior。PPM提取到的特征比global pooling更具代表性。考虑到计算成本，PSPNet相比于其 baseline(the original dilated FCN network)并没有增加多少计算成本。通过end-to-end learning，全局的PPM特征和局部的FCN特征可以最优化。</p><h1 id="4-Deep-Supervision-for-ResNet-Based-FCN"><a href="#4-Deep-Supervision-for-ResNet-Based-FCN" class="headerlink" title="4. Deep Supervision for ResNet-Based FCN"></a>4. Deep Supervision for ResNet-Based FCN</h1><p>深度网络可以得到好的性能，但增加网络深度可能导致optimization difficulty，ResNet通过在每个block中使用skip connection解决了这个问题。ResNet的Latter layers主要基于previous ones学习residues。</p><p>相反地，本文通过1个additional loss生成initial results，然后通过the final loss学习residue。因此，深度网络的优化被分解成易解的2个。</p><p>略……</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文名称&lt;/p&gt;
&lt;p&gt;  Pyramid Scene Parsing Network&lt;/p&gt;
&lt;/li&gt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="scene parsing" scheme="https://chouxianyu.github.io/tags/scene-parsing/"/>
    
      <category term="PSPNet" scheme="https://chouxianyu.github.io/tags/PSPNet/"/>
    
  </entry>
  
  <entry>
    <title>FPN网络图解及论文笔记</title>
    <link href="https://chouxianyu.github.io/2021/03/02/FPN%E7%BD%91%E7%BB%9C%E5%9B%BE%E8%A7%A3%E5%8F%8A%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>https://chouxianyu.github.io/2021/03/02/FPN网络图解及论文笔记/</id>
    <published>2021-03-02T14:38:21.000Z</published>
    <updated>2021-03-02T15:00:11.761Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FPN网络图解"><a href="#FPN网络图解" class="headerlink" title="FPN网络图解"></a>FPN网络图解</h1><p><img src="https://pic4.zhimg.com/v2-4bda90f7c75b31ff8d669a5f71860943_b.jpg" alt="FPN"></p><p><strong>原图片以及PPT源文件下载链接（欢迎关注我的知乎！）：</strong></p><p>链接：<a href="https://pan.baidu.com/s/10y78HagInyCuCA-aMeNJpg" target="_blank" rel="noopener">https://pan.baidu.com/s/10y78HagInyCuCA-aMeNJpg</a></p><p>提取码：iccm </p><p>复制这段内容后打开百度网盘手机App，操作更方便哦</p><h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  Feature Pyramid Networks for Object Detection</p></li><li><p>作者</p><p>  Tsung-Yi Lin等</p><p>  Facebook AI Research</p></li><li><p>发表时间</p><p>  2017年</p></li><li><p>来源</p><p>  CVPR2017</p></li></ul><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><ul><li><p>知识</p><ul><li><p>高级语义信息是有助于识别目标但有害于定位目标，低级空间信息有害于识别目标但有助于定位目标（由于其下采样次数较少，所以其可以更准确地定位）。</p></li><li><p>SSD没有利用higher-resolution maps，而本文证明了higher-resolution maps对于检测小物体很重要。</p></li><li><p>Introduction整体思路</p><p>  Feature Pyramids可以用于检测不同尺度的目标。</p><p>  Featurized Image Pyramids将图片缩放到不同大小并分别提取其特征并进行检测，其结构如图1(a)所示，其被大量应用于手工特征，其优点是每个层次都具有很强的语义信息（即使是high-resolution levels），其缺点是分别提取每个层次的特征并进行检测导致推理时间相应加倍。</p><p>  在recognition任务中，卷积网络逐渐取代了手工特征。卷积网络（Single feature map）逐渐不仅可以表示高级语义，在尺度变化方面也更具稳健性，因此可以只在一个尺度的特征图上进行检测，其结构如图1(b)所示，但其缺点是未利用到卷积网络固有的Pyramidal feature hierarchy。</p><p>  卷积网络（Pyramidal feature hierarchy）天然具有多尺度的、金字塔型的特征层次，其结构如图1(c)所示，其缺点是造成不同层次特征图之间的语义差异，high-resolution maps中包含的低级特征降低了该特征图在目标检测任务中的表征能力。SSD并没有利用higher-resolution maps，但本文证明了higher-resolution maps对于检测小物体很重要。</p><p>  如图2(top)所示，一些采用自上到下路径和skip connections的方法仅在自上到下路径顶部的单个特征图上进行预测，实际上这些方法还需要image pyramids以识别多尺度的目标。</p><p>  如图2(bottom)和图1(d)所示，FPN自然地利用卷积网络固有的金字塔层次同时创建每层都有较强语义信息的特征金字塔，通过1个从上到下的路径和侧边连接将low-resolution、semantically strong的特征和high-resolution、semantically weak的特征结合起来，在自上到下路径所有特征图上都进行预测，最终仅从单尺度的输入图片得到各层都有较强语义信息的特征金字塔，且不需额外计算成本。</p></li><li><p>FPN以<strong>任意大小</strong>的单张图片为输入。</p></li><li><p>对于ResNet，FPN并不将其第1个stage的输出包含到FPN中因为其内存占用量比较大。</p></li><li><p>FPN中bottom-up路径中相邻层间下采样比例为2</p></li><li><p>FPN的building block</p><p>  <strong>图3</strong>展示了创建top-down路径中特征图的building block。将top-down路径中coarser-resolution的特征图<strong>上采样（比例为2）</strong>，将bottom-up路径中的特征图<strong>通过1×1卷积减少通道数</strong>，然后将两者<strong>相加（element-wise）</strong>。这个过程一直迭代到生成最大的特征图。在开始top-down路径之前会在bottom-up路径顶层<strong>使用1×1卷积生成尺度最小的特征图</strong>。在每个相加操作之后<strong>使用3×3卷积减少上采样带来的混淆效应</strong>（aliasing effect）。</p></li><li><p>FPN和传统的featurized image pyramid一样，各个金字塔层都使用<strong>共享的classifier/regressor</strong>。</p></li></ul></li><li><p>一些未知的东西</p><ul><li><p>特征金字塔</p><p>  原文：E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image processing. RCA engineer, 1984.</p></li><li><p>尺度不变性（scale-invariant）</p></li></ul></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul><li>Feature Pyramids（特征金字塔）是识别系统中用来检测不同尺度的目标的一个基本组件</li><li><p>近期深度学习目标检测方法却避免了pyramid representations，如原文图1(b)，部分原因是它们是计算和内存密集型的</p></li><li><p>FPN(Feature Pyramid Network)</p><ul><li>使用少量成本，利用卷积网络固有的多尺度金字塔层次结构来构建特征金字塔</li><li>是一种带有侧向连接的自顶向下的结构，可以<strong>在所有尺度上构建高级语义特征</strong></li></ul></li><li>性能<ul><li>将FPN应用于基本的Faster-RCNN，在COCO detection benchmark上超过SOTA的single model</li><li>在GPU上达6FPS</li></ul></li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li><p>识别不同尺寸的目标是计算机视觉领域的一个基础挑战</p></li><li><p><strong>Featurized Image Pyramids</strong>（特征化的图像金字塔）</p><ul><li><p>定义</p><p>  Featurized Image Pyramids即Feature pyramids built upon image pyramids（建立在图像金字塔上的特征金字塔），见原文<strong>图1(a)</strong>。也就是<strong>将图片缩放到不同大小并分别提取其特征并进行检测</strong>。</p><p>  其构成了标准解决方案的基础，其具有<strong>尺度不变性（scale-invariant）</strong>，因为可以通过金字塔中的不同层来处理不同尺寸的目标，可以基于层维度和位置维度对目标进行扫描检测。</p></li><li><p>应用</p><p>  Featurized Image Pyramids在<strong>手工设计的特征</strong>中被大量使用，如DPM算法。</p></li><li><p>优点</p><p>  所有最近的ImageNet和COCO比赛的前几名都在Featurized Image Pyramids上进行多尺度测试。</p><p>  Featurized Image Pyramids的主要优点是<strong>每个层次都具有很强的语义信息，即使是high-resolution levels</strong>。</p></li><li><p>缺点</p><ul><li>分别提取每个层次的特征并进行检测，推理时间相应地加倍。</li><li><p>考虑到内存等因素，不可能端到端地在图像金字塔上训练卷积网络，最多只能在测试的时候使用图像金字塔（同时这也造成训练和测试的不一致性）</p><p>因为这些原因，Fast和Faster R-CNN默认也就没有使用FIP。</p></li></ul></li></ul></li><li><p>卷积网络（Single feature map）</p><p>  在Recognition任务中，手工特征逐渐被卷积网络取代。</p><p>  卷积网络不仅可以表示高级语义，在尺度变化方面也更具稳健性，因此可以<strong>只在一个尺度的特征图上进行检测</strong>，如原文<strong>图1(b)</strong>。</p><p>  但即使具备这样的稳健性，卷积网络中仍然需要金字塔（可以通过Pyramidal feature hierarchy改进）。</p></li><li><p>卷积网络（Pyramidal feature hierarchy）</p><ul><li><p>固有金字塔特征层次</p><p>  然而image pyramids并不是实现多尺度特征表示的唯一方法，<strong>卷积神经网络天然具有多尺度的、金字塔型的特征层次</strong>（feature hierarchy），如原文<strong>图1(c)</strong>。</p></li><li><p>缺点</p><p>  卷积网络的特征层次产生了不同尺寸的特征图，但是<strong>造成不同层次特征图之间的语义差异</strong>，尺寸大的特征图（high-resolution maps）包含着低级特征，这些<strong>低级特征降低了该特征图在目标检测任务中的表征能力</strong>。</p><p>  SSD是第一批尝试利用卷积网络中Pyramidal feature hierarchy的方法之一，但其也存在不足。理想情况中，SSD系列会利用forward pass中生成的多尺度特征，因此没有额外计算成本。但是为了避免使用低级特征，SSD没有使用已有的层，而是从网络高层开始构建金字塔并添加几个新层。因此<strong>SSD没有利用higher-resolution maps，而本文证明了higher-resolution maps对于检测小物体很重要</strong>。</p></li></ul></li><li><p>Feature Pyramid Network</p><ul><li><p>定义</p><p>  自然地利用卷积网络固有的金字塔层次同时创建每层都有较强语义信息的特征金字塔。如<strong>图1(d)和图2(bottom)</strong>所示，通过1个从上到下的路径和侧边连接<strong>将low-resolution、semantically strong的特征和high-resolution、semantically weak的特征结合</strong>起来，<strong>仅从单尺度的输入图片得到各层都有较强语义信息的特征金字塔</strong>，且不需额外计算成本。</p></li><li><p>相关工作</p><p>  如<strong>图2(top)</strong>所示，其它采用自上到下路径和skip connections的方法<strong>仅在自上到下路径顶部的单个特征图上进行预测</strong>，实际上<strong>这些方法还需要image pyramids以识别多尺度的目标</strong>，而FPN是在自上到下路径所有特征图上都进行预测。</p></li><li><p>效果</p><p>  在detection和segmentation系统上进行评估，仅将FPN应用于基础的Faster R-CNN在COCO detection benchmark上实现SOTA。</p><p>  在ablation experiments中，对于bounding box proposals，FPN将AR(Average Recall)提升了8 points；对于目标检测，将COCO-style Average Precision (AP)提升了2.3 points，将PASCAL-style AP，提高了3.8 points。</p><p>  FPN易于应用于到mask proposals并能提升实例分割的AR和速度。</p><p>  FPN可以进行端到端的多尺度训练，并且在训练和测试时都可以用，还不增加计算成本。</p></li></ul></li></ul><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><ul><li><p>Hand-engineered features and early neural networks</p><p>  SIFT特征用于特征点匹配，HOG和后来的SIFT在整个图像金字塔上进行密集计算，它们在图像分类、目标检测、人体姿态估计等任务中被大量使用。</p><p>  很快地也有了许多关于featurized image pyramids的研究。Dollar等人首先按尺度稀疏采样得到1个金字塔，然后对缺失的level进行插值，实现了快速的金字塔计算。在HOG和SIFT之前，使用卷积网络进行人脸检测的早期工作计算图像金字塔上的浅层网络以检测跨多尺度的人脸。</p></li><li><p>Deep ConvNet object detectors</p><p>  OverFeat采用1种与早期神经网络人脸检测器相似的策略，其将1个卷积网络作为1个sliding window detector应用于图像金字塔。</p><p>  R-CNN采用1种基于region proposal的策略，其中在使用卷积网络分类之前each proposal was scale-normalized。</p><p>  SPPNet表明这些基于region的detector可以更有效地应用于从单尺度图片提取出的特征图上。</p><p>  最近更加准确的Fast R-CNN和Faster R-CNN等算法提倡使用从单尺度图片计算得到的特征，因为这可以实现accuracy和speed的trade-off。</p><p>  <strong>Multi-scale detection仍然表现更佳，特别是对于小目标</strong>。</p></li><li><p>Methods using multiple layers</p><p>  最近大量方法利用卷积网络中的不同层提高了检测和分割的性能。FCN在多个尺度上计算每个category的的partial score以得到语义分割，Hypercolumns将类似方法应用于object instance segmentation。HyperNet、ParseNet和ION等方法在计算结果前将多层特征拼接，这等价于对转换后的特征求和。SSD和MS-CNN在多个特征层分别预测结果，并没有将多层特征结合。</p><p>  最近有很多方法在探索可以将低级特征和高级特征融合的lateral/skip connections，包括用于segmentation的U-Net和SharpMask、用于人脸检测的Recombinator networks、用于关键点预测的Stacked Hourglass networks。Ghiasi等人实现了在FCN上实现了1个Laplacian pyramid presentation，可以逐渐优化分割结果。即使这些方法采用了金字塔形状的结构，<strong>它们并不像featurized image pyramids一样在所有层独立预测结果</strong>，如图2所示，实际上<strong>这些方法还需要image pyramids以识别多尺度的目标</strong>。</p></li></ul><h1 id="3-Feature-Pyramid-Networks"><a href="#3-Feature-Pyramid-Networks" class="headerlink" title="3. Feature Pyramid Networks"></a>3. Feature Pyramid Networks</h1><p>FPN的目标是构造1个各层都具有较强高级语义信息的特征金字塔。FPN is general-purpose（多用途的），本文将其应用于RPN和Fast R-CNN，在Sec. 6中将其generalize到instance segmentation proposals。</p><p>FPN通过全卷积的方式，以任意大小的单张图片为输入，在多个尺度输出对应比例大小的特征图。该过程独立于骨干网络的具体架构，本文展示了基于ResNet的结果。FPN的构建包括1个bottom-up路径、1个top-down路径和lateral connections。</p><ul><li><p>Bottom-up pathway</p><p>  Bottom-up pathway就是骨干网络的feedforward，<strong>每层间下采样比例为2</strong>。网络中经常有连续几个层输出的特征图尺寸相同，我们称这些层位于同1个stage。在FPN中，为每个stage定义1个金字塔层。取每个stage中最后1层的输出代表该stage，因为每个stage中最深的层应该具有最强的特征。</p><p>  对于ResNet，使用后4个stage（相对于输入的步长分别为4、8、16、32）的输出，并不将第1个stage的输出包含到FPN中因为<strong>其内存占用量比较大</strong>。</p></li><li><p>Top-down pathway and lateral connections</p><p>  top-down pathway将金字塔中空间信息粗糙、语义信息更强的高层特征图上采样生成尺寸较大的特征图，然后通过lateral connections用bottom-up路径中的特征对这些特征进行增强。每个lateral connection将bottom-up路径和top-down路径中相同尺寸的特征图融合。bottom-up路径中的特征图具有较低级别的语义信息，但是<strong>由于其下采样次数较少</strong>所以其可以更准确地定位。</p><p>  <strong>图3</strong>展示了创建top-down路径中特征图的building block。将top-down路径中coarser-resolution的特征图<strong>上采样（比例为2）</strong>，将bottom-up路径中的特征图<strong>通过1×1卷积减少通道数</strong>，然后将两者<strong>相加（element-wise）</strong>。这个过程一直迭代到生成最大的特征图。在开始top-down路径之前会在bottom-up路径顶层<strong>使用1×1卷积生成尺度最小的特征图</strong>。在每个相加操作之后<strong>使用3×3卷积减少上采样带来的混淆效应</strong>（aliasing effect）。</p><p>  因为各个金字塔层都和传统的featurized image pyramid一样使用共享的classifier/regressor，所以<strong>将所有额外卷积层的输出通道数设置为256</strong>。在这些额外的层中，并不存在non-linearities，但我们凭经验发现其影响很小。</p><p>  simplicity对FPN非常重要，我们发现FPN对很多设计选择具有鲁棒性。我们使用更sophisticated的block（比如使用multilayer residual blocks作为连接）进行实验并观察到了略胜一筹的结果。设计更优的connection并非本文的重点，所以我们采用了上述的简单设计。</p></li></ul><h1 id="4-Applications"><a href="#4-Applications" class="headerlink" title="4. Applications"></a>4. Applications</h1><p>略……</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FPN网络图解&quot;&gt;&lt;a href=&quot;#FPN网络图解&quot; class=&quot;headerlink&quot; title=&quot;FPN网络图解&quot;&gt;&lt;/a&gt;FPN网络图解&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4bda90f7c75b31
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="FPN" scheme="https://chouxianyu.github.io/tags/FPN/"/>
    
  </entry>
  
  <entry>
    <title>ResNet50网络结构图及结构详解</title>
    <link href="https://chouxianyu.github.io/2021/02/26/ResNet50%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%9B%BE%E5%8F%8A%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3/"/>
    <id>https://chouxianyu.github.io/2021/02/26/ResNet50网络结构图及结构详解/</id>
    <published>2021-02-26T14:23:59.000Z</published>
    <updated>2021-03-17T12:32:32.765Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>之前我读了ResNet的论文Deep Residual Learning for Image Recognition，也做了<a href="https://zhuanlan.zhihu.com/p/353228657" target="_blank" rel="noopener">论文笔记</a>，笔记里记录了ResNet的理论基础（核心思想、基本Block结构、Bottleneck结构、ResNet多个版本的大致结构等等），看本文之间可以先看看打打理论基础。</p><p>一个下午的时间，我用PPT纯手工做了一张图片详细说明ResNet50的具体结构，本文将结合该图片详细介绍ResNet50。</p><p>这张图和这篇文章估计全网最详细了（狗头）。</p><p>废话不多说，先放图片（<strong>文末有图片和PPT源文件的下载链接</strong>）。</p><p><img src="https://pic1.zhimg.com/80/v2-b1ac9497249c5de6b812b1af729f4c44_720w.jpg" alt="img"></p><p>上图（称为<strong>本图</strong>）可划分为左、中、右3个部分，三者内容分别如下</p><ol><li>ResNet50整体结构</li><li>ResNet50各个Stage具体结构</li><li>Bottleneck具体结构</li></ol><p>接下来为正文内容，本文<strong>先后介绍了本图从左到右的3个部分，并对Bottleneck进行了简要分析</strong>。</p><h1 id="ResNet50整体结构"><a href="#ResNet50整体结构" class="headerlink" title="ResNet50整体结构"></a>ResNet50整体结构</h1><p>首先需要声明，这张图的内容是ResNet的Backbone部分（即图中没有ResNet中的全局平均池化层和全连接层）。</p><p>如本图所示，输入<code>INPUT</code>经过ResNet50的5个阶段（<em>Stage 0</em>、<em>Stage 1</em>、……）得到输出<code>OUTPUT</code>。</p><p>下面附上ResNet原文展示的ResNet结构，大家可以结合着看，看不懂也没关系，只看本文也可以无痛理解的。</p><p><img src="https://pic1.zhimg.com/80/v2-c4d2be9eb4ec809fded093a341efeab5_720w.png" alt="img"></p><p>上图描述了ResNet多个版本的具体结构，本文描述的“ResNet50”中的50指有50个层。和上图一样，本图描述的ResNet也分为5个阶段。</p><h1 id="ResNet各个Stage具体结构"><a href="#ResNet各个Stage具体结构" class="headerlink" title="ResNet各个Stage具体结构"></a>ResNet各个Stage具体结构</h1><p>如本图所示，ResNet分为5个stage（阶段），其中<em>Stage 0</em>的结构比较简单，可以视其为对<code>INPUT</code>的预处理，后4个Stage都由<em>Bottleneck</em>组成，结构较为相似。<em>Stage 1</em>包含3个<em>Bottleneck</em>，剩下的3个stage分别包括4、6、3个<em>Bottleneck</em>。</p><p>现在对<em>Stage 0</em>和<em>Stage 1</em>进行详细描述，同理就可以理解后3个Stage。</p><h2 id="Stage-0"><a href="#Stage-0" class="headerlink" title="Stage 0"></a>Stage 0</h2><ul><li><p><code>(3,224,224)</code>指输入<code>INPUT</code>的通道数(channel)、高(height)和宽(width)，即<code>(C,H,W)</code>。现假设输入的高度和宽度相等，所以用<code>(C,W,W)</code>表示。</p></li><li><p>该stage中第1层包括3个先后操作</p><ol><li><p><code>CONV</code></p><p> <code>CONV</code>是卷积（Convolution）的缩写，<code>7×7</code>指卷积核大小，<code>64</code>指卷积核的数量（即该卷积层输出的通道数），<code>/2</code>指卷积核的步长为2。</p></li><li><p><code>BN</code></p><p> <code>BN</code>是Batch Normalization的缩写，即常说的BN层。</p></li><li><p><code>RELU</code></p><p> <code>RELU</code>指ReLU激活函数。</p></li></ol></li><li><p>该stage中第2层为<code>MAXPOOL</code>，即最大池化层，其kernel大小为<code>3×3</code>、步长为<code>2</code>。</p></li><li><p><code>(64,56,56)</code>是该stage输出的通道数(channel)、高(height)和宽(width)，其中<code>64</code>等于该stage第1层卷积层中卷积核的数量，<code>56</code>等于<code>224/2/2</code>（步长为2会使输入尺寸减半）。</p></li></ul><p>总体来讲，在<em>Stage 0</em>中，形状为<code>(3,224,224)</code>的输入先后经过卷积层、BN层、ReLU激活函数、MaxPooling层得到了形状为<code>(64,56,56)</code>的输出。</p><h2 id="Stage-1"><a href="#Stage-1" class="headerlink" title="Stage 1"></a>Stage 1</h2><p>在理解了<em>Stage 0</em>以及熟悉图中各种符号的含义之后，可以很容易地理解<em>Stage 1</em>。理解了<em>Stage 1</em>之后，剩下的3个stage就不用我讲啦，你自己就能看懂。</p><p><em>Stage 1</em>的输入的形状为<code>(64,56,56)</code>，输出的形状为<code>(64,56,56)</code>。</p><p>下面介绍<em>Bottleneck</em>的具体结构（难点），把<em>Bottleneck</em>搞懂后，你就懂<em>Stage 1</em>了。</p><h1 id="Bottleneck具体结构"><a href="#Bottleneck具体结构" class="headerlink" title="Bottleneck具体结构"></a>Bottleneck具体结构</h1><p>现在让我们把目光放在本图最右侧，最右侧介绍了2种<em>Bottleneck</em>的结构。</p><p>“BTNK”是<em>BottleNeck</em>的缩写（本文自创，请谨慎使用）。</p><p>2种<em>Bottleneck</em>分别对应了2种情况：输入与输出通道数相同（<code>BTNK2</code>）、输入与输出通道数不同（<code>BTNK1</code>），这一点可以结合ResNet原文去看喔。</p><h2 id="BTNK2"><a href="#BTNK2" class="headerlink" title="BTNK2"></a>BTNK2</h2><p>我们首先来讲<code>BTNK2</code>。</p><p><code>BTNK2</code>有2个可变的参数<code>C</code>和<code>W</code>，即输入的形状<code>(C,W,W)</code>中的<code>c</code>和<code>W</code>。</p><p>令形状为<code>(C,W,W)</code>的输入为$x$，令<code>BTNK2</code>左侧的3个卷积块（以及相关BN和RELU）为函数$F(x)$，两者相加（$F(x)+x$）后再经过1个ReLU激活函数，就得到了<code>BTNK2</code>的输出，该输出的形状仍为<code>(C,W,W)</code>，即上文所说的<code>BTNK2</code>对应输入$x$与输出$F(x)$通道数相同的情况。</p><h2 id="BTNK1"><a href="#BTNK1" class="headerlink" title="BTNK1"></a>BTNK1</h2><p><code>BTNK1</code>有4个可变的参数<code>C</code>、<code>W</code>、<code>C1</code>和<code>S</code>。</p><p>与<code>BTNK2</code>相比，<code>BTNK2</code>多了1个右侧的卷积层，令其为函数$G(x)$。<code>BTNK1</code>对应了输入$x$与输出$F(x)$通道数不同的情况，也正是这个添加的卷积层将$x$变为$G(x)$，起到匹配输入与输出维度差异的作用（$G(x)$和$F(x)$通道数相同），进而可以进行求和$F(x)+G(x)$。</p><h2 id="简要分析"><a href="#简要分析" class="headerlink" title="简要分析"></a>简要分析</h2><p>可知，ResNet后4个stage中都有<code>BTNK1</code>和<code>BTNK2</code>。</p><ul><li><p>4个stage中<code>BTNK2</code>参数规律相同</p><p>  4个stage中<code>BTNK2</code>的参数全都是1个模式和规律，只是输入的形状<code>(C,W,W)</code>不同。</p></li><li><p><em>Stage 1</em>中<code>BTNK1</code>参数的规律与后3个stage不同</p><p>  然而，4个stage中<code>BTNK1</code>的参数的模式并非全都一样。具体来讲，后3个stage中<code>BTNK1</code>的参数模式一致，<em>Stage 1</em>中<code>BTNK1</code>的模式与后3个stage的不一样，这表现在以下2个方面：</p><ol><li><p>参数<code>S</code>：<code>BTNK1</code>左右两个1×1卷积层是否下采样</p><p> <em>Stage 1</em>中的<code>BTNK1</code>：步长<code>S</code>为1，没有进行下采样，输入尺寸和输出尺寸相等。</p><p> 后3个stage的<code>BTNK1</code>：步长<code>S</code>为2，进行了下采样，输入尺寸是输出尺寸的2倍。</p></li><li><p>参数<code>C</code>和<code>C1</code>：<code>BTNK1</code>左侧第一个1×1卷积层是否减少通道数</p><p> <em>Stage 1</em>中的<code>BTNK1</code>：输入通道数<code>C</code>和左侧1×1卷积层通道数<code>C1</code>相等（<code>C=C1=64</code>），即左侧1×1卷积层没有减少通道数。</p><p> 后3个stage的<code>BTNK1</code>：输入通道数<code>C</code>和左侧1×1卷积层通道数<code>C1</code>不相等（<code>C=2*C1</code>），左侧1×1卷积层有减少通道数。</p></li></ol></li><li><p>为什么<em>Stage 1</em>中<code>BTNK1</code>参数的规律与后3个stage不同？（个人观点）</p><ul><li><p>关于<code>BTNK1</code>左右两个1×1卷积层是否下采样</p><p>  因为<em>Stage 0</em>中刚刚对网络输入进行了卷积和最大池化，还没有进行残差学习，此时直接下采样会损失大量信息；而后3个stage直接进行下采样时，前面的网络已经进行过残差学习了，所以可以直接进行下采样。</p></li><li><p>关于<code>BTNK1</code>左侧第一个1×1卷积层是否减少通道数</p><p>  根据ResNet原文可知，<em>Bottleneck</em>左侧两个1×1卷积层的主要作用分别是减少通道数和恢复通道数，这样就可以使它们中间的3×3卷积层的输入和输出的通道数都较小，因此效率更高。</p><p>  <em>Stage 1</em>中<code>BTNK1</code>的输入通道数<code>C</code>为64，它本来就比较小，因此没有必要通过左侧第一个1×1卷积层减少通道数。</p></li></ul></li></ul><h1 id="福利"><a href="#福利" class="headerlink" title="福利"></a>福利</h1><p><img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" alt="img"></p><p>扫码关注微信公众号后回复<code>resnet</code>即可直接获取图片和PPT源文件的下载链接。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://www.bilibili.com/read/cv2051292" target="_blank" rel="noopener">https://www.bilibili.com/read/cv2051292</a></p><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">https://arxiv.org/abs/1512.03385</a></p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;之前我读了ResNet的论文Deep Residual Learning for Image Recognitio
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="ResNet" scheme="https://chouxianyu.github.io/tags/ResNet/"/>
    
  </entry>
  
  <entry>
    <title>ResNet论文阅读笔记</title>
    <link href="https://chouxianyu.github.io/2021/02/26/ResNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://chouxianyu.github.io/2021/02/26/ResNet论文阅读笔记/</id>
    <published>2021-02-26T13:33:20.000Z</published>
    <updated>2021-03-17T12:34:45.326Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><ul><li><p>论文名称</p><p>  Deep Residual Learning for Image Recognition</p></li><li><p>作者</p><p>  Kaiming He</p><p>  Microsoft Research</p></li><li><p>发表时间</p><p>  2015年</p></li><li><p>来源</p><p>  CVPR2016</p></li></ul><h1 id="主要收获"><a href="#主要收获" class="headerlink" title="主要收获"></a>主要收获</h1><ul><li>知识<ul><li>较深的神经网络难以训练</li><li><strong>网络的深度</strong>对于许多视觉识别任务至关重要<ul><li>152层的ResNet，<strong>8× deeper than VGG</strong>，但却有<strong>更低的计算复杂度</strong>，在ImageNet测试集上错误率为3.57%，获ILSVRC 2015中图像分类任务的第1名。</li><li>ResNet在COCO目标检测数据集上获得了<strong>28％</strong>的相对改进，在其它几个数据集和任务上也取得第1名。</li></ul></li><li><strong>假设</strong>多个非线性层可以渐近地逼近复杂函数（该假设还有待讨论，详见引文），则等效于假设它们可以渐近地近似残差函数。</li><li>理论基础：假设目标函数为H(x)，可将其表示为H(x)=F(x)+x。若最优解是identity mapping，则此时F(x)为0，<strong>使参数为0比学习H(x)更加容易</strong>。</li><li>F(x)+x可以通过<strong>shortcut connections</strong>实现，不需额外参数，仅需微小计算量（element-wise addition）。</li><li>基础网络（Plain Network）采用VGGNet的设计思路，主要使用3×3卷积并遵守2个设计原则：①若输出特征图的尺寸不变，则通道数也应不变；②若输出特征的尺寸减半，则通道数应增1倍以保留时间复杂度。</li><li><a href="https://zhuanlan.zhihu.com/p/353235794" target="_blank" rel="noopener">ResNet50网络结构图解以及Bottleneck结构图解</a></li></ul></li><li>未知点<ul><li>residual：并不只是ResNet中的residual，在其它领域和算法中也有residual的概念，有待了解。</li><li>inception：《Going deeper with convolutions》</li><li>highway networks：《Highway networks》、《Training very deep networks》</li><li>VGGNet的设计思想</li><li>bacth normalization</li><li>mini-batch</li></ul></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul><li>较深的神经网络难以训练，本文提出一个residual learning framework训练较深的神经网络。</li><li>We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.</li><li>通过大量empirical实验证明，本文中提出的残差网络<strong>更易优化</strong>，并可以从大量增加的网络深度中获取<strong>准确性</strong>。用<strong>152层（8× deeper than VGG）</strong>，但却有<strong>更低的计算复杂度</strong>，在ImageNet测试集上错误率为3.57%(top5)，获ILSVRC 2015中图像分类任务的第1名。</li><li>网络的深度对于许多视觉识别任务至关重要。ResNet在COCO目标检测数据集上获得了<strong>28％</strong>的相对改进，在其它几个数据集和任务上也取得第1名。</li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><ul><li><p>背景/问题</p><ul><li><p><strong>深度网络以1种end-to-end的多层的方式，自然地集成了low/mid/high-level features和classifiers[引]，并且features的“levels”可以通过堆叠层数（深度）来enrich。</strong></p></li><li><p>最近有研究[引]证明网络的深度十分重要，ImageNet上的领先结果[引]都使用了”very deep” model[引]（16到30层），较深的网络在其它视觉识别任务中也有好的效果。</p></li><li><p>既然深度如此重要，那有个问题是：<strong>Is learning better networks as easy as stacking more layers</strong>。要回答这个问题，1个障碍是<strong>梯度消失/爆炸</strong>（会影响模型收敛），该问题已通过<strong>normalized initialization</strong>[引]和<strong>intermediate normalization layers</strong>[引]得到很大的解决，可以使数十层的网络通过SGD收敛。</p></li><li><p>深度模型收敛之后，又遇到了<strong>the degradation problem：随着网络深度增加，准确率变得饱和然后迅速下降</strong>，并且实验结果说明这种下降并不是过拟合导致的。相关研究[引]指出<strong>给1个层数合适的网络添加更多的层会导致训练误差增高</strong>，本文图1和图4也彻底证明了这一点。</p><p>  考虑1个shallower architecture并基于此添加一些层形成1个deeper counterpart，如果添加的这些层是identity mapping，那这个deeper counterpart则等同于这个shallower architecture。这说明<strong>1个deeper model的训练误差应不高于其shallower counterpart</strong>。但实验证明，目前的模型并没有达到这个效果。</p></li></ul></li><li><p>本文所提方法的原理及优点</p><ul><li><p>本文提出1个deep residual learning framework来解决degradation问题。each few stacked layers并不直接拟合1个desired underlying mapping：$H(x)$，而是拟合1个residual mapping：$F(x)=H(x)-x$，即原先想要的underlying mapping为$H(x)=F(x)+x$。</p></li><li><p>我们假定该residual mapping比原先的mapping更易优化。<strong>在极端情况下如果identity mapping是最优的，使residual为0比拟合1个identity mapping更加容易。</strong></p><p>  $F(x)+x$可以<strong>通过shortcut connections实现</strong>，在本方法中其仅需进行identity mapping和相加，如原文图2所示。<strong>这种identity shortcut connections不需要额外参数和计算量</strong>。整个网络仍可以通过SGD进行end-to-end训练，并易于通过Caffe等开源库实现。</p></li></ul></li><li><p>实验与结果</p><ul><li>本文在ImageNet上进行了全面实验以说明degradation问题和评估本文的方法。<ol><li>Our extremely deep residual nets are <strong>easy to optimize</strong>, but the counterpart “plain” nets (that simply stack layers) exhibit <strong>higher training error when the depth increases</strong>.</li><li>Our deep residual nets can <strong>easily enjoy accuracy gains from greatly increased depth</strong>, producing results <strong>substantially better than previous networks</strong>.</li></ol></li><li>在CIFAR-10上进行实验也出现了类似现象，说明degradation问题和<strong>本文方法并不只适用于特定数据集</strong>。我们训练了超过100层的网络，还尝试了超过1000层的网络。</li><li>在ImageNet上，我们的<strong>152层</strong>网络是最深的，但其<strong>比VGG计算复杂度更低</strong>，在ImageNet测试集上错误率为3.57%(top5)，获ILSVRC 2015中图像分类任务的第1名。在其它识别任务中也有出色表现，有多项第1名（略）。这说明the residual learning principle is generic，我们猜想<strong>本方法也适用于其它视觉和非视觉问题</strong>。</li></ul></li></ul><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><ul><li><p><strong>Residual Representations</strong></p><ul><li><p>image recognition</p><p>  VLAD[引]、Fisher Vector[引]、encoding residual vectors[引]</p></li><li><p>low-level vision and computer graphics</p><p>  Multigrid method[引]、hierarchical basis preconditioning[引]、</p></li></ul></li><li><p><strong>Shortcut Connections</strong></p><ul><li><p>some practices and theories of shortcut connections</p><p>  具体看引文</p></li><li><p>comparison with the shortcut connections with gating functions（shortcut connections with gating functions）</p></li></ul></li></ul><h1 id="3-Deep-Residual-Learning"><a href="#3-Deep-Residual-Learning" class="headerlink" title="3. Deep Residual Learning"></a>3. Deep Residual Learning</h1><h2 id="3-1-Residual-Learning"><a href="#3-1-Residual-Learning" class="headerlink" title="3.1. Residual Learning"></a>3.1. Residual Learning</h2><p>假设某几个连续的层（并非必须是整个网络）要拟合的目标是函数H(x)，x是其中第1层的输入。<strong>假设</strong>多个非线性层可以渐近地逼近复杂函数（该假设还有待讨论，详见引文），则等效于假设它们可以渐近地近似残差函数，即F(x)=H(x)-x（假设输入和输出的维数相同）。<strong>虽然H(x)和F(x)+x形式相同， 但学习成本是不同的。</strong>这种重构的思路来源于原文图1所示的the degradation problem，其指出深度模型在通过多个层拟合identity mappings时遇到了问题。通过重构，训练师使参数趋于0会更容易。</p><p>在真实情况中，<strong>虽然identity mappings并不一定是最优解，但该思路还是可能对the degradation problem进行了precondition</strong>。相对于1个zero mapping，如果最优解更接近于identity mappings，那么参考identity mappings做扰动应该比从头学习1个新的函数要更容易。本文通过<strong>实验（图7）</strong>表明，学习到的残差函数通常具有较小的响应（small responses），这表明identity mappings提供了合理的precondition。</p><h2 id="3-2-Identity-Mapping-by-Shortcuts"><a href="#3-2-Identity-Mapping-by-Shortcuts" class="headerlink" title="3.2. Identity Mapping by Shortcuts"></a>3.2. Identity Mapping by Shortcuts</h2><p>每几个层都进行residual learning，形成1个具有多层的block（见原文图2）。图2所示的block有2层，其公式（式1）为$y=F(x,\{W_i\})+x=\sigma(W_2\sigma(W_1x+b_1)+b)$，其中$\sigma$是ReLU。</p><p>F(x)+x通过shortcut connection实现，即保存输入x然后和输出F(x)相加得到F(x)+x，并且使用的这种shortcut connection并不增加参数和计算量（仅增加了微不足道的element-wise addition）。</p><p>在式1中，F(x)和x的通道数必须是一样的，否则就需要在shortcut connections中对x进行1个线性投影（<strong>linear projection</strong>）以匹配维度，即式2：$y=F(x,\{W_i\})+W_sx$，其中$W_s$仅被用来匹配F(x)的维度。</p><p>每个block中可以有更多层，比如3（如图5所示的BottleNeck），但如果每个block中只有1层的话，公式就变成了1个线性层$y = W_1x+x$。</p><p>这种block也适用于卷积层，其中F(x)+x是每个维度对应位置元素相加。</p><h2 id="3-3-Network-Architectures"><a href="#3-3-Network-Architectures" class="headerlink" title="3.3. Network Architectures"></a>3.3. Network Architectures</h2><ul><li><p><strong>Plain Network</strong></p><p>  根据VGG网络的理论，主要使用3×3卷积并遵守2个设计原则：①若输出特征图的尺寸不变，则通道数也应不变；②若输出特征的尺寸减半，则通道数应增1倍以保留时间复杂度。</p><p>  本文直接通过步长为2的卷积层进行下采样，网络尾部是1个global average pooling layer和1个1000-way fully-connected layer with softmax，共有34层权重（图3中间的网络）。</p><p>  该模型中的通道比VGG网络更少，有更低的复杂度，FLOPS是VGG19的18%。</p></li><li><p><strong>Residual Network</strong></p><p>  基于上述Plain Network，添加shortcut connections（图3右边的网络），形成ResNet34。</p><p>  对于identity shortcut，当输出和输入的通道数相同时（<strong>实线</strong>），就直接每个通道对应位置元素相加；</p><p>  当输出的通道数大于输入时（<strong>虚线</strong>），有2个方法：①The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. ②通过式2表示的projection shortcut匹配通道（以1×1卷积实现）。对于这2个方法，当输出和输入的尺寸不同时，这是通过步长为2实现（尺寸减半，通道数对应增加1倍）的。</p></li></ul><h2 id="3-4-Implementation"><a href="#3-4-Implementation" class="headerlink" title="3.4. Implementation"></a>3.4. Implementation</h2><p>在ImageNet上</p><p>按照引文41，将图片根据短边随机缩放到[256,480]。随机224×224 crop，随机水平翻转，减去像素均值。</p><p>按照引文21，每个卷积后和每个激活函数前使用bacth normalization。</p><p>按照引文13，初始化模型参数。</p><p>从头训练，SGD，mini-batch大小为256，学习率从0.1开始当loss稳定时除以10，训练了60W个iteration。</p><p>weight decay 0.0001，momentum 0.9。</p><p>不使用dropout。</p><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><h2 id="4-1-ImageNet-Classification"><a href="#4-1-ImageNet-Classification" class="headerlink" title="4.1. ImageNet Classification"></a>4.1. ImageNet Classification</h2><ul><li><p><strong>Plain Networks</strong></p><p>  略</p></li><li><p><strong>Residual Networks</strong></p><p>  略</p></li><li><p><strong>Deeper Bottleneck Architectures</strong></p><p>  考虑到可接受的训练时长，使用Bottleneck（图5所示），每个block中包含3个层：1×1卷积、3×3卷积、1×1卷积，其中2个1×1卷积分别用来减少和恢复维度，使得3×3卷积接收和输出更少的通道数。</p><p>  不需额外参数的identity shortcuts对于Bottleneck非常重要，如果把Bottleneck中的identity shortcut换成projection，模型的时间复杂度和参数量就会增加1倍，因为shortcut连接到了2个高维特征上。所以identity shortcuts使得基于Bottleneck的模型更加有效。</p></li><li><p><strong>50-layer ResNet</strong></p><p>  ResNet50，将ResNet34中包含2层的block换成Bottleneck，就得到了ResNet50。当输出的通道数大于输入时，使用第2种方法（式2）。</p></li><li><p>略……</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文名称&lt;/p&gt;
&lt;p&gt;  Deep Residual Learning for
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文笔记" scheme="https://chouxianyu.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="ResNet" scheme="https://chouxianyu.github.io/tags/ResNet/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-6.4学习率调整方法</title>
    <link href="https://chouxianyu.github.io/2021/02/15/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-6-4%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95/"/>
    <id>https://chouxianyu.github.io/2021/02/15/李宏毅机器学习课程笔记-6-4学习率调整方法/</id>
    <published>2021-02-15T04:18:25.000Z</published>
    <updated>2021-02-15T04:19:24.329Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>2013年Hinton在Coursera提出。</p><ul><li><p>背景</p><p>  RMSProp是Adagrad的升级版。</p><p>  在训练神经网络时，损失函数不一定是凸函数（局部最小值即为全局最小值），可能是各种各样的函数，有时需要较大的学习率，有时需要较小的学习率，而Adagrad并不能实现这种效果，因此产生了RMSProp。</p></li><li><p>定义</p><script type="math/tex; mode=display">  w^{t+1}=w^t-\frac{\eta}{\sigma^t}g^t\\  \sigma^0=g^0,\sigma^t=\sqrt{\alpha(\sigma^{t-1})^2+(1-\alpha)(g^t)^2}</script><p>  其中$w$是某个参数；$\eta$是学习率；$g$是梯度；$\alpha$代表旧的梯度的重要性，值越小则旧的梯度越不重要。</p></li><li><p>神经网络中很难找到最优的参数吗？</p><p>  面临的问题有plateau、saddle point和local minima。</p><p>  |     英文     |    中文    |                  梯度                   |<br>  | :—————: | :————: | :——————————————————-: |<br>  |   plateau    |   停滞期   | $\frac{\partial L}{\partial w}\approx0$ |<br>  | saddle point |    鞍点    |    $\frac{\partial L}{\partial w}=0$    |<br>  | local minima | 局部最小值 |    $\frac{\partial L}{\partial w}=0$    |</p><p>  2007年有人（名字读音好像是young la ken）指出神经网络的error surface是很平滑的，没有很多局部最优。</p><p>  假设有1000个参数，一个参数处于局部最优的概率是$p$，则整个神经网络处于局部最优的概率是$p^{1000}$，这个值是很小的。</p></li></ul><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>1986年提出</p><ul><li><p>如何处理停滞期、鞍点、局部最小值等问题？</p><p>  考虑现实世界中物体具有惯性、动量（Momentum）的特点，尽可能避免“小球”陷入error surface上的这几种位置。</p></li><li><p>定义</p><p>  如下图所示，不仅考虑当前的梯度，还考虑上一次的移动方向：$v^t=\lambda v^{t-1}-\eta g^t,v^0=0$，</p><p>  其中上标$t$是迭代次数；$v$指移动方向（movement），类似于物理中的速度；$g$是梯度（gradient）；$\lambda$用来控制惯性的重要性，值越大代表惯性越重要；$\eta$是学习率。</p><p>  <img src="https://img2020.cnblogs.com/blog/1478490/202012/1478490-20201216171725468-884779641.jpg" alt="Momentum"></p></li></ul><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>RMSProp+Momentum+Bias Correction，2015年提出</p><h2 id="Adam-VS-SGDM"><a href="#Adam-VS-SGDM" class="headerlink" title="Adam VS SGDM"></a>Adam VS SGDM</h2><p>目前常用的就是Adam和SGDM。</p><p>Adam训练速度快，large generalization gap（在训练集和验证集上的性能差异大），但不稳定；SGDM更稳定，little generalization gap，更加converge（收敛）。</p><div class="table-container"><table><thead><tr><th style="text-align:center">领域</th><th style="text-align:center">技术/模型</th><th style="text-align:center">优化器</th></tr></thead><tbody><tr><td style="text-align:center">Q&amp;A、文意理解、文章生成</td><td style="text-align:center">BERT</td><td style="text-align:center">Adam</td></tr><tr><td style="text-align:center">BERT的Backbone、翻译</td><td style="text-align:center">Transformer</td><td style="text-align:center">Adam</td></tr><tr><td style="text-align:center">语音生成</td><td style="text-align:center">Tacotron</td><td style="text-align:center">Adam</td></tr><tr><td style="text-align:center">目标检测</td><td style="text-align:center">YOLO</td><td style="text-align:center">SGDM</td></tr><tr><td style="text-align:center">目标检测</td><td style="text-align:center">Mask R-CNN</td><td style="text-align:center">SGDM</td></tr><tr><td style="text-align:center">图片分类</td><td style="text-align:center">ResNet</td><td style="text-align:center">SGDM</td></tr><tr><td style="text-align:center">图片生成</td><td style="text-align:center">Big-GAN</td><td style="text-align:center">Adam</td></tr><tr><td style="text-align:center">元学习</td><td style="text-align:center">MAML</td><td style="text-align:center">Adam</td></tr></tbody></table></div><p>SGDM适用于计算机视觉，Adam适用于NLP、Speech Synthesis、GAN、Reinforcement Learning。</p><h2 id="SWATS"><a href="#SWATS" class="headerlink" title="SWATS"></a>SWATS</h2><p>2017年提出，尝试把Adam和SGDM结合，其实就是前一段时间用Adam，后一段时间用SGDM，但在切换时需要解决一些问题。</p><h2 id="尝试改进Adam"><a href="#尝试改进Adam" class="headerlink" title="尝试改进Adam"></a>尝试改进Adam</h2><ul><li><p>AMSGrad</p><ul><li><p>Adam的问题</p><blockquote><p>Non-informative gradients contribute more than informative gradients.</p></blockquote><p>  在Adam中，之前所有的梯度都会对第$t$步的movement产生影响。然而较早阶段(比如第1、2步)的梯度信息是相对无效的，较晚阶段（比如$t-1$、$t-2$步）的梯度信息是相对有效的。在Adam中，可能发生较早阶段梯度相对于较晚阶段梯度比重更大的问题。</p></li><li><p>提出AMSGrad</p><p>  2018年提出</p></li></ul></li><li><p>AdaBound</p><p>  2019年提出，目的也是改进Adam。</p></li><li><p>Adam需要warm up吗？需要</p><p>  warm up：开始时学习率小，后面学习率大。</p><p>  因为实验结果说明在刚开始的几次（大概是10次）迭代中，参数值的分布比较散乱（distort），因此梯度值就比较散乱，导致梯度下降不稳定。</p></li><li><p>RAdam</p><p>  2020年提出</p></li><li><p>Lookahead</p><p>  2019年提出，像一个wrapper一样套在优化器外面，适用于Adam、SGDM等任何优化器。</p><p>  迭代几次后会回头检查一下。</p></li><li><p>Nadam</p><p>  2016年提出，把NAG的概念应用到Adam上。</p></li><li><p>AdamW</p><p>  2017年提出，这个优化器还是有重要应用的（训练出了某个BERT模型）。</p></li></ul><h2 id="尝试改进SGDM"><a href="#尝试改进SGDM" class="headerlink" title="尝试改进SGDM"></a>尝试改进SGDM</h2><ul><li><p>LR range test</p><p>  2017年提出</p></li><li><p>Cyclical LR</p><p>  2017年提出</p></li><li><p>SGDR</p><p>  2017年提出，模拟Cosine但并不是Cosine</p></li><li><p>One-cycle LR</p><p>  2017年提出，warm-up+annealing+fine-tuning</p></li><li><p>SGDW</p><p>  2017年提出，</p></li></ul><h2 id="改进Momentum"><a href="#改进Momentum" class="headerlink" title="改进Momentum"></a>改进Momentum</h2><ul><li><p>背景</p><p>如果梯度指出要停下来，但动量说要继续走，这样可能导致坏的结果。</p></li><li><p>NAG（Nesterov accelerated gradient）</p><p>  1983年提出，会预测下一步。</p></li></ul><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p>如果学习率调整得较好，随着迭代次数增加，神经网络在训练集上的loss会越来越小，但因为验证集（Validation set）和训练集不完全一样，所以神经网络在验证集上的loss可能不降反升，所以我们应该在神经网络在验证集上loss最小时停止训练。</p><p><a href="https://keras.io/getting_started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore" target="_blank" rel="noopener">Keras文档</a>中就有关于Early stopping的说明。</p><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;RMSProp&quot;&gt;&lt;a href=&quot;#RMSProp&quot; class=&quot;headerlink&quot; title=&quot;RMSProp&quot;&gt;&lt;/a&gt;RMSProp&lt;/h2&gt;&lt;p&gt;2013年Hinton在Coursera提出。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="学习率" scheme="https://chouxianyu.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-6.3常用激活函数</title>
    <link href="https://chouxianyu.github.io/2021/02/15/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-6-3%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>https://chouxianyu.github.io/2021/02/15/李宏毅机器学习课程笔记-6-3常用激活函数/</id>
    <published>2021-02-15T03:59:27.000Z</published>
    <updated>2021-02-15T04:01:05.856Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h2 id="梯度消失（Vanishing-Gradient-Problem）"><a href="#梯度消失（Vanishing-Gradient-Problem）" class="headerlink" title="梯度消失（Vanishing Gradient Problem）"></a>梯度消失（Vanishing Gradient Problem）</h2><ul><li><p>定义</p><p>  1980年代常用的激活函数是sigmoid函数。以MNIST手写数字识别为例，在使用sigmoid函数时会发现随着神经网络层数增加，识别准确率逐渐下降，这个现象的原因并不是过拟合（原因见上文），而是梯度消失。</p><p>  <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201117115824VanishingGradientProblem.jpg" alt="VanishingGradientProblem"></p><p>  如上图所示，当神经网络层数很多时，靠近输入层的参数的梯度会很小，靠近输出层的参数的梯度会很大。当每个参数的学习率相同时，靠近输入层的参数会更新得很慢，靠近输出层的几层参数会更新得很快。所以，当靠近输入层的参数几乎还是随机数时，靠近输出层的参数已经收敛了。</p></li><li><p>原因</p><p>  按照反向传播的式子，这确实是会发生的。</p><p>  直观感觉上，sigmoid函数输入的范围是无穷大，但输出的范围是[0,1]，也就是说sigmoid函数减弱了输入变化导致输出变化的幅度。那为什么靠近输出层的参数的梯度更大呢？sigmoid函数是一层层叠起来的，不断地减弱靠近输入层的参数的变化导致输出变化的幅度，所以更靠后的参数的梯度越大。</p></li><li><p>解决方法</p><p>  Hinton提出无监督逐层训练方法以解决这个问题，其基本思想是每次训练一层隐节点。</p><p>  后来Hinton等人提出修改激活函数，比如换成ReLU。</p></li></ul><h2 id="ReLU-Rectified-Linear-Unit"><a href="#ReLU-Rectified-Linear-Unit" class="headerlink" title="ReLU(Rectified Linear Unit)"></a>ReLU(Rectified Linear Unit)</h2><ul><li><p>定义</p><p>  当输入小于等于0时，输出为0；当输入大于0时，输出等于输入。</p></li><li><p>优点</p><p>  相比于sigmoid函数，它有以下优点</p><ol><li>运算更快</li><li>更符合生物学</li><li>等同于无穷多个bias不同的sigmoid函数叠加起来</li><li>可以解决梯度消失问题</li></ol></li><li><p>如何解决梯度消失问题</p><p>  当ReLU输出为0时该激活函数对神经网络不起作用，所以在神经网络中生效的激活函数都是输出等于输入，所以就不会出现sigmoid函数导致的减弱输入变化导致输出变化的幅度的情况。</p></li><li><p>ReLU会使整个神经网络变成线性的吗？</p><p>  可知有效的激活函数都是线性的，但整个神经网络还是非线性的。当输入改变很小、不改变每个激活函数的Operation Region（操作区域，大概意思就是输入范围）时，整个神经网络是线性的；当输入改变很大、改变了Operation Region时，整个神经网络就是非线性的。==目前我是凭直觉理解这一点，还未细究==</p></li><li><p>ReLU可以做微分吗？</p><p>  不用处理输入为0的情况，当输入小于0时，微分就是0，当输入大于0时微分就是1。</p></li></ul><h2 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2><p>当输入小于等于0时，输出为输入的0.01倍；当输入大于0时，输出等于输入。</p><h2 id="Parametric-ReLU"><a href="#Parametric-ReLU" class="headerlink" title="Parametric ReLU"></a>Parametric ReLU</h2><p>当输入小于等于0时，输出为输入的$\alpha$倍；当输入大于0时，输出等于输入。</p><p>其中$\alpha$是通过梯度下降学习到的参数</p><h2 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h2><ul><li><p>定义</p><p>  通过学习得到一个激活函数，人为将每层输出的多个值分组，然后输出每组值中的最大值。（跟maxpooling一模一样）</p></li><li><p>ReLU是Maxout的一个特例</p><p>  <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201216075138Maxout1.png" alt="Maxout1"></p></li><li><p>Maxout比ReLU包含了更多的函数</p><p>  <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201216075144Maxout2.jpg" alt="Maxout2"></p></li><li><p>Maxout可以得到任意的分段线性凸函数（piecewise linear convex），有几个分段取决于每组里有几个值</p><p>  <img src="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201216075148Maxout3.jpg" alt="Maxout3"></p></li><li><p>如何训练Maxout</p><p>  Maxout只是选择输出哪一个线性函数的值而已，因此Maxout激活函数还是线性的。</p><p>  因为在多个值中只选择最大值进行输出，所以会形成一个比较瘦长/窄深的神经网络。</p><p>  在多个值中只选择最大值进行输出，这并不会导致一些参数无法被训练：因为输入不同导致一组值中的最大值不同，所以各个参数都可能被训练到。</p><p>  当输入不同时，形成的也是不同结构的神经网络。</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h2 id=&quot;梯度消失（Vanishing-Gradient-Problem）&quot;&gt;&lt;a href=&quot;#梯度消失（Vanishing-Gradient-Problem）&quot; class=&quot;headerlink&quot; title=&quot;梯度消失（Vanishing 
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="梯度消失" scheme="https://chouxianyu.github.io/tags/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/"/>
    
      <category term="激活函数" scheme="https://chouxianyu.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-6.2神经网络精度低不一定是因为过拟合</title>
    <link href="https://chouxianyu.github.io/2021/02/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-6-2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%B2%BE%E5%BA%A6%E4%BD%8E%E4%B8%8D%E4%B8%80%E5%AE%9A%E6%98%AF%E5%9B%A0%E4%B8%BA%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>https://chouxianyu.github.io/2021/02/10/李宏毅机器学习课程笔记-6-2神经网络精度低不一定是因为过拟合/</id>
    <published>2021-02-10T04:59:34.000Z</published>
    <updated>2021-02-10T05:00:13.727Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>相比于决策树等方法，神经网络更不容易过拟合。</p><p>  K近邻、决策树等方法在训练集上更容易得到100%等很高的正确率，神经网络一般不能，训练神经网络首先遇到的问题一般是在训练集上的精度不高。</p></li><li><p>不要总是把精度低归咎于过拟合</p><p>  如果模型在训练集上精度高，对于K近邻、决策树等方法我们可以直接判断为过拟合，但对于神经网络来说我们还需要检查神经网络在测试集上的精度。<strong>如果神经网络在训练集上精度高但在测试集上精度低，这才说明神经网络过拟合了</strong>。</p><p>  如果56层的神经网络和20层的神经网络相比，56层网络在测试集上的精度低于20层网络，这还不能判断为56层网络包含了过多参数导致过拟合。一般来讲，56层网络优于20层网络，但如果我们发现56层网络在训练集上的精度本来就低于20层网络，那原因可能有很多而非过拟合，比如56层网络没训练好导致一个不好的局部最优、虽然56层网络的参数多但结构有问题等等。</p><p>  感兴趣可以看看ResNet论文<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a>，这篇论文可能与该问题有关。</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;相比于决策树等方法，神经网络更不容易过拟合。&lt;/p&gt;
&lt;p&gt;  K近邻、决策树等方法在训练集上更容易得到100%等很高的正确率，神经网络一般不能，训练神经网络首先遇到的问题一般是在训练集上的精度不高。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;不要总是把精度低归咎
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="过拟合" scheme="https://chouxianyu.github.io/tags/%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅机器学习课程笔记-6.1神经网络训练问题与解决方案</title>
    <link href="https://chouxianyu.github.io/2021/02/09/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-6-1%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://chouxianyu.github.io/2021/02/09/李宏毅机器学习课程笔记-6-1神经网络训练问题与解决方案/</id>
    <published>2021-02-09T10:36:07.000Z</published>
    <updated>2021-02-09T10:56:28.190Z</updated>
    
    <content type="html"><![CDATA[<h2 id="明确问题类型及其对应方法"><a href="#明确问题类型及其对应方法" class="headerlink" title="明确问题类型及其对应方法"></a>明确问题类型及其对应方法</h2><p>在深度学习中，一般有两种问题：</p><ol><li>在训练集上性能不好</li><li>在测试集上性能不好。</li></ol><p>当一个方法被提出时，它往往是针对这两个问题其中之一的，比如dropout方法是用来处理在测试集上性能不好的情况。</p><h2 id="处理神经网络在训练集上性能不好的情况的方法"><a href="#处理神经网络在训练集上性能不好的情况的方法" class="headerlink" title="处理神经网络在训练集上性能不好的情况的方法"></a>处理神经网络在训练集上性能不好的情况的方法</h2><ul><li><p>修改神经网络架构，比如换成更好的激活函数</p><p>  sigmoid函数会导致梯度消失，可以换成ReLU、Leaky ReLU、Parametric ReLU、Maxout</p></li><li><p>调整学习率</p><p>  比如RMSProp、Momentum、Adam</p></li></ul><h2 id="处理神经网络在测试集上性能不好的情况的方法"><a href="#处理神经网络在测试集上性能不好的情况的方法" class="headerlink" title="处理神经网络在测试集上性能不好的情况的方法"></a>处理神经网络在测试集上性能不好的情况的方法</h2><ul><li><p>Early Stopping、Regularization，这两个是比较传统的方法，不只适用于深度学习</p></li><li><p>Dropout，比较有深度学习的特色</p></li></ul><h2 id="一些性能优化方法的简介"><a href="#一些性能优化方法的简介" class="headerlink" title="一些性能优化方法的简介"></a>一些性能优化方法的简介</h2><p>下面3点都是在增加模型的随机性，鼓励模型做更多的exploration。</p><ul><li><p>Shuffling</p><p>  输入数据的顺序不要固定，mini-batch每次要重新生成</p></li><li><p>Dropout</p><p>  鼓励每个神经元都学到东西，也可以广义地理解为增加随机性</p></li><li><p>Gradient noise</p><p>  2015年提出，计算完梯度后，加上Gaussian noise。</p><p>  随着迭代次数增加，noise应该逐渐变小。</p></li></ul><p>下面3点是关于学习率调整的技巧</p><ul><li><p>warm up</p><p>  开始时学习率较小，等稳定之后学习率变大</p></li><li><p>Curriculum learning</p><p>  2009年提出，先使用简单的数据训练模型（一方面此时模型比较弱，另一方面在clean data中更容易提取到核心特征），然后再用难的数据训练模型。</p><p>  这样可以提高模型的鲁棒性。</p></li><li><p>Fine-tuning</p></li></ul><p>下面3点是关于数据预处理的技巧，避免模型学习到太极端的参数</p><ul><li><p>Normalization</p><p>  有Batch Normalization、Instance Normalization、Group Normalization、Layer Normalization、Positional Normalization</p></li><li><p>Regularization</p></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;明确问题类型及其对应方法&quot;&gt;&lt;a href=&quot;#明确问题类型及其对应方法&quot; class=&quot;headerlink&quot; title=&quot;明确问题类型及其对应方法&quot;&gt;&lt;/a&gt;明确问题类型及其对应方法&lt;/h2&gt;&lt;p&gt;在深度学习中，一般有两种问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在
      
    
    </summary>
    
    
      <category term="机器学习" scheme="https://chouxianyu.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://chouxianyu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://chouxianyu.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="学习率" scheme="https://chouxianyu.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
      <category term="正则化" scheme="https://chouxianyu.github.io/tags/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    
      <category term="激活函数" scheme="https://chouxianyu.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
      <category term="Dropout" scheme="https://chouxianyu.github.io/tags/Dropout/"/>
    
  </entry>
  
  <entry>
    <title>PAT甲级1055The World&#39;s Richest</title>
    <link href="https://chouxianyu.github.io/2021/02/05/PAT%E7%94%B2%E7%BA%A71055The-World-s-Richest/"/>
    <id>https://chouxianyu.github.io/2021/02/05/PAT甲级1055The-World-s-Richest/</id>
    <published>2021-02-05T11:32:24.000Z</published>
    <updated>2021-02-10T05:06:52.780Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="题目介绍"><a href="#题目介绍" class="headerlink" title="题目介绍"></a>题目介绍</h1><ul><li><p>题目链接</p><p>  <a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805421066272768" target="_blank" rel="noopener">https://pintia.cn/problem-sets/994805342720868352/problems/994805421066272768</a></p></li><li><p>题目考点</p><p>  排序，重点在时间复杂度优化上</p></li><li><p>题目难度</p><p>  PAT甲级25分</p></li><li><p>题目大意</p><p>  给出N个人，请找出指定年龄范围内最有钱的M个人、</p></li><li><p>输入</p><ul><li>N：正整数，不超过100000，人的数量</li><li>K：正整数，不超过1000，查询的数量</li><li>N个人：每行包括名字（不包含空格、最多8个字符的字符串）、年龄（范围<code>(0,200]</code>）、净值（范围为±1e6）</li><li>K次查询：M（筛选最有钱的几个人，不超过100）、年龄范围<code>[Amin,Amax]</code></li></ul></li><li><p>输出</p><p>  对于每1个查询，输出是第几个查询（从1到K），然后按净值非增序输出指定年龄范围内最有钱的M个人的信息，如果有重复则按年龄非降序输出，如果还有重复则按名字非降序输出。如果指定年龄范围内没人，就输出<code>None</code>。</p></li></ul><h1 id="题解一"><a href="#题解一" class="headerlink" title="题解一"></a>题解一</h1><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><ul><li><p>关于指定年龄范围，只要在输出时进行年龄判断即可。</p><p>  也可以考虑在结构体比较函数中进行年龄判断，当有不符合年龄限制的人时，将其往后放。</p></li><li><p>关于最有钱的人的数量，排序后只输出前M个符合年龄范围的人即可。</p></li><li><p><strong>用这个思路，测试点1和2会超时</strong>，因为这个算法的时间复杂度为$O(k\cdot n)$，最大为1e8（耗时约1秒）。</p><p>  M最大值为100，N最大值为100000，N和M差距很大，在N个人中寻找符合条件的M个人非常耗时。</p></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Problem: PAT Advanced 1055</span></span><br><span class="line"><span class="comment">// URL: https://pintia.cn/problem-sets/994805342720868352/problems/994805421066272768</span></span><br><span class="line"><span class="comment">// Tags: 排序</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> ageMin, ageMax;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Person</span>&#123;</span></span><br><span class="line">    <span class="built_in">string</span> name;</span><br><span class="line">    <span class="keyword">int</span> age, wealth;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">personCmp</span><span class="params">(Person&amp; p1, Person&amp; p2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p1.age&lt;ageMin || p1.age&gt;ageMax)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (p2.age&lt;ageMin || p2.age&gt;ageMax)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span> (p1.wealth == p2.wealth)&#123;</span><br><span class="line">        <span class="keyword">if</span> (p1.age == p2.age)</span><br><span class="line">            <span class="keyword">return</span> p1.name &lt; p2.name;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> p1.age &lt; p2.age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> p1.wealth &gt; p2.wealth;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n,k;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;k);</span><br><span class="line">    <span class="built_in">vector</span>&lt;Person&gt; people(n);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; people[i].name &gt;&gt; people[i].age &gt;&gt; people[i].wealth;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> x=<span class="number">1</span>, m; x&lt;=k; x++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d%d%d"</span>, &amp;m, &amp;ageMin, &amp;ageMax);</span><br><span class="line">        sort(people.begin(), people.end(), personCmp);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Case #%d:\n"</span>, x);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n &amp;&amp; count&lt;m; i++)</span><br><span class="line">            <span class="keyword">if</span> (people[i].age&gt;=ageMin &amp;&amp; people[i].age&lt;=ageMax)&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"%s %d %d\n"</span>, people[i].name.c_str(), people[i].age, people[i].wealth);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span> (count==<span class="number">0</span>)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"None\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="题解二"><a href="#题解二" class="headerlink" title="题解二"></a>题解二</h1><h2 id="解题思路-1"><a href="#解题思路-1" class="headerlink" title="解题思路"></a>解题思路</h2><ul><li>年龄范围是<code>(0,200]</code>，每次最多找100个人（M不超过100），那我们取出每个年龄的前100名，这样最多有20000个人。在这20000个人中查找M个人，还是快的（时间复杂度从题解一的1e8到现在的2e7），就不会超时了。</li></ul><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Problem: PAT Advanced 1055</span></span><br><span class="line"><span class="comment">// URL: https://pintia.cn/problem-sets/994805342720868352/problems/994805421066272768</span></span><br><span class="line"><span class="comment">// Tags: 排序</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> ageMin, ageMax;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Person</span>&#123;</span></span><br><span class="line">    <span class="built_in">string</span> name;</span><br><span class="line">    <span class="keyword">int</span> age, wealth;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">personCmp</span><span class="params">(Person&amp; p1, Person&amp; p2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p1.wealth == p2.wealth)&#123;</span><br><span class="line">        <span class="keyword">if</span> (p1.age == p2.age)</span><br><span class="line">            <span class="keyword">return</span> p1.name &lt; p2.name;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> p1.age &lt; p2.age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> p1.wealth &gt; p2.wealth;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n,k, selectedNum[<span class="number">201</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;k);</span><br><span class="line">    <span class="built_in">vector</span>&lt;Person&gt; people(n), selectedPeople;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; people[i].name &gt;&gt; people[i].age &gt;&gt; people[i].wealth;</span><br><span class="line">    sort(people.begin(), people.end(), personCmp);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)</span><br><span class="line">        <span class="keyword">if</span> (selectedNum[people[i].age] &lt; <span class="number">100</span>)&#123;</span><br><span class="line">            selectedPeople.push_back(people[i]);</span><br><span class="line">            selectedNum[people[i].age]++;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> x=<span class="number">1</span>, m; x&lt;=k; x++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d%d%d"</span>, &amp;m, &amp;ageMin, &amp;ageMax);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Case #%d:\n"</span>, x);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;selectedPeople.size() &amp;&amp; count&lt;m; i++)</span><br><span class="line">            <span class="keyword">if</span> (selectedPeople[i].age&gt;=ageMin &amp;&amp; selectedPeople[i].age&lt;=ageMax)&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"%s %d %d\n"</span>, selectedPeople[i].name.c_str(), selectedPeople[i].age, selectedPeople[i].wealth);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span> (count==<span class="number">0</span>)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"None\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://blog.csdn.net/liuchuo/article/details/52225204" target="_blank" rel="noopener">https://blog.csdn.net/liuchuo/article/details/52225204</a></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;题目介绍&quot;&gt;&lt;a href=&quot;#题目介绍&quot; class=&quot;headerlink&quot; title=&quot;题目介绍&quot;&gt;&lt;/a&gt;题目介绍&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;题目链接&lt;/p&gt;
&lt;p&gt;  &lt;a href=&quot;https://pintia.cn
      
    
    </summary>
    
    
      <category term="算法" scheme="https://chouxianyu.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="PAT" scheme="https://chouxianyu.github.io/tags/PAT/"/>
    
      <category term="排序" scheme="https://chouxianyu.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>PAT甲级1028List Sorting</title>
    <link href="https://chouxianyu.github.io/2021/02/05/PAT%E7%94%B2%E7%BA%A71028List-Sorting/"/>
    <id>https://chouxianyu.github.io/2021/02/05/PAT甲级1028List-Sorting/</id>
    <published>2021-02-05T07:07:53.000Z</published>
    <updated>2021-02-05T07:10:13.346Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="题目介绍"><a href="#题目介绍" class="headerlink" title="题目介绍"></a>题目介绍</h1><ul><li><p>题目链接</p><p>  <a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805468327690240" target="_blank" rel="noopener">https://pintia.cn/problem-sets/994805342720868352/problems/994805468327690240</a></p></li><li><p>题目考点</p><p>  结构体排序，就超简单……</p></li><li><p>题目难度</p><p>  PAT甲级25分</p></li><li><p>题目大意</p><p>  Excel可以按照某一列进行排序，要求实现这个函数。</p></li><li><p>输入</p><ul><li>N：整数，不超过100000，记录的数量</li><li>C：整数，按照C这1列进行排序</li><li>N个学生：每行包括学生ID（6位数字），名字（不包含空格、不超过8个字符的字符串），分数（范围[0,100]）</li></ul></li><li><p>输出</p><p>  输出N个学生。如果C=1则按照ID升序排列；如果C=2则按照名字非降序排列；如果C=3则按照分数非降序。如果名字或分数重复就按ID增序排列。</p></li></ul><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><ul><li>啊这（问号脸，这么简单？），学生放在vector里，写个结构体排序函数（把C定义成全局变量，在排序函数里根据C选取排序规则），调用STL里的sort就好了。</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Problem: PAT Advanced 1028</span></span><br><span class="line"><span class="comment">// URL: https://pintia.cn/problem-sets/994805342720868352/problems/994805468327690240</span></span><br><span class="line"><span class="comment">// Tags: </span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> c;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Student</span>&#123;</span></span><br><span class="line">    <span class="built_in">string</span> id, name;</span><br><span class="line">    <span class="keyword">int</span> grade;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">studentCmp</span><span class="params">(Student&amp; s1, Student&amp; s2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (c==<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> s1.id &lt; s2.id;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(c==<span class="number">2</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> s1.name==s2.name? s1.id &lt; s2.id : s1.name &lt; s2.name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> s1.grade==s2.grade? s1.id &lt; s2.id : s1.grade &lt; s2.grade;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;n, &amp;c);</span><br><span class="line">    <span class="built_in">vector</span>&lt;Student&gt; students(n);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; students[i].id &gt;&gt; students[i].name &gt;&gt; students[i].grade;</span><br><span class="line">    sort(students.begin(), students.end(), studentCmp);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> it=students.begin(); it!=students.end(); it++)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%s %s %d\n"</span>, it-&gt;id.c_str(), it-&gt;name.c_str(), it-&gt;grade);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;题目介绍&quot;&gt;&lt;a href=&quot;#题目介绍&quot; class=&quot;headerlink&quot; title=&quot;题目介绍&quot;&gt;&lt;/a&gt;题目介绍&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;题目链接&lt;/p&gt;
&lt;p&gt;  &lt;a href=&quot;https://pintia.cn
      
    
    </summary>
    
    
      <category term="算法" scheme="https://chouxianyu.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="PAT" scheme="https://chouxianyu.github.io/tags/PAT/"/>
    
      <category term="排序" scheme="https://chouxianyu.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>PAT甲级1025PAT Ranking</title>
    <link href="https://chouxianyu.github.io/2021/02/02/PAT%E7%94%B2%E7%BA%A71025PAT-Ranking/"/>
    <id>https://chouxianyu.github.io/2021/02/02/PAT甲级1025PAT-Ranking/</id>
    <published>2021-02-02T13:47:56.000Z</published>
    <updated>2021-02-03T07:26:18.925Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="题目介绍"><a href="#题目介绍" class="headerlink" title="题目介绍"></a>题目介绍</h1><ul><li><p>题目链接</p><p>  <a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805474338127872" target="_blank" rel="noopener">https://pintia.cn/problem-sets/994805342720868352/problems/994805474338127872</a></p></li><li><p>题目考点</p><p>  排序、模拟。排序是简单的结构体排序，模拟也不难。这题比较简单（我竟然没看题解做出来了，捂脸）</p></li><li><p>题目难度</p><p>  PAT甲级25分</p></li><li><p>题目大意</p><p>  汇总PAT各个考场的ranklist，生成最后的rank</p></li><li><p>输入</p><ul><li><p>N：正数，不超过100，考场的数量</p></li><li><p>N个ranklist：</p><p>  1个ranklist包括：第1个数字是K（正整数，不超过300，考生数量），然后K行，每行包括注册号（13位数字）和考生总分</p></li></ul></li><li><p>输出</p><ul><li><p>考生总数</p></li><li><p>最终的ranklist：包括注册号、final rank、考场号（索引为<code>[1,N]</code>）、考场中排名</p><p>  先按final rank非降序输出，再按注册号非降序输出。</p></li></ul></li></ul><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><ul><li><p>每读取1个考场的考生数据，就将其存入该考场的vector，然后排序计算local rank，再存入保存所有考生的vector，最后把所有考生排序，计算final rank，输出。</p></li><li><p>要根据分数排序，输出时还要根据排名和注册号排序</p><p>  后者已经包括了前者，因为rank升序就是分数降序，所以写一个排序函数就行了。</p></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Problem: PAT Advanced 1025</span></span><br><span class="line"><span class="comment">// URL: https://pintia.cn/problem-sets/994805342720868352/problems/994805474338127872</span></span><br><span class="line"><span class="comment">// Tags: 排序 模拟</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Testee</span>&#123;</span></span><br><span class="line">    <span class="built_in">string</span> id;</span><br><span class="line">    <span class="keyword">int</span> location, score, rank[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    Testee()&#123;&#125;</span><br><span class="line">    Testee(<span class="built_in">string</span> id_, <span class="keyword">int</span> location_, <span class="keyword">int</span> score_)&#123;</span><br><span class="line">        id = id_;</span><br><span class="line">        location = location_;</span><br><span class="line">        score = score_;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">display</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; id;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">" %d %d %d\n"</span>, rank[<span class="number">1</span>], location, rank[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">testeeCmp</span><span class="params">(Testee&amp; a, Testee&amp; b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a.score==b.score ? a.id&lt;b.id : a.score&gt;b.score ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">calcRank</span><span class="params">(<span class="built_in">vector</span>&lt;Testee&gt;&amp; testees,<span class="keyword">int</span> j)</span></span>&#123;</span><br><span class="line">    testees[<span class="number">0</span>].rank[j] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i &lt; testees.size(); i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (testees[i].score == testees[i<span class="number">-1</span>].score)</span><br><span class="line">            testees[i].rank[j] = testees[i<span class="number">-1</span>].rank[j];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            testees[i].rank[j] = i+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 定义变量</span></span><br><span class="line">    <span class="keyword">int</span> n, k, score;</span><br><span class="line">    <span class="built_in">string</span> id;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">    <span class="built_in">vector</span>&lt;Testee&gt; allTestees;</span><br><span class="line">    <span class="comment">// 读取输入，并计算local rank，将不同考场的考生保存到同1个vector里</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++)&#123; <span class="comment">// 遍历考场</span></span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;k);</span><br><span class="line">        <span class="built_in">vector</span>&lt;Testee&gt; localTestees(k);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;k; j++)&#123; <span class="comment">// 读取考生信息</span></span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; localTestees[j].id &gt;&gt; localTestees[j].score;</span><br><span class="line">            localTestees[j].location = i;</span><br><span class="line">        &#125;</span><br><span class="line">        sort(localTestees.begin(), localTestees.end(), testeeCmp);</span><br><span class="line">        calcRank(localTestees, <span class="number">0</span>); <span class="comment">// 计算该考场考生的local rank</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;k; j++) <span class="comment">// 将不同考场的考生保存到一个vector里</span></span><br><span class="line">            allTestees.push_back(localTestees[j]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 计算final rank</span></span><br><span class="line">    sort(allTestees.begin(), allTestees.end(), testeeCmp);</span><br><span class="line">    calcRank(allTestees, <span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 输出结果</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, allTestees.size());</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; allTestees.size(); i++)&#123;</span><br><span class="line">        allTestees[i].display();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;题目介绍&quot;&gt;&lt;a href=&quot;#题目介绍&quot; class=&quot;headerlink&quot; title=&quot;题目介绍&quot;&gt;&lt;/a&gt;题目介绍&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;题目链接&lt;/p&gt;
&lt;p&gt;  &lt;a href=&quot;https://pintia.cn
      
    
    </summary>
    
    
      <category term="算法" scheme="https://chouxianyu.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="PAT" scheme="https://chouxianyu.github.io/tags/PAT/"/>
    
      <category term="排序" scheme="https://chouxianyu.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>PAT甲级1016Phone Bills</title>
    <link href="https://chouxianyu.github.io/2021/02/02/PAT%E7%94%B2%E7%BA%A71016Phone-Bills/"/>
    <id>https://chouxianyu.github.io/2021/02/02/PAT甲级1016Phone-Bills/</id>
    <published>2021-02-02T05:00:04.000Z</published>
    <updated>2021-02-03T07:26:18.925Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="题目介绍"><a href="#题目介绍" class="headerlink" title="题目介绍"></a>题目介绍</h1><ul><li><p>题目链接</p><p>  <a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805493648703488" target="_blank" rel="noopener">https://pintia.cn/problem-sets/994805342720868352/problems/994805493648703488</a></p></li><li><p>题目考点</p><p>map、排序、模拟。map是简单地用了一下，排序也不难，重在模拟。</p></li><li><p>题目难度</p><p>  PAT甲级25分</p></li><li><p>题目大意</p><ul><li>长途电话每分钟收取一定费用，收费金额取决于拨打电话的时间。每次电话的开始时间和结束时间都会被记录，给出一系列通话记录，请生成每个月的账单。</li></ul></li><li><p>输入</p><ul><li>1行24个非负整数：00:00 - 01:00、01:00 - 02:00，以此类推一天中每个小时的收费金额（单位是<code>cents/minute</code>）</li><li>N：正整数，不超过1000，通话记录的数量</li><li>N行通话记录：1行的内容包括客户名称（最多20个字符的字符串，不包含空格）、日期和时间（<code>MM:dd:HH:mm</code>）、1个词（<code>on-line</code>或<code>off-line</code>），这些通话记录都是同一个月的。1个<code>on-line</code>记录与同1客户的时间顺序上的1个<code>off-line</code>记录配对，没有形成pair的通话记录会被忽略。输入中至少会有1个pair。假设同1客户不会有2条时间相同的记录。输入中时间的格式为24小时制。</li></ul></li><li><p>输出</p><p>  按客户名称的字母表顺序为每1个客户输出账单。对于每1个客户来说，要输出其名字、账单的月份、每条通话记录（包括开始时间、结束时间、通话分钟数、收费金额，要按时间顺序输出多条通话记录）、账单总金额</p></li></ul><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><ul><li><p>找到pair，生成通话记录</p><p>  先根据时间排序，然后判断是否形成pair（时间顺序、同用户、onLine和offLine）</p></li><li><p><strong>计算1次通话的费用，还有总费用</strong></p><p>  可以从0开始计算（需要先对收费金额进行处理），这样计算更容易。<code>on-line</code>和<code>off-line</code>2个时间点，计算0点到这2个时间点一直通话的收费，取差额即为应该支付的费用。</p></li><li><p>确定月份</p><p>  输出时取第1个记录的月份</p></li><li><p>客户名字字典序</p><p>  map实现（也可以通过结构体排序实现）</p></li><li><p>Record时间序输出</p><p>  通过结构体排序实现</p></li><li><p><strong>读取输入时不一定要保存原信息</strong></p><p>  比如时间本身是string，我们可以直接记录<code>month</code>、<code>day</code>等，把<code>:</code>等分隔符去除。<code>on-line</code>和<code>off-line</code>是字符串，我们可以用bool表示。</p></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Problem: PAT Advanced 1016</span></span><br><span class="line"><span class="comment">// URL: https://pintia.cn/problem-sets/994805342720868352/problems/994805493648703488</span></span><br><span class="line"><span class="comment">// Tags: map 排序</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> charge[<span class="number">25</span>];</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Record</span>&#123;</span></span><br><span class="line">    <span class="built_in">string</span> name;</span><br><span class="line">    <span class="keyword">bool</span> status;</span><br><span class="line">    <span class="keyword">int</span> month, day, hour, minute, time;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setStatus</span><span class="params">(<span class="built_in">string</span>&amp; status)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (status == <span class="string">"on-line"</span>)</span><br><span class="line">            <span class="keyword">this</span>-&gt;status = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;status = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">calcTime</span><span class="params">()</span></span>&#123; <span class="comment">// 以月初为零点的时间</span></span><br><span class="line">        time = day * <span class="number">24</span> * <span class="number">60</span> + hour * <span class="number">60</span> + minute;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">pairWith</span><span class="params">(Record&amp; r)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name == r.name &amp;&amp; status==<span class="literal">true</span> &amp;&amp; r.status==<span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">calcCostFromZero</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">double</span> cost = day * <span class="number">60</span> * charge[<span class="number">24</span>] + minute * charge[hour]; <span class="comment">// 天、分钟</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;hour; i++)&#123; <span class="comment">// 小时</span></span><br><span class="line">            cost += <span class="number">60</span> * charge[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> cost / <span class="number">100.0</span>; <span class="comment">// cent换算成美元</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%02d:%02d:%02d "</span>, day, hour, minute);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">recordCmp</span><span class="params">(Record&amp; a, Record&amp; b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a.name == b.name ? a.time &lt; b.time : a.name &lt; b.name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 读取输入并处理数据</span></span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">24</span>; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, charge + i);</span><br><span class="line">        charge[<span class="number">24</span>] += charge[i]; <span class="comment">// charge[24]是1整天都在通话时1天的收费</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">    <span class="built_in">vector</span>&lt;Record&gt; records(n);</span><br><span class="line">    <span class="built_in">string</span> status;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; records[i].name;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d:%d:%d:%d"</span>, &amp;records[i].month, &amp;records[i].day, &amp;records[i].hour, &amp;records[i].minute);</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; status;</span><br><span class="line">        records[i].setStatus(status);</span><br><span class="line">        records[i].calcTime(); <span class="comment">// 以月初为0点</span></span><br><span class="line">    &#125;</span><br><span class="line">    sort(records.begin(), records.end(), recordCmp);</span><br><span class="line">    <span class="comment">// 为每个客户记录订单</span></span><br><span class="line">    <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="built_in">vector</span>&lt;Record&gt;&gt; customers;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;n; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (records[i<span class="number">-1</span>].pairWith(records[i]))&#123;</span><br><span class="line">            customers[records[i].name].push_back(records[i<span class="number">-1</span>]);</span><br><span class="line">            customers[records[i].name].push_back(records[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出结果</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> customer=customers.begin(); customer!=customers.end(); customer++)&#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; customer-&gt;first;</span><br><span class="line">        records = customer-&gt;second;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">" %02d\n"</span>, records[<span class="number">0</span>].month);</span><br><span class="line">        <span class="keyword">double</span> totalCost = <span class="number">0</span>, cost;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;records.size(); i+=<span class="number">2</span>)&#123;</span><br><span class="line">            cost = records[i].calcCostFromZero() - records[i<span class="number">-1</span>].calcCostFromZero();</span><br><span class="line">            totalCost += cost;</span><br><span class="line">            records[i<span class="number">-1</span>].print();</span><br><span class="line">            records[i].print();</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%d "</span>, records[i].time - records[i<span class="number">-1</span>].time);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"$%.02f\n"</span>, cost);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Total amount: $%.02f\n"</span>, totalCost);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://blog.csdn.net/liuchuo/article/details/52294397" target="_blank" rel="noopener">https://blog.csdn.net/liuchuo/article/details/52294397</a></li></ul><hr><p>Github（github.com）：<a href="https://github.com/chouxianyu" target="_blank" rel="noopener">@chouxianyu</a></p><p>Github Pages（github.io）：<a href="https://chouxianyu.github.io/">@臭咸鱼</a></p><p>知乎（zhihu.com）：<a href="https://www.zhihu.com/people/chouxianyu0" target="_blank" rel="noopener">@臭咸鱼</a></p><p>博客园（cnblogs.com）：<a href="https://www.cnblogs.com/chouxianyu/" target="_blank" rel="noopener">@臭咸鱼</a></p><p>B站（bilibili.com）：<a href="https://space.bilibili.com/346368054" target="_blank" rel="noopener">@绝版臭咸鱼</a></p><p>微信公众号：<a href="https://images.cnblogs.com/cnblogs_com/chouxianyu/1511971/o_201224051323wechat_qrcode.jpg" target="_blank" rel="noopener">@臭咸鱼</a></p><p>转载请注明出处，欢迎讨论和交流!</p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;题目介绍&quot;&gt;&lt;a href=&quot;#题目介绍&quot; class=&quot;headerlink&quot; title=&quot;题目介绍&quot;&gt;&lt;/a&gt;题目介绍&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;题目链接&lt;/p&gt;
&lt;p&gt;  &lt;a href=&quot;https://pintia.cn
      
    
    </summary>
    
    
      <category term="算法" scheme="https://chouxianyu.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="PAT" scheme="https://chouxianyu.github.io/tags/PAT/"/>
    
      <category term="map" scheme="https://chouxianyu.github.io/tags/map/"/>
    
      <category term="排序" scheme="https://chouxianyu.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
</feed>
